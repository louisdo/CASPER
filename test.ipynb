{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from scipy.sparse import csr_matrix, vstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand([8,4, 64]) # 8 batch size, 4 doc length, 64 voc size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "values, indices = torch.max(a, dim = 1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[4.3256e-01, 4.1120e-01, 7.3411e-01,  ..., 3.2532e-01,\n",
       "          4.7160e-01, 8.5539e-01],\n",
       "         [9.4293e-01, 7.3076e-01, 7.6004e-01,  ..., 6.8945e-01,\n",
       "          9.5728e-01, 6.4862e-01],\n",
       "         [8.9709e-01, 1.4690e-01, 5.5702e-01,  ..., 7.4110e-01,\n",
       "          5.9739e-02, 5.2283e-03],\n",
       "         [2.9843e-01, 5.2270e-01, 4.4281e-01,  ..., 8.4998e-01,\n",
       "          7.7924e-01, 2.2082e-01]],\n",
       "\n",
       "        [[3.6714e-01, 9.9507e-01, 1.2187e-01,  ..., 7.7613e-02,\n",
       "          9.0787e-01, 6.3306e-02],\n",
       "         [6.9210e-01, 8.4273e-01, 5.7964e-01,  ..., 8.2597e-01,\n",
       "          8.2159e-01, 5.5555e-01],\n",
       "         [3.2833e-01, 4.1674e-01, 8.3518e-01,  ..., 2.0491e-01,\n",
       "          9.0301e-01, 8.2214e-01],\n",
       "         [3.9421e-01, 5.0856e-01, 4.6419e-01,  ..., 2.2918e-01,\n",
       "          5.3357e-01, 4.9946e-01]],\n",
       "\n",
       "        [[3.5248e-01, 8.4723e-01, 9.9243e-01,  ..., 2.0415e-01,\n",
       "          4.1951e-01, 1.6478e-01],\n",
       "         [5.9342e-01, 5.0655e-02, 1.4945e-01,  ..., 9.9110e-01,\n",
       "          3.4506e-01, 6.6266e-01],\n",
       "         [6.2718e-01, 9.9640e-01, 9.5050e-01,  ..., 9.0923e-02,\n",
       "          9.6431e-01, 9.9663e-01],\n",
       "         [6.4458e-03, 7.4942e-01, 1.4805e-01,  ..., 2.3392e-01,\n",
       "          2.7255e-01, 8.5190e-02]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[6.7574e-01, 4.2001e-01, 6.1071e-01,  ..., 3.9517e-01,\n",
       "          5.8285e-01, 6.5989e-01],\n",
       "         [6.4451e-01, 8.8770e-02, 3.1684e-01,  ..., 7.2116e-04,\n",
       "          9.1870e-02, 3.1581e-01],\n",
       "         [1.2470e-01, 5.4875e-01, 1.0338e-01,  ..., 6.0138e-01,\n",
       "          4.8366e-01, 5.9375e-01],\n",
       "         [4.5322e-01, 8.2051e-02, 7.2439e-01,  ..., 5.4783e-01,\n",
       "          1.1334e-01, 1.1751e-01]],\n",
       "\n",
       "        [[6.0635e-01, 7.0582e-01, 8.6544e-01,  ..., 6.4099e-03,\n",
       "          8.8074e-01, 5.6645e-01],\n",
       "         [9.9657e-01, 7.5890e-01, 8.8529e-01,  ..., 4.9989e-01,\n",
       "          2.0051e-01, 5.3015e-01],\n",
       "         [2.1186e-02, 5.1885e-01, 8.2698e-01,  ..., 5.1073e-01,\n",
       "          2.1342e-01, 6.4241e-01],\n",
       "         [8.3943e-01, 1.6068e-01, 5.7550e-01,  ..., 3.8568e-01,\n",
       "          5.2488e-01, 7.8041e-01]],\n",
       "\n",
       "        [[5.1269e-01, 2.2402e-01, 7.9909e-01,  ..., 3.7402e-01,\n",
       "          6.7052e-01, 4.1551e-01],\n",
       "         [1.4565e-01, 9.7281e-01, 3.1397e-01,  ..., 9.9926e-02,\n",
       "          5.1815e-01, 6.7304e-01],\n",
       "         [6.8132e-01, 3.6695e-01, 7.9427e-01,  ..., 7.0644e-01,\n",
       "          4.0874e-01, 5.3024e-01],\n",
       "         [9.8100e-01, 1.4716e-01, 7.8999e-01,  ..., 4.2386e-01,\n",
       "          8.6570e-02, 5.7110e-01]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1, 64])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "placeholder = torch.zeros([8, 4, 64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "placeholder.scatter_(1, indices, values)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.8554],\n",
       "         [0.9429, 0.7308, 0.7600,  ..., 0.0000, 0.9573, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.8500, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.9951, 0.0000,  ..., 0.0000, 0.9079, 0.0000],\n",
       "         [0.6921, 0.0000, 0.0000,  ..., 0.8260, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.8352,  ..., 0.0000, 0.0000, 0.8221],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.9924,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.9911, 0.0000, 0.0000],\n",
       "         [0.6272, 0.9964, 0.0000,  ..., 0.0000, 0.9643, 0.9966],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.6757, 0.0000, 0.0000,  ..., 0.0000, 0.5828, 0.6599],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.5487, 0.0000,  ..., 0.6014, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.7244,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.8807, 0.0000],\n",
       "         [0.9966, 0.7589, 0.8853,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.5107, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.7804]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.7991,  ..., 0.0000, 0.6705, 0.0000],\n",
       "         [0.0000, 0.9728, 0.0000,  ..., 0.0000, 0.0000, 0.6730],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.7064, 0.0000, 0.0000],\n",
       "         [0.9810, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(crow_indices=tensor([ 0, 15, 29, 46, 64]),\n",
       "        col_indices=tensor([ 3,  5,  9, 10, 11, 13, 20, 22, 24, 26, 27, 30, 42,\n",
       "                            45, 63,  0,  1,  2,  6, 18, 23, 25, 38, 39, 40, 43,\n",
       "                            46, 52, 62,  4,  8, 12, 14, 17, 28, 29, 35, 37, 44,\n",
       "                            48, 50, 51, 53, 55, 56, 58,  7, 15, 16, 19, 21, 31,\n",
       "                            32, 33, 34, 36, 41, 47, 49, 54, 57, 59, 60, 61]),\n",
       "        values=tensor([0.8764, 0.8575, 0.9882, 0.9648, 0.7992, 0.5426, 0.1634,\n",
       "                       0.8931, 0.7473, 0.9835, 0.7123, 0.5764, 0.8665, 0.9447,\n",
       "                       0.8554, 0.9429, 0.7308, 0.7600, 0.7471, 0.9647, 0.9474,\n",
       "                       0.7450, 0.6540, 0.8524, 0.7276, 0.9742, 0.8493, 0.8260,\n",
       "                       0.9573, 0.9987, 0.8877, 0.7441, 0.7542, 0.6329, 0.9272,\n",
       "                       0.8985, 0.6817, 0.6455, 0.5277, 0.9358, 0.9507, 0.5005,\n",
       "                       0.9883, 0.9908, 0.7071, 0.9543, 0.7757, 0.9519, 0.9848,\n",
       "                       0.8545, 0.6129, 0.7222, 0.9872, 0.7125, 0.9421, 0.9925,\n",
       "                       0.7543, 0.8853, 0.9026, 0.8220, 0.9609, 0.4251, 0.8703,\n",
       "                       0.8500]), size=(4, 64), nnz=64, layout=torch.sparse_csr),\n",
       " tensor(crow_indices=tensor([ 0, 13, 29, 43, 64]),\n",
       "        col_indices=tensor([ 1,  4, 11, 24, 25, 28, 30, 33, 35, 47, 53, 57, 62,\n",
       "                             0,  3,  8, 14, 15, 16, 17, 22, 23, 31, 45, 49, 54,\n",
       "                            55, 56, 61,  2,  5,  6, 19, 27, 40, 44, 46, 48, 50,\n",
       "                            52, 58, 60, 63,  7,  9, 10, 12, 13, 18, 20, 21, 26,\n",
       "                            29, 32, 34, 36, 37, 38, 39, 41, 42, 43, 51, 59]),\n",
       "        values=tensor([0.9951, 0.6990, 0.9021, 0.5641, 0.8427, 0.8112, 0.9451,\n",
       "                       0.9082, 0.9593, 0.8222, 0.9557, 0.8996, 0.9079, 0.6921,\n",
       "                       0.9139, 0.9752, 0.9643, 0.9349, 0.9667, 0.7542, 0.9716,\n",
       "                       0.8977, 0.6457, 0.9991, 0.7763, 0.9686, 0.7436, 0.4663,\n",
       "                       0.8260, 0.8352, 0.8635, 0.9274, 0.8584, 0.7583, 0.9867,\n",
       "                       0.3933, 0.9234, 0.8151, 0.9165, 0.8894, 0.6362, 0.9658,\n",
       "                       0.8221, 0.7394, 0.9972, 0.7377, 0.7103, 0.5606, 0.5056,\n",
       "                       0.8822, 0.9938, 0.9980, 0.9100, 0.9670, 0.8575, 0.7087,\n",
       "                       0.5869, 0.8028, 0.5717, 0.9894, 0.9822, 0.8265, 0.8867,\n",
       "                       0.9183]), size=(4, 64), nnz=64, layout=torch.sparse_csr),\n",
       " tensor(crow_indices=tensor([ 0, 14, 25, 50, 64]),\n",
       "        col_indices=tensor([ 2,  3,  4,  9, 13, 14, 17, 19, 20, 41, 45, 46, 52,\n",
       "                            58,  7, 10, 15, 30, 32, 36, 42, 43, 50, 56, 61,  0,\n",
       "                             1,  5,  6,  8, 11, 12, 16, 18, 24, 25, 29, 31, 34,\n",
       "                            37, 38, 39, 44, 47, 51, 53, 55, 57, 62, 63, 21, 22,\n",
       "                            23, 26, 27, 28, 33, 35, 40, 48, 49, 54, 59, 60]),\n",
       "        values=tensor([0.9924, 0.7560, 0.6975, 0.4431, 0.5799, 0.8987, 0.8259,\n",
       "                       0.8755, 0.6459, 0.5179, 0.9393, 0.8306, 0.8719, 0.9625,\n",
       "                       0.9123, 0.9173, 0.9443, 0.4537, 0.4931, 0.9997, 0.9544,\n",
       "                       0.9642, 0.9707, 0.7650, 0.9911, 0.6272, 0.9964, 0.6239,\n",
       "                       0.8143, 0.9608, 0.8700, 0.7496, 0.9654, 0.6095, 0.9544,\n",
       "                       0.8005, 0.6491, 0.9273, 0.9932, 0.6516, 0.7031, 0.9413,\n",
       "                       0.8788, 0.6602, 0.7848, 0.9986, 0.7502, 0.9733, 0.9643,\n",
       "                       0.9966, 0.8489, 0.7663, 0.9825, 0.6213, 0.8215, 0.8592,\n",
       "                       0.6010, 0.2934, 0.8923, 0.5313, 0.7805, 0.5149, 0.8101,\n",
       "                       0.9008]), size=(4, 64), nnz=64, layout=torch.sparse_csr),\n",
       " tensor(crow_indices=tensor([ 0, 13, 34, 51, 64]),\n",
       "        col_indices=tensor([ 4, 23, 26, 27, 28, 35, 38, 41, 43, 49, 50, 58, 63,\n",
       "                             0,  1,  3,  5,  7,  9, 10, 13, 15, 17, 19, 31, 34,\n",
       "                            39, 42, 45, 51, 52, 53, 54, 55,  8, 12, 16, 18, 20,\n",
       "                            21, 24, 25, 29, 32, 33, 37, 40, 44, 46, 60, 61,  2,\n",
       "                             6, 11, 14, 22, 30, 36, 47, 48, 56, 57, 59, 62]),\n",
       "        values=tensor([0.9256, 0.8286, 0.7479, 0.9875, 0.9946, 0.8576, 0.8427,\n",
       "                       0.6410, 0.8799, 0.6663, 0.9419, 0.8331, 0.9585, 0.4181,\n",
       "                       0.9388, 0.8491, 0.8349, 0.9723, 0.3092, 0.9790, 0.4987,\n",
       "                       0.9334, 0.3953, 0.8384, 0.9644, 0.8758, 0.9755, 0.8720,\n",
       "                       0.9896, 0.8078, 0.5270, 0.8957, 0.8650, 0.4872, 0.9551,\n",
       "                       0.9894, 0.5847, 0.6114, 0.9745, 0.9922, 0.7828, 0.9590,\n",
       "                       0.5259, 0.6136, 0.6330, 0.6576, 0.8550, 0.9056, 0.9064,\n",
       "                       0.7635, 0.8070, 0.9743, 0.9201, 0.8936, 0.9690, 0.7613,\n",
       "                       0.8397, 0.6728, 0.9916, 0.8934, 0.9620, 0.4623, 0.8111,\n",
       "                       0.8808]), size=(4, 64), nnz=64, layout=torch.sparse_csr),\n",
       " tensor(crow_indices=tensor([ 0, 21, 31, 46, 64]),\n",
       "        col_indices=tensor([ 0,  7, 10, 11, 13, 16, 17, 19, 23, 31, 33, 36, 37,\n",
       "                            39, 42, 43, 45, 46, 52, 57, 63,  4,  8, 18, 24, 25,\n",
       "                            27, 29, 40, 53, 62,  1,  3,  9, 14, 26, 28, 30, 34,\n",
       "                            41, 44, 49, 51, 55, 58, 59,  2,  5,  6, 12, 15, 20,\n",
       "                            21, 22, 32, 35, 38, 47, 48, 50, 54, 56, 60, 61]),\n",
       "        values=tensor([0.8188, 0.9815, 0.8434, 0.8109, 0.8494, 0.5571, 0.9833,\n",
       "                       0.8302, 0.7670, 0.9421, 0.7875, 0.5881, 0.7334, 0.9239,\n",
       "                       0.9508, 0.9291, 0.8285, 0.8068, 0.7681, 0.9279, 0.9044,\n",
       "                       0.8843, 0.9157, 0.4356, 0.9350, 0.5387, 0.8498, 0.8421,\n",
       "                       0.9922, 0.8242, 0.9988, 0.8840, 0.5333, 0.8681, 0.8679,\n",
       "                       0.7159, 0.6250, 0.9689, 0.7187, 0.8915, 0.5811, 0.8354,\n",
       "                       0.8313, 0.8877, 0.4459, 0.8311, 0.8971, 0.9447, 0.6277,\n",
       "                       0.8915, 0.8956, 0.2929, 0.8844, 0.4535, 0.9436, 0.7397,\n",
       "                       0.7044, 0.9681, 0.9648, 0.5881, 0.9917, 0.9506, 0.4409,\n",
       "                       0.8312]), size=(4, 64), nnz=64, layout=torch.sparse_csr),\n",
       " tensor(crow_indices=tensor([ 0, 18, 32, 46, 64]),\n",
       "        col_indices=tensor([ 0,  4,  5, 13, 15, 19, 24, 25, 31, 33, 35, 42, 43,\n",
       "                            46, 52, 60, 62, 63,  3,  9, 20, 26, 28, 34, 41, 45,\n",
       "                            47, 51, 53, 54, 56, 58,  1,  6,  7, 10, 11, 16, 29,\n",
       "                            32, 37, 38, 49, 55, 59, 61,  2,  8, 12, 14, 17, 18,\n",
       "                            21, 22, 23, 27, 30, 36, 39, 40, 44, 48, 50, 57]),\n",
       "        values=tensor([0.6757, 0.6867, 0.7571, 0.7413, 0.7694, 0.9908, 0.5440,\n",
       "                       0.8953, 0.7041, 0.8994, 0.9989, 0.8659, 0.7895, 0.9899,\n",
       "                       0.8754, 0.9202, 0.5828, 0.6599, 0.9553, 0.5165, 0.6742,\n",
       "                       0.8136, 0.9412, 0.9836, 0.6906, 0.9949, 0.3874, 0.8787,\n",
       "                       0.3202, 0.7649, 0.9567, 0.9350, 0.5487, 0.6176, 0.8853,\n",
       "                       0.9893, 0.7606, 0.9521, 0.9492, 0.7140, 0.4048, 0.8555,\n",
       "                       0.8849, 0.9544, 0.7647, 0.6014, 0.7244, 0.6798, 0.9901,\n",
       "                       0.6416, 0.7605, 0.5807, 0.9780, 0.9810, 0.9955, 0.7196,\n",
       "                       0.6582, 0.9195, 0.8499, 0.5679, 0.8086, 0.4393, 0.9035,\n",
       "                       0.7871]), size=(4, 64), nnz=64, layout=torch.sparse_csr),\n",
       " tensor(crow_indices=tensor([ 0, 17, 34, 50, 64]),\n",
       "        col_indices=tensor([ 3,  7, 11, 14, 21, 27, 29, 31, 37, 39, 50, 51, 52,\n",
       "                            55, 56, 59, 62,  0,  1,  2,  6, 10, 12, 22, 23, 24,\n",
       "                            30, 33, 36, 38, 40, 49, 53, 60,  4,  5,  8,  9, 13,\n",
       "                            17, 20, 32, 34, 35, 42, 44, 45, 47, 54, 61, 15, 16,\n",
       "                            18, 19, 25, 26, 28, 41, 43, 46, 48, 57, 58, 63]),\n",
       "        values=tensor([0.9353, 0.9639, 0.8883, 0.3447, 0.4922, 0.5629, 0.7409,\n",
       "                       0.9082, 0.5206, 0.8534, 0.8631, 0.6912, 0.9772, 0.8546,\n",
       "                       0.9738, 0.9984, 0.8807, 0.9966, 0.7589, 0.8853, 0.9660,\n",
       "                       0.7842, 0.8798, 0.6553, 0.8544, 0.5144, 0.9254, 0.9717,\n",
       "                       0.6801, 0.8430, 0.8057, 0.6881, 0.7358, 0.6403, 0.6150,\n",
       "                       0.8064, 0.8631, 0.7090, 0.9088, 0.9561, 0.8770, 0.6650,\n",
       "                       0.8991, 0.9190, 0.7789, 0.9257, 0.9119, 0.8028, 0.6483,\n",
       "                       0.5107, 0.6547, 0.7230, 0.9543, 0.6352, 0.6835, 0.6706,\n",
       "                       0.9371, 0.3490, 0.9203, 0.5950, 0.3770, 0.6862, 0.8198,\n",
       "                       0.7804]), size=(4, 64), nnz=64, layout=torch.sparse_csr),\n",
       " tensor(crow_indices=tensor([ 0, 14, 32, 53, 64]),\n",
       "        col_indices=tensor([ 2, 10, 15, 16, 18, 25, 29, 34, 40, 44, 48, 50, 56,\n",
       "                            62,  1,  4,  8, 19, 24, 26, 27, 28, 31, 38, 41, 46,\n",
       "                            51, 54, 55, 58, 59, 63,  3,  5,  6,  7, 12, 13, 14,\n",
       "                            17, 21, 22, 23, 33, 35, 39, 42, 47, 49, 53, 57, 60,\n",
       "                            61,  0,  9, 11, 20, 30, 32, 36, 37, 43, 45, 52]),\n",
       "        values=tensor([0.7991, 0.9187, 0.7880, 0.7287, 0.5794, 0.6906, 0.9530,\n",
       "                       0.6013, 0.7753, 0.9403, 0.7687, 0.7568, 0.6863, 0.6705,\n",
       "                       0.9728, 0.7020, 0.4267, 0.8986, 0.6120, 0.9971, 0.9213,\n",
       "                       0.7677, 0.9759, 0.9898, 0.6025, 0.6523, 0.8139, 0.8788,\n",
       "                       0.9621, 0.9119, 0.9750, 0.6730, 0.7814, 0.8014, 0.6142,\n",
       "                       0.5717, 0.8972, 0.9320, 0.6283, 0.7203, 0.4922, 0.7629,\n",
       "                       0.7704, 0.7802, 0.9130, 0.6087, 0.7061, 0.7425, 0.8354,\n",
       "                       0.6275, 0.5237, 0.8834, 0.7064, 0.9810, 0.6919, 0.6943,\n",
       "                       0.6414, 0.9093, 0.8324, 0.9360, 0.9753, 0.8260, 0.7030,\n",
       "                       0.7667]), size=(4, 64), nnz=64, layout=torch.sparse_csr)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[item.to_sparse_csr() for item in placeholder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected dimension <= 2 array or matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/lamdo/splade/test.ipynb Cell 10\u001b[0m line \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bosprey2.csl.illinois.edu/home/lamdo/splade/test.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m csr_matrix(placeholder)\n",
      "File \u001b[0;32m~/miniconda3/envs/colbert/lib/python3.8/site-packages/scipy/sparse/_compressed.py:85\u001b[0m, in \u001b[0;36m_cs_matrix.__init__\u001b[0;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     82\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39munrecognized \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m_matrix constructor usage\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     83\u001b[0m                          \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m     84\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_self(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m(\n\u001b[0;32m---> 85\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_coo_container(arg1, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[1;32m     86\u001b[0m     ))\n\u001b[1;32m     88\u001b[0m \u001b[39m# Read matrix dimensions given, if any\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[39mif\u001b[39;00m shape \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/colbert/lib/python3.8/site-packages/scipy/sparse/_coo.py:182\u001b[0m, in \u001b[0;36mcoo_matrix.__init__\u001b[0;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[1;32m    179\u001b[0m M \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39matleast_2d(np\u001b[39m.\u001b[39masarray(arg1))\n\u001b[1;32m    181\u001b[0m \u001b[39mif\u001b[39;00m M\u001b[39m.\u001b[39mndim \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m--> 182\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mexpected dimension <= 2 array or matrix\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    184\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shape \u001b[39m=\u001b[39m check_shape(M\u001b[39m.\u001b[39mshape)\n\u001b[1;32m    185\u001b[0m \u001b[39mif\u001b[39;00m shape \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: expected dimension <= 2 array or matrix"
     ]
    }
   ],
   "source": [
    "csr_matrix(placeholder[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_placeholder = csr_matrix(placeholder[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_placeholder.sha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<8x64 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 128 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vstack([sparse_placeholder, sparse_placeholder])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "reshape is not implemented for sparse tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/lamdo/splade/test.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bosprey2.csl.illinois.edu/home/lamdo/splade/test.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m torch\u001b[39m.\u001b[39;49meinsum(\u001b[39m\"\u001b[39;49m\u001b[39mabc,adc->abd\u001b[39;49m\u001b[39m\"\u001b[39;49m, sparse_placeholder, sparse_placeholder)\u001b[39m.\u001b[39mshape\n",
      "File \u001b[0;32m~/miniconda3/envs/venv310/lib/python3.10/site-packages/torch/functional.py:402\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[39mreturn\u001b[39;00m einsum(equation, \u001b[39m*\u001b[39m_operands)\n\u001b[1;32m    399\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(operands) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m2\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m opt_einsum\u001b[39m.\u001b[39menabled:\n\u001b[1;32m    400\u001b[0m     \u001b[39m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[1;32m    401\u001b[0m     \u001b[39m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[0;32m--> 402\u001b[0m     \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39;49meinsum(equation, operands)  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    404\u001b[0m path \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    405\u001b[0m \u001b[39mif\u001b[39;00m opt_einsum\u001b[39m.\u001b[39mis_available():\n",
      "\u001b[0;31mRuntimeError\u001b[0m: reshape is not implemented for sparse tensors"
     ]
    }
   ],
   "source": [
    "torch.einsum(\"abc,adc->abd\", sparse_placeholder, sparse_placeholder).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 6])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9870, 0.9076, 0.8045, 0.7111, 0.9331, 0.9956],\n",
       "        [0.4566, 0.5471, 0.6978, 0.8263, 0.5582, 0.9057],\n",
       "        [0.6588, 0.4227, 0.4806, 0.9573, 0.8615, 0.8149],\n",
       "        [0.4391, 0.6201, 0.7253, 0.9623, 0.4717, 0.6551],\n",
       "        [0.4474, 0.8339, 0.8506, 0.7985, 0.9820, 0.9904],\n",
       "        [0.8233, 0.9183, 0.5700, 0.6797, 0.6216, 0.7837],\n",
       "        [0.6125, 0.9342, 0.8309, 0.4363, 0.9218, 0.7943],\n",
       "        [0.8024, 0.9845, 0.4851, 0.8692, 0.7523, 0.8090]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 2],\n",
       "        [1, 2, 1, 2, 0, 2],\n",
       "        [2, 0, 2, 1, 2, 2],\n",
       "        [1, 0, 2, 1, 2, 1],\n",
       "        [1, 0, 2, 2, 2, 1],\n",
       "        [1, 1, 2, 0, 1, 0],\n",
       "        [1, 2, 2, 1, 0, 1],\n",
       "        [2, 0, 0, 0, 0, 2]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 8\n",
    "q_pad_len = 4\n",
    "d_pad_len = 6\n",
    "out_dim = 64\n",
    "\n",
    "\n",
    "d_rep_full = torch.rand((bs, d_pad_len, out_dim))\n",
    "q_rep_full = torch.rand((bs, q_pad_len, out_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 6, 4])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum(\"abc,adc->abd\", d_rep_full, q_rep_full).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lamdo/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch, os\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"distilbert-base-uncased\"\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_type)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\"machine learning is fun\", \"machine learning is in no way fun\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer(docs, return_tensors = \"pt\", padding = True, truncation=True, max_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 101, 3698, 4083, 2003, 4569,  102,    0,    0,    0],\n",
       "         [ 101, 3698, 4083, 2003, 1999, 2053, 2126, 4569,  102]]),\n",
       " tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[\"input_ids\"], tokens[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.nn.functional.one_hot(tokens[\"input_ids\"][0], num_classes = 30522)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 101, 3698, 4083, 2003, 4569,  102,    0,    0,    0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[\"input_ids\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[1][3698]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_test = test * tokens[\"attention_mask\"][0].unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 30522])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0,  ..., 0, 0, 0])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_test[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 9, 30522])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "values, indices = torch.max(masked_test, dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values[tokens[\"input_ids\"][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0,  ..., 0, 0, 0])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = torch.nonzero(values).squeeze().cpu().tolist()\n",
    "_indices = indices[col].cpu().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 5, 3, 1, 2, 4]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 101], [1, 3698], [2, 4083], [3, 2003], [4, 4569], [5, 102]]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/scratch/lamdo/splade_kp_datasets/kp20kbiomed/raw.tsv\") as f:\n",
    "    for line in f:\n",
    "        query, pos, neg = line.split(\"\\t\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'telepresence, animation, avatars, application sharing, collaborative virtual environments'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'virtually enhancing the perception of user actions. This paper proposes using virtual reality to enhance the perception of actions by distant users on a shared application. Here, distance may refer either to space ( e.g. in a remote synchronous collaboration) or time ( e.g. during playback of recorded actions). Our approach consists in immersing the application in a virtual inhabited 3D space and mimicking user actions by animating avatars. We illustrate this approach with two applications, the one for remote collaboration on a shared application and the other to playback recorded sequences of user actions. We suggest this could be a low cost enhancement for telepresence'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a new method for similarity indexing of market basket data. In recent years, many data mining methods have been proposed for finding useful and structured information from market basket data. The association rule model was recently proposed in order to discover useful patterns and dependencies in such data. This paper discusses a method for indexing market basket data efficiently for similarity search. The technique is likely to be very useful in applications which utilize the similarity in customer buying behavior in order to make peer recommendations. We propose an index called the signature table , which is very flexible in supporting a wide range of similarity functions. The construction of the index structure is independent of the similarity function, which can be specified at query time. The resulting similarity search algorithm shows excellent scalability with increasing memory availability and database size\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
