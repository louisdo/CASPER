{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "308157it [00:00, 448725.71it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "with open(\"/scratch/lamdo/s2orc/processed/citation_contexts_triplets/triplets_intermediate_cs.tsv\") as f:\n",
    "    intermediate = []\n",
    "    for line in tqdm(f):\n",
    "        splitted_line = line.strip().split(\"\\t\")\n",
    "        intermediate.append(splitted_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['In local studies, the use of vector data types is easier .',\n",
       "  '4541949',\n",
       "  '125325689'],\n",
       " ['All arrays were + using the NumPy package .', '16907816', '4541949'],\n",
       " ['Recall is the fraction of the total amount of relevant instances that were actually retrieved .',\n",
       "  '14081058',\n",
       "  '125325689'],\n",
       " ['PCA analysis of replicates from sample A and B The relationships among spectra can be visualized by an unsupervised pattern recognition algorithm PCA .',\n",
       "  '25253360',\n",
       "  '17900407'],\n",
       " ['Phylogenetic and mutational analysis The full length genomes were annotated using VIGOR software available at VIPR server (https://www.viprbrc.org/brc/vigorAnnotator.s pg?method¼ShowCleanInputPage&amp;decorator¼toga) .',\n",
       "  '9194300',\n",
       "  '18902297'],\n",
       " ['NTN Use Cases Non-terrestrial systems have been proposed to enable several applications, including weather forecasting, video surveillance, TV broadcast, remote sensing, and navigation, for several years.On the other hand, recent technological advances in the aerial/space sector have enabled the implementation of more sophisticated use cases, such as distributed computing and content broadcasting, service boosting for users in congested areas, eMBB in underserved areas, and multi-connectivity for service continuity via cellular networks .',\n",
       "  '235829846',\n",
       "  '226486759'],\n",
       " ['Non-terrestrial technology has long been thought to support operations such as home delivery, weather forecasting, video surveillance, television transmission, remote sensing, and navigation.To enable more advanced use cases, such as Communication Resilience and Service Continuity, Global Satellite Overlay, Ubiquitous Internet of Things (IoT) Broadcasting, Advanced Backhauling, and Energy-Efficient Hybrid Multiplay, the recent technological advancements in the aerial/space industry have however made it possible for integration between terrestrial and non-terrestrial technologies.The few enabling technologies to support NTN in 6G for architecture, spectrum, antenna, and higher layers advancements, respectively, include gallium nitride (GaN), cognitive spectrum, multi-beam structures, and TCP spoofing and multiplexing .',\n",
       "  '220055483',\n",
       "  '220259280'],\n",
       " ['Data collection A case study investigates a contemporary phenomenon in depth and within its real-world context, focusing on answering how and why questions .',\n",
       "  '964694',\n",
       "  '7635031'],\n",
       " ['Secondly, from the early 2010s, questions related to the underlying causes of barriers, their short-and long-term consequences, and internal dynamics have been raised, but they remain unanswered .',\n",
       "  '7635031',\n",
       "  '964694'],\n",
       " ['Distribution test based methods have proven to be computationally efficient in SISO systems , - .',\n",
       "  '29473817',\n",
       "  '201069399'],\n",
       " ['Kanterakis and Su tried to reduce the computational complexity of the likelihood based classifier by modifying the likelihood function .',\n",
       "  '9204371',\n",
       "  '3501374'],\n",
       " ['A combination of moment features and likelihood maximization has been experimented for a balance between classification accuracy and computational complexity in .',\n",
       "  '13706276',\n",
       "  '29473817'],\n",
       " ['Dulek proposed a likelihood based method in combination with an online channel estimator .',\n",
       "  '11640841',\n",
       "  '2405012'],\n",
       " ['In , the authors tried to predict volume using Partial Least Squares (PLS) and Support Vector Regression (SVR).',\n",
       "  '89615068',\n",
       "  '52874011'],\n",
       " ['One example is , where the authors predicted future options prices using conventional pricing techniques combined with two learning models: Neural Networks and Support Vector Regression.',\n",
       "  '32320813',\n",
       "  '206457500'],\n",
       " [\"), Lipinski's Rule violations (Lipinski..violations), Ghose Filter violations (Ghose..violations), Veber Rule violations (Veber.violations),Egan Rule violations (Egan..violations), Muegge's Rule violations (Muegge..violations), bioavailability score (bioavailability.score),molecular weight (molweight), P: conc (octanol)/conc (water) (cLogP), S: water solubility in mol/L (cLogS), hydrogen acceptor (H.acceptors), hydrogen donors (H.donors), total surface area (total.surface.area),polar surface area (polar.surface.area),drug likeness (drug likeness), shape index (shape.index),molecular flexibility (molecular.flexibility),electronegative atoms (electronegative.atoms),rotatable bonds (rotatable.bonds),aromatic rings (aromatic.rings),aromatic atoms (aromatic.atoms),sp3 atoms (sp3 atoms), and symmetric atoms (symmetric.atoms)already reported .\",\n",
       "  '30840645',\n",
       "  '7719725'],\n",
       " ['Once a particle was detected, a discriminative correlation filter with channel and spatial reliability (CSR-DCF) was used to track particles between frames .',\n",
       "  '59222267',\n",
       "  '52197168'],\n",
       " ['Custom-fabricated imagers with nanostructures/microstructures designs have been investigated to improve the resolution of fluorescence lensless imaging .',\n",
       "  '52197168',\n",
       "  '59222267'],\n",
       " ['To test and evaluate our approach, we focus on the subpart of KB BIO 101 isolated for the KBGEN surface realisation shared task by .',\n",
       "  '5617711',\n",
       "  '16640049'],\n",
       " ['While offering more flexibility and expressiveness, these systems are difficult to adapt by non-NLG experts because they require the user to understand the architecture of the NLG systems .',\n",
       "  '5475067',\n",
       "  '9500998'],\n",
       " ['Another trend of work relevant to this paper is generation from databases using parallel corpora of data and text.',\n",
       "  '13402912',\n",
       "  '15118849'],\n",
       " ['Finally, recent work by the SWAT project 1 has focused on pro-ducing descriptions of ontologies that are both coherent and efficient .',\n",
       "  '9613237',\n",
       "  '504909'],\n",
       " ['Statistics for our dataset is presented in Table 1.Each abstract was annotated by two annotators.The task was to classify the relations between each possible pair of terms in each sentence in the abstract.The terms in the texts were already extracted.During the annotation, we followed the instructions proposed in .',\n",
       "  '227053552',\n",
       "  '190000105'],\n",
       " ['proposed to create a template for each relation type and then compute increased log probability of the sentences from these templates with the use of BERT as in .For example, a template for the relation \"LOCATED-IN\" might look like this -\"the <e1> is in the <e2>\".So if the first entity is \"toothbrush\" and the second is \"bathroom\", the sentence from the template will be \"the toothbrush is in the bathroom\".With the selected threshold of probability, it will be possible to separate the presence or absence of relation between two entities and also its type.',\n",
       "  '190000105',\n",
       "  '52118895'],\n",
       " ['With the introduction of large language models their use became one of the main methods of solving this problem.However, such methods require a lot of well-annotated data for training.Currently there are no datasets available for this task in a scientific field in Russian, and manual annotation takes a long time and requires the efforts of more than one person to objectively label the relations.Therefore, in this paper we decided to pay our special attention to zero-shot and few-shot approaches that do not require a lot of annotated data.There are some examples of them.',\n",
       "  '52967399',\n",
       "  '160025533'],\n",
       " ['Then we got an estimate of the probability of each sentence using the model GPT2 .After choosing the most probable pattern for each relation, we again compared the probability of sentences from these best templates.The most likely sentence would reflect the true relation between the terms.The schematic work of the method is presented in the Figure 1.To measure the probability we used the perplexity score.In general, this value can be described as the model uncertainty measure when predicting each of the next token, hence the lower the perplexity, the more certain the model in predicting this sequence.',\n",
       "  '160025533',\n",
       "  '160009395'],\n",
       " ['Using prototype vectors of relations The second approach for relation identification that we tried is based on the usage of the prototype vectors of relations.It can be attributed to few-shot approaches.First of all, we manually chose 138 best examples from the train part of the dataset to create a prototype vectors for each type of relations.In selecting the best examples we were guided by the following criterion: the example shows only one type of relations and has short context which includes only two terms of interest.Then we got the vectors of these of sentences.Vectors of sentences are the embeddings of CLS token from BERT .Each prototype vector is an average of the vectors of sentences reflecting each relation.Once these prototype vectors are obtained, they can be used to classify test examples.By computing the value of the cosine similarity of the example and the prototypes, we can determine which relation is most similar to this example.Schematic graphics that reflect the work of this method can be seen in Figure 2.',\n",
       "  '52967399',\n",
       "  '227053552'],\n",
       " ['* files released with previous versions of SCOP and SCOPe .',\n",
       "  '6829315',\n",
       "  '54466991'],\n",
       " ['Hundreds of additional entries are added to SCOPe each month, because after at least one structure from a SCOPe family has been classified by a human curator, most other structures from that family may be added automatically by our rigorously validated software pipeline .',\n",
       "  '14864309',\n",
       "  '16369816'],\n",
       " ['Tags were identified using PDB metadata (SE-QADV records) referring to cloning, expression, or purification tags at the N-or C-terminal of each chain, as described in more detail elsewhere .',\n",
       "  '12866879',\n",
       "  '226207258'],\n",
       " ['To facilitate automated algorithms developed or trained on the SCOPe database, we provide machineparseable annotations of the extent of a single repeat unit for all families of repeats in classes a to g. Tandem repeats are often also annotated in other databases, such as Pfam .',\n",
       "  '226207258',\n",
       "  '11078775'],\n",
       " ['Behavioural intention 3 The intention to use BOPIS continuously .',\n",
       "  '13954684',\n",
       "  '4080949'],\n",
       " ['To create this model, Venkatesh et al reviewed and synthesised aspects of the following models: the theory of reasoned action (TRA), the technology acceptance model (TAM), the motivational model, the theory of planned behaviour (TPB), the decomposed theory of planned behaviour, the combined TAM-TBP model (OTAM-CBT), the model of personal computer utilisation (MPCU), the innovation diffusion theory (IDT), and the social cognitive theory (SCT).',\n",
       "  '13954684',\n",
       "  '6928807'],\n",
       " ['Apart from these constructs, previous studies also found that innovativeness and trust have an effect .',\n",
       "  '12476939',\n",
       "  '18173161'],\n",
       " ['AXIN2 itself is a major target gene of WNT signalling.It acts in a negative feedback loop to limit and finetune the canonical WNT-signaling .The human AXIN2 gene is located on chromosome 17q24.1 and encompasses 10 coding exons that generate a protein of 843 amino acids (Fig.',\n",
       "  '16190212',\n",
       "  '6908182'],\n",
       " ['The implementation of Naïve Bayes in WEKA is based on the work of .',\n",
       "  '667586',\n",
       "  '17438917'],\n",
       " ['The set of people detected in the scene is then tracked over the space and time by the algorithm of .',\n",
       "  '16028462',\n",
       "  '2674272'],\n",
       " ['This conclusion is in agreement with recent intracranial EEG results showing similar object selective (including word selective) response dynamics in the human occipito-temporal cortex during a free viewing visual search task and static visual stimulation.',\n",
       "  '9607522',\n",
       "  '1225535'],\n",
       " ['The authors suggest that there are many areas where we are unlikely to find fundamental principles and that we should consider phenomenological, data-driven models that have practical value to be genuine intellectual accomplishments worthy of our satisfaction.',\n",
       "  '14300215',\n",
       "  '8179293'],\n",
       " ['Depth cameras have become an invaluable source of such reconstructions, and while the raw data from a RGB-D camera is typically a point cloud, voxels are used as intermediate representations in systems such as KinectFusion .',\n",
       "  '11830123',\n",
       "  '42427078'],\n",
       " ['Co-presence was measured by choosing the three co-presence items of BAIL , and we added two items asking about the believability of objects and characters in the voxel-based MR system (BoOC).',\n",
       "  '7367266',\n",
       "  '205045003'],\n",
       " ['The first eight items were chosen from the igroup presence questionnaire IPQ .',\n",
       "  '7480416',\n",
       "  '207424665'],\n",
       " ['Capture An earlier version of the system only supported co-located (not networked) users and used only one Kinect camera ; here the real environment is captured with three Kinect cameras (Figure 5).',\n",
       "  '4704820',\n",
       "  '18030939'],\n",
       " ['Interactive labeling is also supported via voxels, as in Semantic Paint where users can provide labels through gestures and voice.',\n",
       "  '9568132',\n",
       "  '37768224'],\n",
       " ['3.17 ED @ λ = 390nm 3 (Al) 1 (Ag) 0 (none) 18.67 26.',\n",
       "  '44095317',\n",
       "  '75139001'],\n",
       " ['With the rapid development of the inertial sensor technology, numerous researchers have applied sensors based on microelectromechanical systems (MEMS) to measure gait motions .',\n",
       "  '22663276',\n",
       "  '3107404'],\n",
       " ['Estos factores se han estudiado mediante diversas teorías o modelos provenientes de la psicología social, las ciencias de la información y las ciencias sociales (como la administración, la sociología o la economía), lo que ha permitido el desarrollo de modelos interdisciplinarios y modelos de implementación .',\n",
       "  '3138489',\n",
       "  '3468380'],\n",
       " ['Phylogenetic and Clade Analysis of the Citrus NBS Genes To avoid mutation saturation effects in nucleotide sequences over time which can lead to an underestimation of the number of mutation events due to higher substitution rates versus proteins, we used the protein sequences of NBS domains, which are the most conserved part of NBS genes, to construct a phylogenetic tree of NBS genes.We only selected NBS domain sequences longer than 200 amino acid residues and contain both P-loop and MHDV motifs.Finally, 442 C. clementina, 393 C. sinensis China, and 264 C. sinensis USA NBS domain sequences were used for phylogenetic analysis (S2 Table ).A Maximum Likelihood (ML) phylogenetic tree of these 1,099 NBS genes was then constructed using FastTree .As shown in Fig.',\n",
       "  '2854174',\n",
       "  '1246355'],\n",
       " ['Three Groups of Citrus NBS Genes We identified 442, 393 and 264 genes with full length NBS domains from C. clementina, C. sinensis China and C. sinensis USA reference genomes, respectively.There are also many genes with short NBS domains in three Citrus genomes.The Citrus NBS genes can be divided into three groups according to the phylogenetic tree of NBS domains: two of them without TIR domain (CC1 and CC2 groups) and the other group with TIR domain (TIR group).The number of non-TIR NBS genes is three times of the number of TIR NBS genes.In most of the TIR NBS genes, we can find the LRR domains defined in Pfam database .We only can identify LRR domains in small part of non-TIR NBS genes using Pfam LRR domain definition.However, we can find the LxxLs repeats in most of the non-TIR NBS genes as shown in motifs from MEME.This implied that there may be other types of LRR domain in Citrus non-TIR NBS genes.',\n",
       "  '1246355',\n",
       "  '47209500'],\n",
       " ['Transposon Identification All long terminal repeat (LTR) retrotransponsons in each Citrus genome were identified using LTR finder with default parameters (-o 3 -t 1 -e 1 -m 2 -u -2).Then, we used a script program to match the location of the LTR transposons to the NBS-LRR genes in each Citrus genome.',\n",
       "  '47209500',\n",
       "  '1387956'],\n",
       " ['We identified 20 motifs amongst the NBS genes in each of the three main phylogenetic groups separately using MEME SUITE .The motif width was set to between 6 and 50 for MEME.Then, we searched the motif structure of all genes in each group using MAST with default parameters (-ev 10-mt 0.0001).',\n",
       "  '3041302',\n",
       "  '13391175'],\n",
       " ['If Mode sel is set high, the counters will operate normally from 0 10 to 63 .',\n",
       "  '4612570',\n",
       "  '32845648'],\n",
       " ['The MSSIM can be calculated using the equations presented in .',\n",
       "  '231915596',\n",
       "  '115443863'],\n",
       " ['The Huffman encoder is used in the JPEG standard and it can provide good compression results, however the RLE is more suitable for real-time applications .',\n",
       "  '358483',\n",
       "  '67873234'],\n",
       " ['The compression is unacceptable if the PSNR is below 20dB .',\n",
       "  '213795298',\n",
       "  '21689319'],\n",
       " [\"In , the output of Tong et al 's algorithm was considered the best performing feature that can be used for image quality assessment using neural networks.\",\n",
       "  '49885649',\n",
       "  '14484920'],\n",
       " ['The mini-app or dwarf approach we describe above is also starting to become more widely prevalent, mainly because it provides a tractable route to both predict performance and develop and test performance optimisations for new hardware.Its applicability to weather and climate modelling follows directly from the fact that the source of the methodology was in disciplines for which similar flat multiphysics profiles were involved.The concept of attempting to represent a cross section of applications or application characteristics is becoming well recognised (e.g.',\n",
       "  '15525086',\n",
       "  '51903739'],\n",
       " ['In most codes now we see one of two strategies: either MPI is still used across both nodes and node-local cores, or MPI is used in conjunction with OpenMP .In the latter case OpenMP is used to run some parallel threads on a core (\"hyper threading\") or parallel threads on a socket, but the threads are sharing memory, and directives are used to identify where this can occur.However, this strategy of mixing explicit library calls (MPI) and compiler directives (OpenMP) does not always work better than MPI alone, not least because vendor implementations of MPI can implement on-node \"messages\" with highly optimised libraries exploiting shared memory with performance that can be hard to beat.Conversely, MPI decomposition cannot be changed without paying a synchronisation and data movement penalty, whereas OpenMP can exploit the dynamic scheduling of work and better handle load balancing around (for example) physics parameterisations such as precipitation.Hence, the modeller following a hybrid approach has many issues to consider: how many parallel threads per node (between zero and the number of hyper-threads supportable on the node) and what parts of the code can take advantage of this technique?There is a balance to be struck which depends on knowledge of the code, the hardware, and the quality of the vendor-supplied compilers and libraries, and this balance needs to be re-evaluated for each architecture and model version.',\n",
       "  '8975400',\n",
       "  '16648176'],\n",
       " ['Requirements Existing models are known to have high levels of software quality , and maintaining quality models in the face of increasing scientific demands on what is simulated, ensemble size, and resolution will be crucial.It is these that define scientific quality -a model could be very high performance and portable, but not be suitable for the scientific objectives, and there is a spectrum between \"not suitable\" and \"ideal\".In fact, some level of model quality is often compromised for performance; for example, resolution and complexity are routinely sacrificed to fit weather forecasts in a particular time window or to reach a requisite number of simulated years per day for climate.',\n",
       "  '18043249',\n",
       "  '27643723'],\n",
       " ['and .The latter rewrote (in another language) the radiation code in a widely used fast low-resolution climate model for a custom chip -an effort which took 2.5 person years and delivered a significant speed-up on the new processor over the original version.However, although this rewrite achieved significant speed-ups even in the CPU version, it is no longer being actively used, primarily because of the downstream technical and scientific burden associated with assimilating and maintaining (both technically and scientifically) a rewrite in another language.This downstream burden is one of the main issues associated with re-engineering code: if the code \"owner\" is not engaged, then rewrites are unlikely to survive.',\n",
       "  '16648176',\n",
       "  '16027141'],\n",
       " ['Thus, CL-MRSC schemes allowing for multiple receivers of a single message have been proposed by Selvi et al and others, who focused on various security requirements [27]- Fig.',\n",
       "  '19596897',\n",
       "  '212646785'],\n",
       " ['Ming et al developed multi-message MRSC for healthcare IoT environments.',\n",
       "  '221386341',\n",
       "  '212646785'],\n",
       " ['Taking the smart metering environment as an example, leakage of sensitive information, such as personal power usage, may make it possible to cut off power to the home by executing a command that stops the power supply , .',\n",
       "  '41726504',\n",
       "  '26538884'],\n",
       " ['Computation Cost The Wang et al satisfies the security requirements, but as signcryption can be used to generate only single messages, the number of messages is high.',\n",
       "  '218971095',\n",
       "  '13900209'],\n",
       " ['Specifically, the transformed features with relatively low dimensionality of an input sample is calculated based on the distance between itself and all the other samples using .',\n",
       "  '15895606',\n",
       "  '212819545'],\n",
       " ['Motivated by , a relative-feature representation for input samples is applied to reduce the dimensionality of original features.',\n",
       "  '209202120',\n",
       "  '118717396'],\n",
       " ['The losses generated during relative-feature representation, CNN encoding, and prediction process, are calculated using the designed cost function as addressed by (5), (7), and .',\n",
       "  '212819545',\n",
       "  '198179736'],\n",
       " ['(9) 15: end for 16: end for 17: end while 18: return M public dataset UNSW-NB15, generated by the Australian security laboratory for CPS , is applied to evaluate the general prediction performance of the proposed method.',\n",
       "  '27653988',\n",
       "  '2829149'],\n",
       " ['Li et al built a dual deep learning (DL) model with an energy auditing mechanism, to monitor and identify cyber and physical attacks in IoT environments.',\n",
       "  '86650353',\n",
       "  '15895606'],\n",
       " ['In recent years, network-based ITS has become a hot topic of scientific research, such as the ELM-ART system developed by Yang et al .',\n",
       "  '46064641',\n",
       "  '3663262'],\n",
       " ['Learners can solve many problems through computer-based dialogue counseling .',\n",
       "  '3663262',\n",
       "  '63382381'],\n",
       " ['Lemma 4 If the input noise meets |υ − υ 0 | ≤ ι, the following inequalities can be obtained in a finite time: |ϑ 1 − υ 0 | ≤ a 1 ι =l |ϱ 1 −υ 0 | ≤ b 1 ι 1 2 =ε wherel andε are both positive constants exclusively depended on the design constants of the FOSMD.',\n",
       "  '4892587',\n",
       "  '198135450'],\n",
       " ['For example, the dynamic surface control (DSC) method was employed to estimate the derivative of the virtual controller in .',\n",
       "  '67872788',\n",
       "  '204832856'],\n",
       " ['And then, the finite-time consensus tracking control was handled in for multi agent systems with prescribed performance and mismatched uncertainties.',\n",
       "  '216047760',\n",
       "  '203089131'],\n",
       " ['In the case of K computer, it is reported that better communication performance is obtained when using larger message size .',\n",
       "  '64085782',\n",
       "  '222133'],\n",
       " ['Actually, 6 measurements have been executed for each image resolution and number of composition nodes, however as similar to we ignored the first measurement which can have higher initialization overhead and might produce biased results.',\n",
       "  '4983155',\n",
       "  '64085782'],\n",
       " ['The performance evaluation graph presented in shows that Binary-Swap and some versions of Radix-k time almost doubled from 512 to 65,536 composition nodes, and in the case of Radix-k with k value equal to 32 the time almost tripled.',\n",
       "  '4983155',\n",
       "  '18188187'],\n",
       " ['2-3 Swap can be considered an extension for Binary-Swap to provide image composition of non-power of two number of composition nodes.',\n",
       "  '5875143',\n",
       "  '144322'],\n",
       " ['3 and Table 1, we see that the proposed method has an accuracy of linearization similar to the previous method , but the running time for the linearization is much shorten.',\n",
       "  '126925572',\n",
       "  '206593680'],\n",
       " ['This method has been expanded into a pseudoformal linearization method by using an automatic choosing function and Chebyshev expansion .',\n",
       "  '126925572',\n",
       "  '206593680'],\n",
       " ['Numerical experiments indicate that the performance of the proposed method is superior to that of the previous method .',\n",
       "  '126925572',\n",
       "  '206593680'],\n",
       " ['The second baseline is an encoder-decoder Transformer model , where the input sequence is the individual words in g added with their 1D positional encodings, and the output sequence is the 2D encoded observation s added with their 2D positional encodings.',\n",
       "  '13756489',\n",
       "  '67856232'],\n",
       " ['To study this, we separate goals into G ID and G OOD following the principle of leaving one attribute combination out (shown in Table 1 and similar to the \"visual\" split in ).',\n",
       "  '212658007',\n",
       "  '216868834'],\n",
       " ['This has led to an interest in whether this aspect can be leveraged to get better generalization on unseen goals made up of familiar terms .',\n",
       "  '11974467',\n",
       "  '237353084'],\n",
       " ['The metric is described in more detail in Appendix G Each architecture for S(s, g) was trained using D train for 200,000 iterations with the parameters in Appendix F. The IQM and 95% confidence interval across seeds and top-10 checkpoints are reported in Table 2 using the package and method provided by .',\n",
       "  '237353084',\n",
       "  '237257594'],\n",
       " ['Hence, the cooperation within NOMA users is proposed in and the outage probability of cooperative-NOMA (C-NOMA) is analyzed.',\n",
       "  '60846036',\n",
       "  '8021083'],\n",
       " ['Since NOMA provides a spectral efficient communication, the potential of NOMA for massive machine type communication (MMTC) led researchers to investigate NOMA involved systems and tremendous effort has been devoted to integrate NOMA in future wireless networks , .',\n",
       "  '13746058',\n",
       "  '60846036'],\n",
       " ['It is given for BPSK in , by utilizing [5, eq.', '57762457', '553424'],\n",
       " ['Then, the error probability of C-NOMA within two users is derived for quadrature phase shift keying (QPSK) and binary phase shift keying (BPSK) modulations .',\n",
       "  '57762457',\n",
       "  '553424'],\n",
       " ['INTRODUCTION N ON-orthogonal multiple access (NOMA) is introduced to serve multiple users on the same resource block by implementing superposition coding (SC) at transmitter and iterative successive interference canceler (SIC) at receivers .',\n",
       "  '26735476',\n",
       "  '1682687'],\n",
       " ['One (relatively minor) issue with solving d uncoupled problems is that one subsequently needs to reconcile the solutions to obtain a coherent global model over all the variables, although there are several ways to accomplish this .',\n",
       "  '12155197',\n",
       "  '88524020'],\n",
       " ['If a data analyst has access to additional information about potential latent variables (e.g., the latent variables take on non-negative or categorical values) or wishes to fit to models in which the latent variables have additional structure, one can design tighter convex regularizers than the nuclear norm .',\n",
       "  '51052',\n",
       "  '301178'],\n",
       " ['These are perhaps to be expected based on the theoretical analyses in , and it would be useful to combine and formalize these results in our context by showing that the Type-I and Type-II errors can be provably controlled under appropriate assumptions.',\n",
       "  '88524020',\n",
       "  '301178'],\n",
       " ['Now he will hire the machines on rent with the objective to complete the task with minimum total rental cost and zero idle time of machines .',\n",
       "  '407581',\n",
       "  '225717887'],\n",
       " ['Some situations come from sectors where machines used in production are less expensive but these cannot be stopped and restarted easily .',\n",
       "  '14201587',\n",
       "  '69302663'],\n",
       " ['Let C1 ij and C2 be the renting cost for one unit time of machines η and μ respectively .',\n",
       "  '14201587',\n",
       "  '62580144'],\n",
       " ['Indeed, the path length of the near infrared light and the NIRS sensitivity are both dependent on the scalp-to-cortex distance .',\n",
       "  '34111654',\n",
       "  '26721537'],\n",
       " ['The PFC is of particular interest since it plays a significant role in social interaction .',\n",
       "  '945353',\n",
       "  '15627151'],\n",
       " ['Besides these English data sets, we also obtained doubly-annotated POS data from the French Social Media Bank project .',\n",
       "  '7811096',\n",
       "  '855546'],\n",
       " ['If we pre-filter the data via Wiktionary and use an itemresponse model rather than majority voting, the agreement rises to 80.58%.',\n",
       "  '6617574',\n",
       "  '7811096']]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intermediate[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1147328it [00:12, 94089.65it/s] \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "data = []\n",
    "with open('/scratch/lamdo/s2orc/processed/citation_contexts_triplets/raw_cs_fullsize.tsv') as f:\n",
    "    for i, line in enumerate(tqdm(f)):\n",
    "        splitted_line = line.strip().split(\"\\t\")\n",
    "        data.append(splitted_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Most major cities in the U.S. now use high-resolution orthoimagery and LiDAR to produce their own building footprint datasets .',\n",
       "  'Multi-scale solution for building extraction from LiDAR and image data. ',\n",
       "  'Impact of urban form and design on mid-afternoon microclimate in Phoenix Local Climate Zones.'],\n",
       " ['In local studies, the use of vector data types is easier .',\n",
       "  'Towards Urban Fabrics Characterization Based on Buildings Footprints. ',\n",
       "  'Multi-scale solution for building extraction from LiDAR and image data.'],\n",
       " ['All arrays were + using the NumPy package .',\n",
       "  'The NumPy Array: A Structure for Efficient Numerical Computation. In the Python world, NumPy arrays are the standard representation for numerical data and enable efficient implementation of numerical computations in a high-level language. As this effort shows, NumPy performance can be improved through three techniques: vectorizing calculations, avoiding copying data in memory, and minimizing operation counts.',\n",
       "  'Evaluation of the impact of the surrounding urban morphology on building energy consumption.'],\n",
       " ['Recall is the fraction of the total amount of relevant instances that were actually retrieved .',\n",
       "  \"The Precision-Recall Plot Is More Informative than the ROC Plot When Evaluating Binary Classifiers on Imbalanced Datasets. Binary classifiers are routinely evaluated with performance measures such as sensitivity and specificity, and performance is frequently illustrated with Receiver Operating Characteristics (ROC) plots. Alternative measures such as positive predictive value (PPV) and the associated Precision/Recall (PRC) plots are used less frequently. Many bioinformatics studies develop and evaluate classifiers that are to be applied to strongly imbalanced datasets in which the number of negatives outweighs the number of positives significantly. While ROC plots are visually appealing and provide an overview of a classifier's performance across a wide range of specificities, one can ask whether ROC plots could be misleading when applied in imbalanced classification scenarios. We show here that the visual interpretability of ROC plots in the context of imbalanced datasets can be deceptive with respect to conclusions about the reliability of classification performance, owing to an intuitive but wrong interpretation of specificity. PRC plots, on the other hand, can provide the viewer with an accurate prediction of future classification performance due to the fact that they evaluate the fraction of true positives among positive predictions. Our findings have potential implications for the interpretation of a large number of studies that use ROC plots on imbalanced datasets.\",\n",
       "  'Evaluation of the impact of the surrounding urban morphology on building energy consumption.'],\n",
       " ['This grid structure is aligned with National Land Cover Database (NLCD) products (projected using Albers Equal Area Conic system), enabling researchers to combine or compare our products with standard national-scale datasets such as land cover, tree canopy cover, and urban imperviousness .',\n",
       "  'A new generation of the United States National Land Cover Database: Requirements, research priorities, design, and implementation strategies. ',\n",
       "  'Towards Urban Fabrics Characterization Based on Buildings Footprints.'],\n",
       " ['PCA analysis of replicates from sample A and B The relationships among spectra can be visualized by an unsupervised pattern recognition algorithm PCA .',\n",
       "  'Megavariate data analysis of mass spectrometric proteomics data using latent variable projection method. ',\n",
       "  'Clustering mass spectrometry data using order statistics.'],\n",
       " ['Lastly, polished genome sequences were re-aligned using MAFFT v7.427 and visualised using AliView v1.2.6 .',\n",
       "  'AliView: a fast and lightweight alignment viewer and editor for large datasets. Summary: AliView is an alignment viewer and editor designed to meet the requirements of next-generation sequencing era phylogenetic datasets. AliView handles alignments of unlimited size in the formats most commonly used, i.e. FASTA, Phylip, Nexus, Clustal and MSF. The intuitive graphical interface makes it easy to inspect, sort, delete, merge and realign sequences as part of the manual filtering process of large datasets. AliView also works as an easy-to-use alignment editor for small as well as large datasets. Availability and implementation: AliView is released as open-source software under the GNU General Public License, version 3.0 (GPLv3), and is available at GitHub (www.github.com/AliView). The program is cross-platform and extensively tested on Linux, Mac OS X and Windows systems. Downloads and help are available at http://ormbunkar.se/aliview Contact: anders.larsson@ebc.uu.se Supplementary information: Supplementary data are available at Bioinformatics online.',\n",
       "  'Molecular and phylogenetic analysis of Chikungunya virus in Central India during 2016 and 2017 outbreaks reveal high similarity with recent New Delhi and Bangladesh strains.'],\n",
       " ['Base-called FASTQ reads were then realigned to the draft consensus sequences, and primer sequences were again clipped prior to consensus correction, which was performed using Nanopolish variants v 0.11.1 (options -ploidy 1 -min-flanking-sequence 10).',\n",
       "  'A complete bacterial genome assembled de novo using only nanopore sequencing data. We have assembled de novo the Escherichia coli K-12 MG1655 chromosome in a single 4.6-Mb contig using only nanopore data. Our method has three stages: (i) overlaps are detected between reads and then corrected by a multiple-alignment process; (ii) corrected reads are assembled using the Celera Assembler; and (iii) the assembly is polished using a probabilistic model of the signal-level data. The assembly reconstructs gene order and has 99.5% nucleotide identity.',\n",
       "  'Comparative full genome analysis revealed E1: A226V shift in 2007 Indian Chikungunya virus isolates.'],\n",
       " ['Phylogenetic and mutational analysis The full length genomes were annotated using VIGOR software available at VIPR server (https://www.viprbrc.org/brc/vigorAnnotator.s pg?method¼ShowCleanInputPage&amp;decorator¼toga) .',\n",
       "  'VIGOR, an annotation program for small viral genomes. BackgroundThe decrease in cost for sequencing and improvement in technologies has made it easier and more common for the re-sequencing of large genomes as well as parallel sequencing of small genomes. It is possible to completely sequence a small genome within days and this increases the number of publicly available genomes. Among the types of genomes being rapidly sequenced are those of microbial and viral genomes responsible for infectious diseases. However, accurate gene prediction is a challenge that persists for decoding a newly sequenced genome. Therefore, accurate and efficient gene prediction programs are highly desired for rapid and cost effective surveillance of RNA viruses through full genome sequencing.ResultsWe have developed VIGOR (Viral Genome ORF Reader), a web application tool for gene prediction in influenza virus, rotavirus, rhinovirus and coronavirus subtypes. VIGOR detects protein coding regions based on sequence similarity searches and can accurately detect genome specific features such as frame shifts, overlapping genes, embedded genes, and can predict mature peptides within the context of a single polypeptide open reading frame. Genotyping capability for influenza and rotavirus is built into the program. We compared VIGOR to previously described gene prediction programs, ZCURVE_V, GeneMarkS and FLAN. The specificity and sensitivity of VIGOR are greater than 99% for the RNA viral genomes tested.ConclusionsVIGOR is a user friendly web-based genome annotation program for five different viral agents, influenza, rotavirus, rhinovirus, coronavirus and SARS coronavirus. This is the first gene prediction program for rotavirus and rhinovirus for public access. VIGOR is able to accurately predict protein coding genes for the above five viral types and has the capability to assign function to the predicted open reading frames and genotype influenza virus. The prediction software was designed for performing high throughput annotation and closure validation in a post-sequencing production pipeline.',\n",
       "  'MAFFT: a novel method for rapid multiple sequence alignment based on fast Fourier transform. A multiple sequence alignment program, MAFFT, has been developed. The CPU time is drastically reduced as compared with existing methods. MAFFT includes two novel techniques. (i) Homo logous regions are rapidly identified by the fast Fourier transform (FFT), in which an amino acid sequence is converted to a sequence composed of volume and polarity values of each amino acid residue. (ii) We propose a simplified scoring system that performs well for reducing CPU time and increasing the accuracy of alignments even for sequences having large insertions or extensions as well as distantly related sequences of similar length. Two different heuristics, the progressive method (FFT-NS-2) and the iterative refinement method (FFT-NS-i), are implemented in MAFFT. The performances of FFT-NS-2 and FFT-NS-i were compared with other methods by computer simulations and benchmark tests; the CPU time of FFT-NS-2 is drastically reduced as compared with CLUSTALW with comparable accuracy. FFT-NS-i is over 100 times faster than T-COFFEE, when the number of input sequences exceeds 60, without sacrificing the accuracy.'],\n",
       " ['Molecular clock reconstructions Molecular clock reconstructions (MCR) were performed using PhyloBayes 3.3 .',\n",
       "  'PhyloBayes 3: a Bayesian software package for phylogenetic reconstruction and molecular dating. MOTIVATION A variety of probabilistic models describing the evolution of DNA or protein sequences have been proposed for phylogenetic reconstruction or for molecular dating. However, there still lacks a common implementation allowing one to freely combine these independent features, so as to test their ability to jointly improve phylogenetic and dating accuracy.   RESULTS We propose a software package, PhyloBayes 3, which can be used for conducting Bayesian phylogenetic reconstruction and molecular dating analyses, using a large variety of amino acid replacement and nucleotide substitution models, including empirical mixtures or non-parametric models, as well as alternative clock relaxation processes.',\n",
       "  'A timeline for terrestrialization: consequences for the carbon cycle in the Palaeozoic.'],\n",
       " ['NTN Use Cases Non-terrestrial systems have been proposed to enable several applications, including weather forecasting, video surveillance, TV broadcast, remote sensing, and navigation, for several years.On the other hand, recent technological advances in the aerial/space sector have enabled the implementation of more sophisticated use cases, such as distributed computing and content broadcasting, service boosting for users in congested areas, eMBB in underserved areas, and multi-connectivity for service continuity via cellular networks .',\n",
       "  'Evolution of Non-Terrestrial Networks From 5G to 6G: A Survey. Non-terrestrial networks (NTNs) traditionally have certain limited applications. However, the recent technological advancements and manufacturing cost reduction opened up myriad applications of NTNs for 5G and beyond networks, especially when integrated into terrestrial networks (TNs). This article comprehensively surveys the evolution of NTNs highlighting their relevance to 5G networks and essentially, how it will play a pivotal role in the development of 6G ecosystem. We discuss important features of NTNs integration into TNs and the synergies by delving into the new range of services and use cases, various architectures, technological enablers, and higher layer aspects pertinent to NTNs integration. Moreover, we review the corresponding challenges arising from the technical peculiarities and the new approaches being adopted to develop efficient integrated ground-air-space (GAS) networks. Our survey further includes the major progress and outcomes from academic research as well as industrial efforts representing the main industrial trends, field trials, and prototyping towards the 6G networks.',\n",
       "  'A Novel 3D UAV Channel Model for A2G Communication Environments Using AoD and AoA Estimation Algorithms. In this article, we propose a three-dimensional (3D) multi-input multi-output (MIMO) channel model for air-to-ground (A2G) communications in unmanned aerial vehicles (UAV) environments, where the UAV transmitter and ground receiver are in motion in the air and on the ground, respectively. A novel angular estimation algorithm is proposed to estimate the real-time azimuth angle of departure (AAoD), elevation angle of departure (EAoD), azimuth angle of arrival (AAoA), and elevation angle of arrival (EAoA) based on the non-stationary nature of the channel model. In the model, we investigate the time-varying spatial cross-correlation functions (CCFs) and temporal auto-correlation functions (ACFs) with respect to the different moving directions and velocities of the UAV transmitter and ground receiver. Furthermore, we derive and study the Doppler power spectral densities (PSDs) and power delay profiles (PDPs) of the proposed channel model. Numerical results show that characteristics of the proposed channel model are very close to those of practical measurements, which provide a new and practical approach to evaluate the performance of next generation UAV-MIMO communication systems.'],\n",
       " ['Non-terrestrial technology has long been thought to support operations such as home delivery, weather forecasting, video surveillance, television transmission, remote sensing, and navigation.To enable more advanced use cases, such as Communication Resilience and Service Continuity, Global Satellite Overlay, Ubiquitous Internet of Things (IoT) Broadcasting, Advanced Backhauling, and Energy-Efficient Hybrid Multiplay, the recent technological advancements in the aerial/space industry have however made it possible for integration between terrestrial and non-terrestrial technologies.The few enabling technologies to support NTN in 6G for architecture, spectrum, antenna, and higher layers advancements, respectively, include gallium nitride (GaN), cognitive spectrum, multi-beam structures, and TCP spoofing and multiplexing .',\n",
       "  'Non-Terrestrial Networks in the 6G Era: Challenges and Opportunities. Many organizations recognize non-terrestrial networks (NTNs) as a key component to provide cost-effective and high-capacity connectivity in future 6th generation (6G) wireless networks. Despite this premise, there are still many questions to be answered for proper network design, including those associated to latency and coverage constraints. In this article, after reviewing research activities on NTNs, we present the characteristics and enabling technologies of NTNs in the 6G landscape and shed light on the challenges in the field that are still open for future research. As a case study, we evaluate the performance of an NTN scenario in which aerial/space vehicles use millimeter wave (mmWave) frequencies to provide access connectivity to on-the-ground mobile terminals as a function of different networking configurations.',\n",
       "  '5G NR Communication over GEO or LEO Satellite Systems: 3GPP RAN Higher Layer Standardization Aspects. Satellite communication has the potential to provide coverage over large geographical areas. This ubiquity enables potential for providing connectivity for currently unserved remote areas as well as for moving platforms such as airplanes and ships. However, many satellite systems were developed on private networks, each having their individual solution for the satellite systems and for the end user equipment (UE). In order to extent the satellite-industry-wide standards to global telecommunication standards, there are currently activities in the 3rd generation partnership project (3GPP) to study the feasibility and needed standard adaptations for the 5th generation (5G) new radio (NR) 3GPP standard. In this paper, we provide an overview of the ongoing activity in 3GPP RAN working groups with focus on RAN2 and RAN3 aspects for enabling NR communication over geosynchronous orbit (GEO) and non-GEO satellite systems.'],\n",
       " ['Data collection A case study investigates a contemporary phenomenon in depth and within its real-world context, focusing on answering how and why questions .',\n",
       "  'Qualitative Case Study Methodology: Study Design and Implementation for Novice Researchers. ',\n",
       "  'Interrelationships of the barriers to integrated flood risk management adaptation in Metro Manila, Philippines.'],\n",
       " ['Secondly, from the early 2010s, questions related to the underlying causes of barriers, their short-and long-term consequences, and internal dynamics have been raised, but they remain unanswered .',\n",
       "  'A framework to diagnose barriers to climate change adaptation. ',\n",
       "  'Barriers to climate change adaptation in the Netherlands. Review of recent literature on adaptation to climate change and general literature on policy processes shows that there are a large number of barriers that hamper the development and implementation of climate change adaptation strategies. To reduce and manage the number of barriers and combine both streams of literature, we propose seven clusters of barriers to adaptation. Little is known, however, about the relative importance of these barriers to climate change adaptation policies and practices. An online survey was conducted between March 2010 and July 2010 among 264 scientists, policymakers, and private actors from different sectors and levels who are involved in climate change adaptation projects and programmes in the Netherlands. The survey aimed to gather their experiences with, and perceptions of, the barriers identified in the literature and encountered in their daily work. Both climate-related and non-climate-related barriers were included in the survey. Data were subjected to both qualitative and quantitative analysis. A survey feedback workshop was organized to discuss the results with several of the survey respondents. Results of this study revealed that respondents considered conflicting timescales as the most important cluster of barriers to adaptation. Other highly ranked barriers include conflicting interests; lack of financial resources; unclear division of tasks and responsibilities; uncertain societal costs and future benefits; and fragmentation within and between scales of governance. Furthermore, the analysis demonstrated that scales matter in understanding the barriers to adaptation: actors from lowlevels of governance seem to consider the barriers as more severe than actors from high levels of governance.'],\n",
       " ['The existing literature has given limited attention to the influence of the barriers, for example, impeding progress from one stage to another or resulting in unintended consequences in adaptation policy processes .',\n",
       "  'A framework to diagnose barriers to climate change adaptation. ',\n",
       "  'Modeling and Analysis of Barriers to Climate Change Adaptation in Tehran. Since the impacts of climate change will last for many years, adaptation to this phenomenon should be prioritized in urban management plans. Although Tehran, the capital of Iran, has been subject to a variety of climate change impacts in recent years, appropriate adaptation measures to address them are yet to be taken. This study primarily aims to categorize the barriers to climate change adaptation in Tehran and analyze the way they interact with each other. The study was done in three steps: first, the focus group discussion (FGD) method was used to identify the barriers; next, the survey and the structural equation modeling (SEM) were used to validate the barriers, identify their importance, and examine their possible inter-relationships; and finally, the interpretive structural modeling (ISM) was applied to categorize and visualize the relationships between the barriers. Results show that barriers related to the ‘structure and culture of research’, ‘laws and regulations’, and ‘planning’ belong to the cluster of independent barriers and are of greater significance. The ‘social’ barrier and barriers related to ‘resources and resource management’ are identified as dependent barriers and are of lesser importance. Barriers related to ‘governance’, ‘awareness’, ‘education and knowledge’, ‘communication and interaction’, and ‘economy’ are identified at the intermediate cluster. The findings of this study can provide planners and decision makers with invaluable insights as to how to develop strategies for climate change adaptation in Tehran. Despite the scope of the study being confined to Tehran, its implications go far beyond this metropolis.'],\n",
       " ['Statistical methods and model tting Prior tting we excluded that are expressed at consistently low levels across all samples.',\n",
       "  \"It's DE-licious: A Recipe for Differential Expression Analyses of RNA-seq Experiments Using Quasi-Likelihood Methods in edgeR. \",\n",
       "  'Low Urinary Albumin Excretion in Astronauts during Space Missions.'],\n",
       " ['A small positive value was added to each raw count to avoid taking the logarithm of zero, and logCPM can be interpreted as a normalized count data by the corresponding total sample counts (in millions).We used the linear mixed model approach, tting the condition_treatment as the xed effect and the donor as the random effect by estimating the within-donor correlation.',\n",
       "  'Use of within-array replicate spots for assessing differential expression in microarray experiments. MOTIVATION Spotted arrays are often printed with probes in duplicate or triplicate, but current methods for assessing differential expression are not able to make full use of the resulting information. The usual practice is to average the duplicate or triplicate results for each probe before assessing differential expression. This results in the loss of valuable information about genewise variability.   RESULTS A method is proposed for extracting more information from within-array replicate spots in microarray experiments by estimating the strength of the correlation between them. The method involves fitting separate linear models to the expression data for each gene but with a common value for the between-replicate correlation. The method greatly improves the precision with which the genewise variances are estimated and thereby improves inference methods designed to identify differentially expressed genes. The method may be combined with empirical Bayes methods for moderating the genewise variances between genes. The method is validated using data from a microarray experiment involving calibration and ratio control spots in conjunction with spiked-in RNA. Comparing results for calibration and ratio control spots shows that the common correlation method results in substantially better discrimination of differentially expressed genes from those which are not. The spike-in experiment also confirms that the results may be further improved by empirical Bayes smoothing of the variances when the sample size is small.   AVAILABILITY The methodology is implemented in the limma software package for R, available from the CRAN repository http://www.r-project.org',\n",
       "  'Megalin/Cubulin-Lysosome-mediated Albumin Reabsorption Is Involved in the Tubular Cell Activation of NLRP3 Inflammasome and Tubulointerstitial Inflammation. Background: NLRP3 inflammasome activation is involved in albuminuria-induced renal injury. Results: The inhibition of megalin/cubilin or lysosomal cathepsin B reduced albuminuria-induced NLRP3 inflammasome activation. Conclusion: Megalin/cubilin and lysosome rupture is involved in albumin-triggered tubular injury and TI. Significance: This study provides novel insights into albuminuria-induced TI and implicates the active control of albuminuria as a critical strategy to halt the progression of CKD. Albuminuria contributes to the development and progression of chronic kidney disease by inducing tubulointerstitial inflammation (TI) and fibrosis. However, the exact mechanisms of TI in response to albuminuria are unresolved. We previously demonstrated that NLRP3 and inflammasomes mediate albumin-induced lesions in tubular cells. Here, we further investigated the role of endocytic receptors and lysosome rupture in NLRP3 inflammasome activation. A murine proteinuric nephropathy model was induced by albumin overload as described previously. The priming and activation signals for inflammasome complex formation were evoked simultaneously by albumin excess in tubular epithelial cells. The former signal was dependent on a albumin-triggered NF-κB pathway activation. This process is mediated by the endocytic receptor, megalin and cubilin. However, the silencing of megalin or cubilin inhibited the albumin-induced NLRP3 signal. Notably, subsequent lysosome rupture and the corresponding release of lysosomal hydrolases, especially cathepsin B, were observed in tubular epithelial cells exposed to albumin. Cathepsin B release and distribution are essential for NLRP3 signal activation, and inhibitors of cathepsin B suppressed the NLRP3 signal in tubular epithelial cells. Taken together, our findings suggest that megalin/cubilin and lysosome rupture are involved in albumin-triggered tubular injury and TI. This study provides novel insights into albuminuria-induced TI and implicates the active control of albuminuria as a critical strategy to halt the progression of chronic kidney disease.'],\n",
       " ['This Folk Theorem-type result hinges on the possibility of punishing unilateral deviations, as in e.g., .',\n",
       "  'Equilibria and threats in a fishery management game. A two-country fishery management game is considered in the realm of the theory of non-co-operative and co-operative differential games. Conditions which guarantee global asymptotic stability of the open-loop equilibrium trajectories are given and applied to a class of harvest games. Co-operative solutions of the two-country fishery management games are examined for the case of Kalai-Smorodinsky bargaining schemes. We show that the use of retaliation threats can transform a co-operative solution into an equilibrium memory strategy. Two numerical examples present an analysis of threat effectiveness. The ‘turnpike’ property, or global asymptotic stability of optimally controlled systems, is an important part of the computational approach used.',\n",
       "  'The Common Fisheries Policy. Origin, Evaluation and Future.'],\n",
       " ['Peng et al adopted convolutional neural network (CNN) for the task of modulation classification .',\n",
       "  'Modulation Classification Based on Signal Constellation Diagrams and Deep Learning. Deep learning (DL) is a new machine learning (ML) methodology that has found successful implementations in many application domains. However, its usage in communications systems has not been well explored. This paper investigates the use of the DL in modulation classification, which is a major task in many communications systems. The DL relies on a massive amount of data and, for research and applications, this can be easily available in communications systems. Furthermore, unlike the ML, the DL has the advantage of not requiring manual feature selections, which significantly reduces the task complexity in modulation classification. In this paper, we use two convolutional neural network (CNN)-based DL models, AlexNet and GoogLeNet. Specifically, we develop several methods to represent modulated signals in data formats with gridlike topologies for the CNN. The impacts of representation on classification performance are also analyzed. In addition, comparisons with traditional cumulant and ML-based algorithms are presented. Experimental results demonstrate the significant performance advantage and application feasibility of the DL-based approach for modulation classification.',\n",
       "  'Blind Identification of Spatial Multiplexing and Alamouti Space-Time Block Code via Kolmogorov-Smirnov (K-S) Test.'],\n",
       " ['Similar approaches have been adopted in modulation classification with good effect , .',\n",
       "  'Blind Digital Modulation Identification for Spatially-Correlated MIMO Systems. ',\n",
       "  'Modulation Classification in MIMO Systems.'],\n",
       " ['A combination of moment features and likelihood maximization has been experimented for a balance between classification accuracy and computational complexity in .',\n",
       "  'Automatic Modulation Classification Using Moments and Likelihood Maximization. ',\n",
       "  'Modulation Classification Based on Signal Constellation Diagrams and Deep Learning. Deep learning (DL) is a new machine learning (ML) methodology that has found successful implementations in many application domains. However, its usage in communications systems has not been well explored. This paper investigates the use of the DL in modulation classification, which is a major task in many communications systems. The DL relies on a massive amount of data and, for research and applications, this can be easily available in communications systems. Furthermore, unlike the ML, the DL has the advantage of not requiring manual feature selections, which significantly reduces the task complexity in modulation classification. In this paper, we use two convolutional neural network (CNN)-based DL models, AlexNet and GoogLeNet. Specifically, we develop several methods to represent modulated signals in data formats with gridlike topologies for the CNN. The impacts of representation on classification performance are also analyzed. In addition, comparisons with traditional cumulant and ML-based algorithms are presented. Experimental results demonstrate the significant performance advantage and application feasibility of the DL-based approach for modulation classification.'],\n",
       " ['Secondly, empirical distribution is evaluated from the features obtained from received samples using .',\n",
       "  'Blind Identification of Spatial Multiplexing and Alamouti Space-Time Block Code via Kolmogorov-Smirnov (K-S) Test. ',\n",
       "  'Automatic Modulation Classification Using Deep Learning Based on Sparse Autoencoders With Nonnegativity Constraints.'],\n",
       " ['Distribution test based methods have proven to be computationally efficient in SISO systems , - .',\n",
       "  'Fast and Robust Modulation Classification via Kolmogorov-Smirnov Test. ',\n",
       "  'Blind Modulation Classification Algorithm for Single and Multiple-Antenna Systems Over Frequency-Selective Channels.'],\n",
       " ['Zhang et al proposed a dictionary learning based modulation classifier by training with known signal and classifying unknown signals with their sparse representation on the trained dictionary , .',\n",
       "  'Data Driven Automatic Modulation Classification via Dictionary Learning. Automatic modulation classification (AMC) is the process of identifying the modulation format of the received signal. It is generally a difficult task due to the limited knowledge of the signal. In this letter, we propose a data driven dictionary-learning-based AMC framework, where we first use the known training signals to train the dictionary set and then classify the unknown modulation format via certain sparse representations, for which we design a dictionary-learning-based algorithm called block coordinate descent dictionary learning. Simulation results show that the proposed method out performs other existing approaches, achieving higher accuracy with a less training time.',\n",
       "  'Blind Digital Modulation Identification for Spatially-Correlated MIMO Systems.'],\n",
       " ['Dulek proposed a likelihood based method in combination with an online channel estimator .',\n",
       "  'Online Hybrid Likelihood Based Modulation Classification Using Multiple Sensors. ',\n",
       "  'Blind Modulation Classification Algorithm for Single and Multiple-Antenna Systems Over Frequency-Selective Channels.'],\n",
       " ['Similar feature based methods have also be explored by Han et al .',\n",
       "  'Low Complexity Automatic Modulation Classification Based on Order Statistics. ',\n",
       "  'Online Hybrid Likelihood Based Modulation Classification Using Multiple Sensors.'],\n",
       " ['Mühlhaus et al developed a feature based MIMO modulation classifier using high-order statistics .',\n",
       "  'A Low Complexity Modulation Classification Algorithm for MIMO Systems. ',\n",
       "  'Automatic Modulation Classification Using Deep Learning Based on Sparse Autoencoders With Nonnegativity Constraints.'],\n",
       " ['Kanterakis and Su tried to reduce the computational complexity of the likelihood based classifier by modifying the likelihood function .',\n",
       "  'Modulation Classification in MIMO Systems. ',\n",
       "  'Cramer–von Mises and Anderson‐Darling goodness of fit tests for extreme value distributions with unknown parameters.'],\n",
       " ['In , the authors tried to predict volume using Partial Least Squares (PLS) and Support Vector Regression (SVR).',\n",
       "  'Daily Volume Forecasting using High Frequency Predictors. Daily volume is an important feature when it comes to financial market structure. Effective daily volume forecasting can help areas such as portfolio management and algorithm trading. Intraday updates of daily volume forecasts can explore high frequency data to provide more accurate forecasts. Previous work on daily volume forecasting usually use Bayesian methods. In our work, we approach the problem of daily volume forecasting using the intraday information. Forecasting is accomplished by the use of two machine learning predictors: Support Vector Regression (SVR) and Partial Least Squares (PLS). We empirically test our method using the top nine high liquidity Bovespa traded stocks. Our metrics are the percentage error and the relative error reduction against a naive strategy. Our results show that SVR and PLS provide accurate forecasts. Moreover, the forecasting accuracy improves throughout the day as more intraday information is available.',\n",
       "  'Support-Vector Networks.'],\n",
       " ['In , LSTM networks employ multiplicative gate units to achieve this, adding a memory cell and gate units to the network.',\n",
       "  'Long Short-Term Memory. ',\n",
       "  'The Relation between Price Changes and Trading Volume: A Survey.'],\n",
       " ['One example is , where the authors predicted future options prices using conventional pricing techniques combined with two learning models: Neural Networks and Support Vector Regression.',\n",
       "  'Improving option price forecasts with neural networks and support vector regressions. ',\n",
       "  'Learning long-term dependencies with gradient descent is difficult. Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems. However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases. These results expose a trade-off between efficient learning by gradient descent and latching on information for long periods. Based on an understanding of this problem, alternatives to standard gradient descent are considered.'],\n",
       " ['Selection of Reference Compounds with One or More AOX-AINF-AAG-ASEN Activities through Hierarchical Analysis The Simplified Molecule Input Line Entry System (SMILES) of the compounds with AOX-AINF-AAG-ASEN activity included in the drug dataset was examined in September 2022 in both PubChem and in the SwissADME server (http://www.swissadme.chaccessed on 15 August 2022).We used Osiris Data Warrior V5.2.1 to analyze the physicochemical properties and molecular descriptors.The physicochemical properties and molecular descriptors determined in this work included G-protein-coupled receptors (GPCR) ligand (GPCR.Ligand), ion channel modulator (ion.channel.modulator),kinase inhibitor (kinase.inhibitor),nuclear receptor ligand (nuclear.receptor.ligand),protease inhibitor (protease.inhibitor),enzyme inhibitor (enzyme.inhibitor),number of violations (nviolations), number of atoms (natoms), log k p-values in cm/s (log.Kp..cm.s.',\n",
       "  'PubChem Substance and Compound databases. PubChem (https://pubchem.ncbi.nlm.nih.gov) is a public repository for information on chemical substances and their biological activities, launched in 2004 as a component of the Molecular Libraries Roadmap Initiatives of the US National Institutes of Health (NIH). For the past 11 years, PubChem has grown to a sizable system, serving as a chemical information resource for the scientific research community. PubChem consists of three inter-linked databases, Substance, Compound and BioAssay. The Substance database contains chemical information deposited by individual data contributors to PubChem, and the Compound database stores unique chemical structures extracted from the Substance database. Biological activity data of chemical substances tested in assay experiments are contained in the BioAssay database. This paper provides an overview of the PubChem Substance and Compound databases, including data sources and contents, data organization, data submission using PubChem Upload, chemical structure standardization, web-based interfaces for textual and non-textual searches, and programmatic access. It also gives a brief description of PubChem3D, a resource derived from theoretical three-dimensional structures of compounds in PubChem, as well as PubChemRDF, Resource Description Framework (RDF)-formatted PubChem data for data sharing, analysis and integration with information contained in other databases.',\n",
       "  'Bioinformatics and Drug Discovery.'],\n",
       " ['Histogram of oriented gradients (HOG) and support vector machine (SVM) were used to detect particles in the lensless imaging mode .',\n",
       "  'Improved Face Recognition Rate Using HOG Features and SVM Classifier. A novel face recognition algorithm is presented in this paper. Histogram of Oriented Gradient features are extracted both for the test image and also for the training images and given to the Support Vector Machine classifier. The detailed steps of HOG feature extraction and the classification using SVM is presented. The algorithm is compared with the Eigen feature based face recognition algorithm. The proposed algorithm and PCA are verified using 8 different datasets. Results show that in all the face datasets the proposed algorithm shows higher face recognition rate when compared with the traditional Eigen feature based face recognition algorithm. There is an improvement of 8.75% face recognition rate when compared with PCA based face recognition algorithm. The experiment is conducted on ORL database with 2 face images for testing and 8 face images for training for each person. Three performance curves namely CMC, EPC and ROC are considered. The curves show that the proposed algorithm outperforms when compared with PCA algorithm. IndexTerms: Facial features, Histogram of Oriented Gradients, Support Vector Machine, Principle Component Analysis.',\n",
       "  'Lensfree holographic imaging for on-chip cytometry and diagnostics. We experimentally illustrate a lensfree holographic imaging platform to perform on-chip cytometry. By controlling the spatial coherence of the illumination source, we record a 2D holographic diffraction pattern of each cell or micro-particle on a chip using a high resolution sensor array that has approximately 2 microm pixel size. The recorded holographic image is then processed by using a custom developed decision algorithm for matching the detected hologram texture to existing library images for on-chip characterization and counting of a heterogeneous solution of interest. The holographic diffraction signature of any microscopic object is significantly different from the classical diffraction pattern of the same object. It improves the signal to noise ratio and the signature uniformity of the cell patterns; and also exhibits much better sensitivity for on-chip imaging of weakly scattering phase objects such as small bacteria or cells. We verify significantly improved performance of this holographic on-chip cytometry approach by automatically characterizing heterogeneous solutions of red blood cells, yeast cells, E. coli and various sized micro-particles without the use of any lenses or microscope objectives. This lensless on-chip holography platform will especially be useful for point-of-care cytometry and diagnostics applications involving e.g., infectious diseases such as HIV or malaria.'],\n",
       " ['However, the resolution of lensless fluorescence imaging is limited by the point spread function (PSF), which is dominated by the distance between particles/cells and image sensor .',\n",
       "  'Lensless Imaging and Sensing. High-resolution optical microscopy has traditionally relied on high-magnification and high-numerical aperture objective lenses. In contrast, lensless microscopy can provide high-resolution images without the use of any focusing lenses, offering the advantages of a large field of view, high resolution, cost-effectiveness, portability, and depth-resolved three-dimensional (3D) imaging. Here we review various approaches to lensless imaging, as well as its applications in biosensing, diagnostics, and cytometry. These approaches include shadow imaging, fluorescence, holography, superresolution 3D imaging, iterative phase recovery, and color imaging. These approaches share a reliance on computational techniques, which are typically necessary to reconstruct meaningful images from the raw data captured by digital image sensors. When these approaches are combined with physical innovations in sample preparation and fabrication, lensless imaging can be used to image and sense cells, viruses, nanoparticles, and biomolecules. We conclude by discussing several ways in which lensless imaging and sensing might develop in the near future.',\n",
       "  'Using digital flow cytometry to assess the degradation of three cyanobacteria species after oxidation processes.'],\n",
       " ['Once a particle was detected, a discriminative correlation filter with channel and spatial reliability (CSR-DCF) was used to track particles between frames .',\n",
       "  'The Sixth Visual Object Tracking VOT2018 Challenge Results. ',\n",
       "  'Lensfree super-resolution holographic microscopy using wetting films on a chip. We investigate the use of wetting films to significantly improve the imaging performance of lensfree pixel super-resolution on-chip microscopy, achieving < 1 µm spatial resolution over a large imaging area of ~24 mm2. Formation of an ultra-thin wetting film over the specimen effectively creates a micro-lens effect over each object, which significantly improves the signal-to-noise-ratio and therefore the resolution of our lensfree images. We validate the performance of this approach through lensfree on-chip imaging of various objects having fine morphological features (with dimensions of e.g., ≤0.5 µm) such as Escherichia coli (E. coli), human sperm, Giardia lamblia trophozoites, polystyrene micro beads as well as red blood cells. These results are especially important for the development of highly sensitive field-portable microscopic analysis tools for resource limited settings.'],\n",
       " ['Custom-fabricated imagers with nanostructures/microstructures designs have been investigated to improve the resolution of fluorescence lensless imaging .',\n",
       "  \"Microfluidic Based Optical Microscopes on Chip. Last decade's advancements in optofluidics allowed obtaining an ever increasing integration of different functionalities in lab on chip devices to culture, analyze, and manipulate single cells and entire biological specimens. Despite the importance of optical imaging for biological sample monitoring in microfluidics, imaging is traditionally achieved by placing microfluidics channels in standard bench‐top optical microscopes. Recently, the development of either integrated optical elements or lensless imaging methods allowed optical imaging techniques to be implemented in lab on chip systems, thus increasing their automation, compactness, and portability. In this review, we discuss known solutions to implement microscopes on chip that exploit different optical methods such as bright‐field, phase contrast, holographic, and fluorescence microscopy.\",\n",
       "  'Identification of 72 phytoplankton species by radial basis function neural network analysis of flow cytometric data. Radial basis function artificial neural networks (ANNs) were trained to discriminate between  phytoplankton species based on 7 flow cytometric parameters measured on axenic cultures.  Comparison was made between the performance of networks restricted to using radially-symmetric  basis functions and networks using more general arbitrarily oriented ellipso~dal basis functions, with  the latter proving significantly superior in performance. ANNs trained on 62, 54 and 72 taxa identified  them with respectively 77, 73 and 70% overall success. As well as high success in identification, high  confidence of correct identification was also achieved. Misidentifications resulted from overlap of character  distributions. Improved overall identification success can be achieved by grouping together species  with similar character distributions. This can be done within genera or based on groupings indicated  in dendrograms constructed for the data on all species. When an ANN trained on 1 data set was  tested with data on cells grown under different light conditions, overall successful identification was  low (<20%), but when an ANN was trained on a combined data set identification success was high  (>?0%). Clearly it is essential to include data on cells covering the whole spectrum of biological variatlon.  Ways of obtaining data for training ANNs to identify phytoplankton from field samples are discussed.'],\n",
       " ['Lensless imaging provides an alternative microscopic approach, in which the shadow of a sample is recorded on an image sensor directly .',\n",
       "  'A review of recent progress in lens-free imaging and sensing. ',\n",
       "  'A review of harmful algal blooms and their apparent global increase. 1. NEILSON A.H. & LEWIN R.A. 1974. The uptake and uti\\xad lization of organic carbon by algae: an essay in compar\\xad ative biochemistry (including the addenda by N.J. Antia). Phycologia 13: 227-264. 2. PROCfOR V.W. 1975. The nature of charophyte species. Phycologia 14: 97-113. 3. HOEK C. VAN DEN 1975. Phytogeographic provinces along the coasts of the northern Atlantic Ocean. Phycologia 14: 317-330. 4. JO HANSEN H. W. 1976. Current status of generic concepts in coralline algae (Rhodophyta). Phycologia 15: 221-244. 5. W YNNE M.J. & LoISEAUX S. 1976. Recent advances in life history studies of the Phaeophyta. Phycologia 15: 435452. 6. STARKS T.L., SHUBERT L.E. & T RAINOR F.R. 1981. Ecol\\xad ogy of soil algae: a review. Phycologia 20: 65-80. 7. MOESTRUP 0. 1982. Flagellar structure in algae. A review with observations particularly on the Chrysophyceae, Phaeophyceae (Fucophyceae), Euglenophyceae and Reck\\xad ertia. Phycologia 21: 427-528. 8. WOELKERLING WM J. 1983. The Audouinella (Acrochae\\xad tium-Rhodochorton) complex (Rhodophyta): present per\\xad spectives. Phycologia 22: 59-92. 9. STEIN J.R. & BORDEN c.A. 1984. Causative and beneficial algae in human disease conditions: a review. Phycologia 23: 485-50 1. 10. HOSHAW R. W. & MCCOURT R.M. 1988. The Zygnema\\xad taceae (Chlorophyta): a twenty-year update of research. Phycologia 27: 511-548. II. ANTIA N.J., H ARRI SON P.J. & OLIVEIRA L. 1991. The role of dissolved organic nitrogen in phytoplankton nutrition, cell biology and ecology. Phycologia 30: 1-89. 12. KAPRAUN D.F. 1993. Karyology of marine green algae. Phycologia 32: 1-21.'],\n",
       " ['A fluorescence lensless setup has been reported by inserting an emission filter between image sensor and fluidic channel .',\n",
       "  'Lensless Imaging and Sensing. High-resolution optical microscopy has traditionally relied on high-magnification and high-numerical aperture objective lenses. In contrast, lensless microscopy can provide high-resolution images without the use of any focusing lenses, offering the advantages of a large field of view, high resolution, cost-effectiveness, portability, and depth-resolved three-dimensional (3D) imaging. Here we review various approaches to lensless imaging, as well as its applications in biosensing, diagnostics, and cytometry. These approaches include shadow imaging, fluorescence, holography, superresolution 3D imaging, iterative phase recovery, and color imaging. These approaches share a reliance on computational techniques, which are typically necessary to reconstruct meaningful images from the raw data captured by digital image sensors. When these approaches are combined with physical innovations in sample preparation and fabrication, lensless imaging can be used to image and sense cells, viruses, nanoparticles, and biomolecules. We conclude by discussing several ways in which lensless imaging and sensing might develop in the near future.',\n",
       "  'A novel method for cell counting of Microcystis colonies in water resources using a digital imaging flow cytometer and microscope. Microcystis sp. is one of the most common harmful cyanobacteria that release toxic substances. Counting algal cells is often used for effective control of harmful algal blooms. However, Microcystis sp. is commonly observed as a colony, so counting individual cells is challenging, as it requires significant time and labor. It is urgent to develop an accurate, simple, and rapid method for counting algal cells for regulatory purposes, estimating the status of blooms, and practicing proper management of water resources. The flow cytometer and microscope (FlowCAM), which is a dynamic imaging particle analyzer, can provide a promising alternative for rapid and simple cell counting. However, there is no accurate method for counting individual cells within a Microcystis colony. Furthermore, cell counting based on two-dimensional images may yield inaccurate results and underestimate the number of algal cells in a colony. In this study, a three-dimensional cell counting approach using a novel model algorithm was developed for counting individual cells in a Microcystis colony using a FlowCAM. The developed model algorithm showed satisfactory performance for Microcystis sp. cell counting in water samples collected from two rivers, and can be used for algal management in fresh water systems.'],\n",
       " ['Histogram of oriented gradients (HOG) and support vector machine (SVM) were used to detect particles in the lensless imaging mode .',\n",
       "  'Improved Face Recognition Rate Using HOG Features and SVM Classifier. A novel face recognition algorithm is presented in this paper. Histogram of Oriented Gradient features are extracted both for the test image and also for the training images and given to the Support Vector Machine classifier. The detailed steps of HOG feature extraction and the classification using SVM is presented. The algorithm is compared with the Eigen feature based face recognition algorithm. The proposed algorithm and PCA are verified using 8 different datasets. Results show that in all the face datasets the proposed algorithm shows higher face recognition rate when compared with the traditional Eigen feature based face recognition algorithm. There is an improvement of 8.75% face recognition rate when compared with PCA based face recognition algorithm. The experiment is conducted on ORL database with 2 face images for testing and 8 face images for training for each person. Three performance curves namely CMC, EPC and ROC are considered. The curves show that the proposed algorithm outperforms when compared with PCA algorithm. IndexTerms: Facial features, Histogram of Oriented Gradients, Support Vector Machine, Principle Component Analysis.',\n",
       "  'THE ACTION SPECTRUM, ABSORPTANCE AND QUANTUM YIELD OF PHOTOSYNTHESIS IN CROP PLANTS.'],\n",
       " ['The corresponding cropped images captured in the lensless imaging mode are shown in Figure 6d-f. For better visualization of cropped images captured in the lensless imaging mode, holographic reconstruction was performed with angular spectrum method .',\n",
       "  'Practical algorithms for simulation and reconstruction of digital in-line holograms. Here we present practical methods for simulation and reconstruction of in-line digital holograms recorded with plane and spherical waves. The algorithms described here are applicable to holographic imaging of an object exhibiting absorption as well as phase-shifting properties. Optimal parameters, related to distances, sampling rate, and other factors for successful simulation and reconstruction of holograms are evaluated and criteria for the achievable resolution are worked out. Moreover, we show that the numerical procedures for the reconstruction of holograms recorded with plane and spherical waves are identical under certain conditions. Experimental examples of holograms and their reconstructions are also discussed.',\n",
       "  'Growth promotion of three microalgae, Chlamydomonas reinhardtii, Chlorella vulgaris and Euglena gracilis, by in situ indigenous bacteria in wastewater effluent. BackgroundMicroalgae are a promising biomass feedstock for biofuels production. The use of wastewater effluent as a nutrient medium would improve the economics of microalgal biofuels production. Bacterial communities in aquatic environments may either stimulate or inhibit microalgal growth. Microalgal productivity could be enhanced if the positive effects of indigenous bacteria could be exploited. However, much is unknown about the effects of indigenous bacteria on microalgal growth and the characteristics of bacterial communities associated with microalgae in microalgae–effluent culture. To assess the effects of the indigenous bacteria in wastewater effluent on microalgal growth, three microalgae, Chlamydomonas reinhardtii, Chlorella vulgaris, and Euglena gracilis, were cultured in two municipal wastewater effluents and one swine wastewater effluent with and without indigenous bacteria for 7\\xa0days.ResultsAll microalgae grew better in all effluents with indigenous bacteria than without bacteria. Biomass production of C. reinhardtii, C. vulgaris, and E. gracilis increased\\u2009>\\u20091.5, 1.8–2.8, and >\\u20092.1-fold, respectively, compared to the axenic cultures of each microalga. The in situ indigenous bacterial communities in the effluents therefore promoted the growth of the three microalgae during 7-day cultures. Furthermore, the total numbers of bacterial 16S rRNA genes in the 7-day microalgae–effluent cultures were 109‒793 times the initial numbers. These results suggest that the three microalgae produced and supplied organic carbon that supported bacterial growth in the effluent. At the phylum and class levels, Proteobacteria (Alphaproteobacteria and Betaproteobacteria) and Bacteroidetes (Sphingobacteriia and Saprospirae) were selectively enriched in all microalgae–effluent cultures. The enriched core bacterial families and genera were functions of the microalgal species and effluents. These results suggest that certain members of the bacterial community promote the growth of their “host” microalgal species.ConclusionTo enhance their own growth, microalgae may be able to selectively stimulate specific bacterial groups from among the in situ indigenous bacterial community found in wastewater effluent (i.e., microalgae growth-promoting bacteria: MGPB). The MGPB from effluent cultures could be used as “probiotics” to enhance microalgal growth in effluent culture. Wastewater effluent may therefore be a valuable resource, not only of nutrients, but also of MGPB to enable more efficient microalgal biomass production.'],\n",
       " ['Recently, several techniques have been investigated to improve the resolution of fluorescence lensless imaging including hardware designs and computational algorithms .',\n",
       "  'Lensless Imaging and Sensing. High-resolution optical microscopy has traditionally relied on high-magnification and high-numerical aperture objective lenses. In contrast, lensless microscopy can provide high-resolution images without the use of any focusing lenses, offering the advantages of a large field of view, high resolution, cost-effectiveness, portability, and depth-resolved three-dimensional (3D) imaging. Here we review various approaches to lensless imaging, as well as its applications in biosensing, diagnostics, and cytometry. These approaches include shadow imaging, fluorescence, holography, superresolution 3D imaging, iterative phase recovery, and color imaging. These approaches share a reliance on computational techniques, which are typically necessary to reconstruct meaningful images from the raw data captured by digital image sensors. When these approaches are combined with physical innovations in sample preparation and fabrication, lensless imaging can be used to image and sense cells, viruses, nanoparticles, and biomolecules. We conclude by discussing several ways in which lensless imaging and sensing might develop in the near future.',\n",
       "  'Water monitoring: automated and real time identification and classification of algae using digital microscopy. Microalgae are unicellular photoautotrophs that grow in any habitat from fresh and saline water bodies, to hot springs and ice. Microalgae can be used as indicators to monitor water ecosystem conditions. These organisms react quickly and predictably to a broad range of environmental stressors, thus providing early signals of a changing environment. When grown extensively, microalgae may produce harmful effects on marine or freshwater ecology and fishery resources. Rapid and accurate recognition and classification of microalgae is one of the most important issues in water resource management. In this paper, a methodology for automatic and real time identification and enumeration of microalgae by means of image analysis is presented. The methodology is based on segmentation, shape feature extraction, pigment signature determination and neural network grouping; it attained 98.6% accuracy from a set of 53,869 images of 23 different microalgae representing the major algal phyla. In our opinion this methodology partly overcomes the lack of automated identification systems and is on the forefront of developing a computer-based image processing technique to automatically detect, recognize, identify and enumerate microalgae genera and species from all the divisions. This methodology could be useful for an appropriate and effective water resource management.'],\n",
       " ['Once a particle was detected, a discriminative correlation filter with channel and spatial reliability (CSR-DCF) was used to track particles between frames .',\n",
       "  'The Sixth Visual Object Tracking VOT2018 Challenge Results. ',\n",
       "  'A deep learning-enabled portable imaging flow cytometer for cost-effective, high-throughput, and label-free analysis of natural water samples. We report a deep learning-enabled field-portable and cost-effective imaging flow cytometer that automatically captures phase-contrast color images of the contents of a continuously flowing water sample at a throughput of 100\\u2009mL/h. The device is based on partially coherent lens-free holographic microscopy and acquires the diffraction patterns of flowing micro-objects inside a microfluidic channel. These holographic diffraction patterns are reconstructed in real time using a deep learning-based phase-recovery and image-reconstruction method to produce a color image of each micro-object without the use of external labeling. Motion blur is eliminated by simultaneously illuminating the sample with red, green, and blue light-emitting diodes that are pulsed. Operated by a laptop computer, this portable device measures 15.5\\u2009cm\\u2009×\\u200915\\u2009cm\\u2009×\\u200912.5\\u2009cm, weighs 1\\u2009kg, and compared to standard imaging flow cytometers, it provides extreme reductions of cost, size and weight while also providing a high volumetric throughput over a large object size range. We demonstrated the capabilities of this device by measuring ocean samples at the Los Angeles coastline and obtaining images of its micro- and nanoplankton composition. Furthermore, we measured the concentration of a potentially toxic alga (Pseudo-nitzschia) in six public beaches in Los Angeles and achieved good agreement with measurements conducted by the California Department of Public Health. The cost-effectiveness, compactness, and simplicity of this computational platform might lead to the creation of a network of imaging flow cytometers for large-scale and continuous monitoring of the ocean microbiome, including its plankton composition. A portable device that combines holographic imaging with artificial intelligence can rapidly detect potentially harmful algae in ocean water. Aydogan Ozcan, Zoltan Gorocs and colleagues from the University of California Los Angeles in the United States developed an inexpensive flow cytometer that pumps water samples containing tiny marine organisms, past an LED chip pulsing red, blue, and green light simultaneously. Deep learning algorithms trained to recognize background signals automatically analyze the holographic interference patterns created by the marine organisms and rapidly generate color images with microscale resolution. Sample throughput is boosted 10-fold over conventional imaging flow cytometry by avoiding the use of lenses. Using a lightweight and inexpensive prototype, the team monitored plankton levels at six public beaches and detected a likely toxic organism, the algae Pseudo-nitzschia, at levels matching those from public health laboratories.'],\n",
       " ['Histogram of oriented gradients (HOG) and support vector machine (SVM) were used to detect particles in the lensless imaging mode .',\n",
       "  'Improved Face Recognition Rate Using HOG Features and SVM Classifier. A novel face recognition algorithm is presented in this paper. Histogram of Oriented Gradient features are extracted both for the test image and also for the training images and given to the Support Vector Machine classifier. The detailed steps of HOG feature extraction and the classification using SVM is presented. The algorithm is compared with the Eigen feature based face recognition algorithm. The proposed algorithm and PCA are verified using 8 different datasets. Results show that in all the face datasets the proposed algorithm shows higher face recognition rate when compared with the traditional Eigen feature based face recognition algorithm. There is an improvement of 8.75% face recognition rate when compared with PCA based face recognition algorithm. The experiment is conducted on ORL database with 2 face images for testing and 8 face images for training for each person. Three performance curves namely CMC, EPC and ROC are considered. The curves show that the proposed algorithm outperforms when compared with PCA algorithm. IndexTerms: Facial features, Histogram of Oriented Gradients, Support Vector Machine, Principle Component Analysis.',\n",
       "  'Lensless fluorescence imaging with height calculation. Abstract. Lensless fluorescence imaging (LFI) is the imaging of fluorescence from cells or microspheres using an image sensor with no external lenses or filters. The simplicity of the hardware makes it well suited to replace fluorescence microscopes and flow cytometers in lab-on-a-chip applications, but the images captured by LFI are highly dependent on the distance between the sample and the sensor. This work demonstrates that not only can samples be accurately detected across a range of sample-sensor separations using LFI, but also that the separation can be accurately estimated based on the shape of fluorescence in the LFI image. First, a theoretical model that accurately predicts LFI images of microspheres is presented. Then, the experimental results are compared to the model and an image processing method for accurately predicting sample-sensor separation from LFI images is presented. Finally, LFI images of microspheres and cells passing through a microfluidic channel are presented.'],\n",
       " ['Once a particle was detected, a discriminative correlation filter with channel and spatial reliability (CSR-DCF) was used to track particles between frames .',\n",
       "  'The Sixth Visual Object Tracking VOT2018 Challenge Results. ',\n",
       "  'A light sheet based high throughput 3D-imaging flow cytometer for phytoplankton analysis. This paper reports a light sheet fluorescence imaging flow cytometer for 3D sectioning of phytoplankton. The instrument developed has the inherent advantages of high cell counting throughput and high spatial resolution information derived from flow cytometry and light sheet microscopy. The throughput of the instrument is quantified by the sample volume flow rate of 0.5 μl/min with a spatial resolution as achieved by light sheet microscopy. Preliminary results from 3D morphology of the internal chlorophyll-a structure of two dinoflagellates species show promising application potentials of the method for phytoplankton taxonomy of selected species and species groups.'],\n",
       " ['From a scientifically accurate perspective, however, students who engage in interleaved learning (mixed, juxtaposed learning of different topics) have better scores on long-term performance tests (after several weeks or months have passed) and develop fewer misconceptions than students who sequentially learn content on one topic after another (e.g., .',\n",
       "  'The shuffling of mathematics problems improves learning. ',\n",
       "  \"Atypical inter-hemispheric communication correlates with altered motor inhibition during learning of a new bimanual coordination pattern in developmental coordination disorder. Impairment of motor learning skills in developmental coordination disorder (DCD) has been reported in several studies. Some hypotheses on neural mechanisms of motor learning deficits in DCD have emerged but, to date, brain-imaging investigations are scarce. The aim of the present study is to assess possible changes in communication between brain areas during practice of a new bimanual coordination task in teenagers with DCD (n\\xa0=\\xa010) compared to matched controls (n\\xa0=\\xa010). Accuracy, stability and number of mirror movements were computed as behavioural variables. Neural variables were assessed by electroencephalographic coherence analyses of intra-hemispheric and inter-hemispheric fronto-central electrodes. In both groups, accuracy of the new coordination increased concomitantly with right intra-hemispheric fronto-central coherence. Compared to typically developing teenagers, DCD teenagers presented learning difficulties expressed by less stability, no stabilization of the new coordination and a greater number of mirror movements despite practice. These measures correlated with reduced inter-hemispheric communication, even after practice of the new coordination. For the first time, these findings provide neuro-imaging evidence of a kind of inter-hemispheric 'disconnection' related to altered inhibition of mirror movements during motor learning in DCD.\"],\n",
       " ['Similarly, the NaturalOWL system has been proposed to generate fluent descriptions of museum exhibits from an OWL ontology.',\n",
       "  \"An Open-Source Natural Language Generator for OWL Ontologies and its Use in Protege and Second Life. We demonstrate an open-source natural language generation engine that produces descriptions of entities and classes in English and Greek from OWL ontologies that have been annotated with linguistic and user modeling information expressed in RDF. We also demonstrate an accompanying plug-in for the Protege ontology editor, which can be used to create the ontology's annotations and generate previews of the resulting texts by invoking the generation engine. The engine has been embedded in robots acting as museum tour guides in the physical world and in Second Life; here we demonstrate the latter application.\",\n",
       "  'Project Halo Update - Progress Toward Digital Aristotle. In the winter, 2004 issue of AI Magazine, we reported Vulcan Inc.\\'s first step toward creating a question-answering system called \"Digital Aristotle.\" The goal of that first step was to assess the state of the art in applied Knowledge Representation and Reasoning (KRR) by asking AI experts to represent 70 pages from the advanced placement (AP) chemistry syllabus and to deliver knowledge-based systems capable of answering questions from that syllabus. This paper reports the next step toward realizing a Digital Aristotle: we present the design and evaluation results for a system called AURA, which enables domain experts in physics, chemistry, and biology to author a knowledge base and that then allows a different set of users to ask novel questions against that knowledge base. These results represent a substantial advance over what we reported in 2004, both in the breadth of covered subjects and in the provision of sophisticated technologies in knowledge representation and reasoning, natural language processing, and question answering to domain experts and novice users.'],\n",
       " ['To test and evaluate our approach, we focus on the subpart of KB BIO 101 isolated for the KBGEN surface realisation shared task by .',\n",
       "  'The KBGen Challenge. ',\n",
       "  'Construction of an annotated corpus to support biomedical information extraction. BackgroundInformation Extraction (IE) is a component of text mining that facilitates knowledge discovery by automatically locating instances of interesting biomedical events from huge document collections. As events are usually centred on verbs and nominalised verbs, understanding the syntactic and semantic behaviour of these words is highly important. Corpora annotated with information concerning this behaviour can constitute a valuable resource in the training of IE components and resources.ResultsWe have defined a new scheme for annotating sentence-bound gene regulation events, centred on both verbs and nominalised verbs. For each event instance, all participants (arguments) in the same sentence are identified and assigned a semantic role from a rich set of 13 roles tailored to biomedical research articles, together with a biological concept type linked to the Gene Regulation Ontology. To our knowledge, our scheme is unique within the biomedical field in terms of the range of event arguments identified. Using the scheme, we have created the Gene Regulation Event Corpus (GREC), consisting of 240 MEDLINE abstracts, in which events relating to gene regulation and expression have been annotated by biologists. A novel method of evaluating various different facets of the annotation task showed that average inter-annotator agreement rates fall within the range of 66% - 90%.ConclusionThe GREC is a unique resource within the biomedical field, in that it annotates not only core relationships between entities, but also a range of other important details about these relationships, e.g., location, temporal, manner and environmental conditions. As such, it is specifically designed to support bio-specific tool and resource development. It has already been used to acquire semantic frames for inclusion within the BioLexicon (a lexical, terminological resource to aid biomedical text mining). Initial experiments have also shown that the corpus may viably be used to train IE components, such as semantic role labellers. The corpus and annotation guidelines are freely available for academic purposes.'],\n",
       " ['Finally, recent work by the SWAT project 1 has focused on pro-ducing descriptions of ontologies that are both coherent and efficient .',\n",
       "  'Grouping Axioms for More Coherent Ontology Descriptions. ',\n",
       "  'Automatic Extraction of Subcategorization from Corpora. We describe a novel technique and implemented system for constructing a subcategorization dictionary from textual corpora. Each dictionary entry encodes the relative frequency of occurrence of a comprehensive set of subcategorization classes for English. An initial experiment, on a sample of 14 verbs which exhibit multiple complementation patterns, demonstrates that the technique achieves accuracy comparable to previous approaches, which are all limited to a highly restricted set of subcategorization classes. We also demonstrate that a subcategorization dictionary built with the system improves the accuracy of a parser by an appreciable amount1.'],\n",
       " ['While offering more flexibility and expressiveness, these systems are difficult to adapt by non-NLG experts because they require the user to understand the architecture of the NLG systems .',\n",
       "  'Automatic Report Generation from Ontologies: The MIAKT Approach. ',\n",
       "  'Unsupervised Concept-to-text Generation with Hypergraphs.'],\n",
       " ['KB Bio 101 The foundational component of the KB is the Component Library (CLIB), an upper ontology which is linguistically motivated and designed to support the representation of knowledge for automated reasoning .',\n",
       "  'Project Halo Update - Progress Toward Digital Aristotle. In the winter, 2004 issue of AI Magazine, we reported Vulcan Inc.\\'s first step toward creating a question-answering system called \"Digital Aristotle.\" The goal of that first step was to assess the state of the art in applied Knowledge Representation and Reasoning (KRR) by asking AI experts to represent 70 pages from the advanced placement (AP) chemistry syllabus and to deliver knowledge-based systems capable of answering questions from that syllabus. This paper reports the next step toward realizing a Digital Aristotle: we present the design and evaluation results for a system called AURA, which enables domain experts in physics, chemistry, and biology to author a knowledge base and that then allows a different set of users to ask novel questions against that knowledge base. These results represent a substantial advance over what we reported in 2004, both in the breadth of covered subjects and in the provision of sophisticated technologies in knowledge representation and reasoning, natural language processing, and question answering to domain experts and novice users.',\n",
       "  'Lessons from a failure: Generating tailored smoking cessation letters.'],\n",
       " ['Another trend of work relevant to this paper is generation from databases using parallel corpora of data and text.',\n",
       "  'A Simple Domain-Independent Probabilistic Approach to Generation. ',\n",
       "  'Automatic Extraction of Subcategorization Frames for Czech. We present some novel machine learning techniques for the identification of subcategorization information for verbs in Czech. We compare three different statistical techniques applied to this problem. We show how the learning algorithm can be used to discover previously unknown subcategorization frames from the Czech Prague Dependency Treebank. The algorithm can then be used to label dependents of a verb in the Czech treebank as either arguments or adjuncts. Using our techniques, we are able to achieve 88% precision on unseen parsed text.'],\n",
       " ['train a sequence of discriminative models to predict data selection, ordering and realisation.',\n",
       "  'Generation by Inverting a Semantic Parser that Uses Statistical Machine Translation. ',\n",
       "  'BioInfer: a corpus for information extraction in the biomedical domain. BackgroundLately, there has been a great interest in the application of information extraction methods to the biomedical domain, in particular, to the extraction of relationships of genes, proteins, and RNA from scientific publications. The development and evaluation of such methods requires annotated domain corpora.ResultsWe present BioInfer (Bio Information Extraction Resource), a new public resource providing an annotated corpus of biomedical English. We describe an annotation scheme capturing named entities and their relationships along with a dependency analysis of sentence syntax. We further present ontologies defining the types of entities and relationships annotated in the corpus. Currently, the corpus contains 1100 sentences from abstracts of biomedical research articles annotated for relationships, named entities, as well as syntactic dependencies. Supporting software is provided with the corpus. The corpus is unique in the domain in combining these annotation types for a single set of sentences, and in the level of detail of the relationship annotation.ConclusionWe introduce a corpus targeted at protein, gene, and RNA relationships which serves as a resource for the development of information extraction systems and their components such as parsers and domain analyzers. The corpus will be maintained and further developed with a current version being available at http://www.it.utu.fi/BioInfer.'],\n",
       " ['uses techniques from statistical machine translation to model the generation task and ) learns a probabilistic Context-Free Grammar modelling the structure of the database and of the associated text.',\n",
       "  'Unsupervised Concept-to-text Generation with Hypergraphs. ',\n",
       "  'Automatic Extraction of Subcategorization from Corpora. We describe a novel technique and implemented system for constructing a subcategorization dictionary from textual corpora. Each dictionary entry encodes the relative frequency of occurrence of a comprehensive set of subcategorization classes for English. An initial experiment, on a sample of 14 verbs which exhibit multiple complementation patterns, demonstrates that the technique achieves accuracy comparable to previous approaches, which are all limited to a highly restricted set of subcategorization classes. We also demonstrate that a subcategorization dictionary built with the system improves the accuracy of a parser by an appreciable amount1.'],\n",
       " ['For instance, the OWL verbaliser integrated in the Protégé tool is a CNL based generation tool, which provides a verbalisation of every axiom present in the ontology under consideration.',\n",
       "  'Verbalizing OWL in Attempto Controlled English. ',\n",
       "  'Acquisition and evaluation of verb subcategorization resources for biomedicine.'],\n",
       " ['JVenny was used to generate a comparative Venn diagram of proteins from all cell lines .',\n",
       "  'jvenn: an interactive Venn diagram viewer. Venn diagrams are commonly used to display list comparison. In biology, they are widely used to show the differences between gene lists originating from different differential analyses, for instance. They thus allow the comparison between different experimental conditions or between different methods. However, when the number of input lists exceeds four, the diagram becomes difficult to read. Alternative layouts and dynamic display features can improve its use and its readability. jvenn is a new JavaScript library. It processes lists and produces Venn diagrams. It handles up to six input lists and presents results using classical or Edwards-Venn layouts. User interactions can be controlled and customized. Finally, jvenn can easily be embeded in a web page, allowing to have dynamic Venn diagrams. jvenn is an open source component for web environments helping scientists to analyze their data. The library package, which comes with full documentation and an example, is freely available at http://bioinfo.genotoul.fr/jvenn.',\n",
       "  'Mapping identifiers for the integration of genomic datasets with the R/Bioconductor package biomaRt.'],\n",
       " ['g:Profiler was used to analyze the pathway enrichment of identified FECH partners .',\n",
       "  'g:Profiler: a web server for functional enrichment analysis and conversions of gene lists (2019 update. Abstract Biological data analysis often deals with lists of genes arising from various studies. The g:Profiler toolset is widely used for finding biological categories enriched in gene lists, conversions between gene identifiers and mappings to their orthologs. The mission of g:Profiler is to provide a reliable service based on up-to-date high quality data in a convenient manner across many evidence types, identifier spaces and organisms. g:Profiler relies on Ensembl as a primary data source and follows their quarterly release cycle while updating the other data sources simultaneously. The current update provides a better user experience due to a modern responsive web interface, standardised API and libraries. The results are delivered through an interactive and configurable web design. Results can be downloaded as publication ready visualisations or delimited text files. In the current update we have extended the support to 467 species and strains, including vertebrates, plants, fungi, insects and parasites. By supporting user uploaded custom GMT files, g:Profiler is now capable of analysing data from any organism. All past releases are maintained for reproducibility and transparency. The 2019 update introduces an extensive technical rewrite making the services faster and more flexible. g:Profiler is freely available at https://biit.cs.ut.ee/gprofiler.',\n",
       "  'The ubiquitous mitochondrial protein unfoldase CLPX regulates erythroid heme synthesis by control of iron utilization and heme synthesis enzyme activation and turnover.'],\n",
       " ['Statistics for our dataset is presented in Table 1.Each abstract was annotated by two annotators.The task was to classify the relations between each possible pair of terms in each sentence in the abstract.The terms in the texts were already extracted.During the annotation, we followed the instructions proposed in .',\n",
       "  'Entity Recognition and Relation Extraction from Scientific and Technical Texts in Russian. This paper is devoted to the study of methods for information extraction (entity recognition and relation classification) from scientific texts on information technology. Scientific publications provide valuable information into cutting-edge scientific advances, but efficient processing of increasing amounts of data is a time-consuming task. In this paper, several modifications of methods for the Russian language are proposed. It also includes the results of experiments comparing a keyword extraction method, vocabulary method, and some methods based on neural networks. Text collections for these tasks exist for the English language and are actively used by the scientific community, but at present, such datasets in Russian are not publicly available. In this paper, we present a corpus of scientific texts in Russian, RuSERRC. This dataset consists of 1600 unlabeled documents and 80 labeled with entities and semantic relations (6 relation types were considered). The dataset and models are available at https://github.com/iis-research-team. We hope they can be useful for research purposes and development of information extraction systems.',\n",
       "  'Measuring Bias in Contextualized Word Representations. Contextual word embeddings such as BERT have achieved state of the art performance in numerous NLP tasks. Since they are optimized to capture the statistical properties of training data, they tend to pick up on and amplify social stereotypes present in the data as well. In this study, we (1) propose a template-based method to quantify bias in BERT; (2) show that this method obtains more consistent results in capturing social biases than the traditional cosine based method; and (3) conduct a case study, evaluating gender bias in a downstream task of Gender Pronoun Resolution. Although our case study focuses on gender bias, the proposed technique is generalizable to unveiling other biases, including in multiclass settings, such as racial and religious biases.'],\n",
       " ['With the introduction of large language models their use became one of the main methods of solving this problem.However, such methods require a lot of well-annotated data for training.Currently there are no datasets available for this task in a scientific field in Russian, and manual annotation takes a long time and requires the efforts of more than one person to objectively label the relations.Therefore, in this paper we decided to pay our special attention to zero-shot and few-shot approaches that do not require a lot of annotated data.There are some examples of them.',\n",
       "  'BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.',\n",
       "  'Language Models are Unsupervised Multitask Learners.'],\n",
       " ['Then we got an estimate of the probability of each sentence using the model GPT2 .After choosing the most probable pattern for each relation, we again compared the probability of sentences from these best templates.The most likely sentence would reflect the true relation between the terms.The schematic work of the method is presented in the Figure 1.To measure the probability we used the perplexity score.In general, this value can be described as the model uncertainty measure when predicting each of the next token, hence the lower the perplexity, the more certain the model in predicting this sequence.',\n",
       "  'Language Models are Unsupervised Multitask Learners. ',\n",
       "  'Multi-Task Identification of Entities, Relations, and Coreference for Scientific Knowledge Graph Construction. We introduce a multi-task setup of identifying entities, relations, and coreference clusters in scientific articles. We create SciERC, a dataset that includes annotations for all three tasks and develop a unified framework called SciIE with shared span representations. The multi-task setup reduces cascading errors between tasks and leverages cross-sentence relations through coreference links. Experiments show that our multi-task model outperforms previous models in scientific information extraction without using any domain-specific features. We further show that the framework supports construction of a scientific knowledge graph, which we use to analyze information in scientific literature.'],\n",
       " ['proposed to create a template for each relation type and then compute increased log probability of the sentences from these templates with the use of BERT as in .For example, a template for the relation \"LOCATED-IN\" might look like this -\"the <e1> is in the <e2>\".So if the first entity is \"toothbrush\" and the second is \"bathroom\", the sentence from the template will be \"the toothbrush is in the bathroom\".With the selected threshold of probability, it will be possible to separate the presence or absence of relation between two entities and also its type.',\n",
       "  'Measuring Bias in Contextualized Word Representations. Contextual word embeddings such as BERT have achieved state of the art performance in numerous NLP tasks. Since they are optimized to capture the statistical properties of training data, they tend to pick up on and amplify social stereotypes present in the data as well. In this study, we (1) propose a template-based method to quantify bias in BERT; (2) show that this method obtains more consistent results in capturing social biases than the traditional cosine based method; and (3) conduct a case study, evaluating gender bias in a downstream task of Gender Pronoun Resolution. Although our case study focuses on gender bias, the proposed technique is generalizable to unveiling other biases, including in multiclass settings, such as racial and religious biases.',\n",
       "  'Language Models are Unsupervised Multitask Learners.'],\n",
       " ['Using prototype vectors of relations The second approach for relation identification that we tried is based on the usage of the prototype vectors of relations.It can be attributed to few-shot approaches.First of all, we manually chose 138 best examples from the train part of the dataset to create a prototype vectors for each type of relations.In selecting the best examples we were guided by the following criterion: the example shows only one type of relations and has short context which includes only two terms of interest.Then we got the vectors of these of sentences.Vectors of sentences are the embeddings of CLS token from BERT .Each prototype vector is an average of the vectors of sentences reflecting each relation.Once these prototype vectors are obtained, they can be used to classify test examples.By computing the value of the cosine similarity of the example and the prototypes, we can determine which relation is most similar to this example.Schematic graphics that reflect the work of this method can be seen in Figure 2.',\n",
       "  'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).',\n",
       "  'Language Models are Unsupervised Multitask Learners.'],\n",
       " [\"We use Ensembl's Variant Effect Predictor (VEP) tool to retrieve associated Ensembl reference transcripts, gene name, predicted variant consequence, and affected protein position of the variant provided by the user.\",\n",
       "  'The Ensembl Variant Effect Predictor. The Ensembl Variant Effect Predictor is a powerful toolset for the analysis, annotation, and prioritization of genomic variants in coding and non-coding regions. It provides access to an extensive collection of genomic annotation, with a variety of interfaces to suit different requirements, and simple options for configuring and extending analysis. It is open source, free to use, and supports full reproducibility of results. The Ensembl Variant Effect Predictor can simplify and accelerate variant interpretation in a wide range of study designs.',\n",
       "  'SCOPe: Manual Curation and Artifact Removal in the Structural Classification of Proteins - extended Database.'],\n",
       " ['These results are similar to the novelty of newly classified structures in SCOP and SCOPe, as we previously reported .',\n",
       "  'SCOPe: Manual Curation and Artifact Removal in the Structural Classification of Proteins - extended Database. ',\n",
       "  '3DSwap: curated knowledgebase of proteins involved in 3D domain swapping. Three-dimensional domain swapping is a unique protein structural phenomenon where two or more protein chains in a protein oligomer share a common structural segment between individual chains. This phenomenon is observed in an array of protein structures in oligomeric conformation. Protein structures in swapped conformations perform diverse functional roles and are also associated with deposition diseases in humans. We have performed in-depth literature curation and structural bioinformatics analyses to develop an integrated knowledgebase of proteins involved in 3D domain swapping. The hallmark of 3D domain swapping is the presence of distinct structural segments such as the hinge and swapped regions. We have curated the literature to delineate the boundaries of these regions. In addition, we have defined several new concepts like ‘secondary major interface’ to represent the interface properties arising as a result of 3D domain swapping, and a new quantitative measure for the ‘extent of swapping’ in structures. The catalog of proteins reported in 3DSwap knowledgebase has been generated using an integrated structural bioinformatics workflow of database searches, literature curation, by structure visualization and sequence–structure–function analyses. The current version of the 3DSwap knowledgebase reports 293 protein structures, the analysis of such a compendium of protein structures will further the understanding molecular factors driving 3D domain swapping. Database URL: http://caps.ncbs.res.in/3dswap'],\n",
       " ['Tags were identified using PDB metadata (SE-QADV records) referring to cloning, expression, or purification tags at the N-or C-terminal of each chain, as described in more detail elsewhere .',\n",
       "  'SCOPe: Manual Curation and Artifact Removal in the Structural Classification of Proteins - extended Database. ',\n",
       "  \"Accurate prediction of protein structures and interactions using a 3-track neural network. Deep learning takes on protein folding In 1972, Anfinsen won a Nobel prize for demonstrating a connection between a protein's amino acid sequence and its three-dimensional structure. Since 1994, scientists have competed in the biannual Critical Assessment of Structure Prediction (CASP) protein-folding challenge. Deep learning methods took center stage at CASP14, with DeepMind's Alphafold2 achieving remarkable accuracy. Baek et al. explored network architectures based on the DeepMind framework. They used a three-track network to process sequence, distance, and coordinate information simultaneously and achieved accuracies approaching those of DeepMind. The method, RoseTTA fold, can solve challenging x-ray crystallography and cryo–electron microscopy modeling problems and generate accurate models of protein-protein complexes. Science, abj8754, this issue p. 871 Protein structure modeling enables the rapid solution of protein structures and provides insights into function. DeepMind presented notably accurate predictions at the recent 14th Critical Assessment of Structure Prediction (CASP14) conference. We explored network architectures that incorporate related ideas and obtained the best performance with a three-track network in which information at the one-dimensional (1D) sequence level, the 2D distance map level, and the 3D coordinate level is successively transformed and integrated. The three-track network produces structure predictions with accuracies approaching those of DeepMind in CASP14, enables the rapid solution of challenging x-ray crystallography and cryo–electron microscopy structure modeling problems, and provides insights into the functions of proteins of currently unknown structure. The network also enables rapid generation of accurate protein-protein complex models from sequence information alone, short-circuiting traditional approaches that require modeling of individual subunits followed by docking. We make the method available to the scientific community to speed biological research.\"],\n",
       " [\"We publish two kinds of SCOPe releases: major 'stable' releases that contain updates to the hierarchic structure, manual curation, and error correction relative to previous stable releases; and minor periodic updates .\",\n",
       "  'SCOPe: classification of large macromolecular structures in the structural classification of proteins—extended database. Abstract The SCOPe (Structural Classification of Proteins—extended, https://scop.berkeley.edu) database hierarchically classifies domains from the majority of proteins of known structure according to their structural and evolutionary relationships. SCOPe also incorporates and updates the ASTRAL compendium, which provides multiple databases and tools to aid in the analysis of the sequences and structures of proteins classified in SCOPe. Protein structures are classified using a combination of manual curation and highly precise automated methods. In the current release of SCOPe, 2.07, we have focused our manual curation efforts on larger protein structures, including the spliceosome, proteasome and RNA polymerase I, as well as many other Pfam families that had not previously been classified. Domains from these large protein complexes are distinctive in several ways: novel non-globular folds are more common, and domains from previously observed protein families often have N- or C-terminal extensions that were disordered or not present in previous structures. The current monthly release update, SCOPe 2.07–2018-10–18, classifies 90 992 PDB entries (about two thirds of PDB entries).',\n",
       "  'The value of protein structure classification information—Surveying the scientific literature. The Structural Classification of Proteins (SCOP) and Class, Architecture, Topology, Homology (CATH) databases have been valuable resources for protein structure classification for over 20 years. Development of SCOP (version 1) concluded in June 2009 with SCOP 1.75. The SCOPe (SCOP–extended) database offers continued development of the classic SCOP hierarchy, adding over 33,000 structures. We have attempted to assess the impact of these two decade old resources and guide future development. To this end, we surveyed recent articles to learn how structure classification data are used. Of 571 articles published in 2012–2013 that cite SCOP, 439 actually use data from the resource. We found that the type of use was fairly evenly distributed among four top categories: A) study protein structure or evolution (27% of articles), B) train and/or benchmark algorithms (28% of articles), C) augment non‐SCOP datasets with SCOP classification (21% of articles), and D) examine the classification of one protein/a small set of proteins (22% of articles). Most articles described computational research, although 11% described purely experimental research, and a further 9% included both. We examined how CATH and SCOP were used in 158 articles that cited both databases: while some studies used only one dataset, the majority used data from both resources. Protein structure classification remains highly relevant for a diverse range of problems and settings. Proteins 2015; 83:2025–2038. © 2015 The Authors. Proteins: Structure, Function, and Bioinformatics Published by Wiley Periodicals, Inc.'],\n",
       " ['Two different domain lengths were calculated: one based on PDB SEQRES records, the genetically encoded protein sequence; and one based on ATOM records, which were the parts of the protein that were experimentally observed .',\n",
       "  'The ASTRAL compendium for protein structure and sequence analysis. The ASTRAL compendium provides several databases and tools to aid in the analysis of protein structures, particularly through the use of their sequences. The SPACI scores included in the system summarize the overall characteristics of a protein structure. A structural alignments database indicates residue equivalencies in superimposed protein domain structures. The PDB sequence-map files provide a linkage between the amino acid sequence of the molecule studied (SEQRES records in a database entry) and the sequence of the atoms experimentally observed in the structure (ATOM records). These maps are combined with information in the SCOPdatabase to provide sequences of protein domains. Selected subsets of the domain database, with varying degrees of similarity measured in several different ways, are also available. ASTRALmay be accessed at http://astral.stanford.edu/',\n",
       "  'SCOPe: Structural Classification of Proteins—extended, integrating SCOP and ASTRAL data and classification of new structures. Structural Classification of Proteins—extended (SCOPe, http://scop.berkeley.edu) is a database of protein structural relationships that extends the SCOP database. SCOP is a manually curated ordering of domains from the majority of proteins of known structure in a hierarchy according to structural and evolutionary relationships. Development of the SCOP 1.x series concluded with SCOP 1.75. The ASTRAL compendium provides several databases and tools to aid in the analysis of the protein structures classified in SCOP, particularly through the use of their sequences. SCOPe extends version 1.75 of the SCOP database, using automated curation methods to classify many structures released since SCOP 1.75. We have rigorously benchmarked our automated methods to ensure that they are as accurate as manual curation, though there are many proteins to which our methods cannot be applied. SCOPe is also partially manually curated to correct some errors in SCOP. SCOPe aims to be backward compatible with SCOP, providing the same parseable files and a history of changes between all stable SCOP and SCOPe releases. SCOPe also incorporates and updates the ASTRAL database. The latest release of SCOPe, 2.03, contains 59 514 Protein Data Bank (PDB) entries, increasing the number of structures classified in SCOP by 55% and including more than 65% of the protein structures in the PDB.'],\n",
       " ['To facilitate automated algorithms developed or trained on the SCOPe database, we provide machineparseable annotations of the extent of a single repeat unit for all families of repeats in classes a to g. Tandem repeats are often also annotated in other databases, such as Pfam .',\n",
       "  'Pfam: The protein families database in 2021. Abstract The Pfam database is a widely used resource for classifying protein sequences into families and domains. Since Pfam was last described in this journal, over 350 new families have been added in Pfam 33.1 and numerous improvements have been made to existing entries. To facilitate research on COVID-19, we have revised the Pfam entries that cover the SARS-CoV-2 proteome, and built new entries for regions that were not covered by Pfam. We have reintroduced Pfam-B which provides an automatically generated supplement to Pfam and contains 136 730 novel clusters of sequences that are not yet matched by a Pfam family. The new Pfam-B is based on a clustering by the MMseqs2 software. We have compared all of the regions in the RepeatsDB to those in Pfam and have started to use the results to build and refine Pfam repeat families. Pfam is freely available for browsing and download at http://pfam.xfam.org/.',\n",
       "  'SCOPe: Manual Curation and Artifact Removal in the Structural Classification of Proteins - extended Database.'],\n",
       " ['* files released with previous versions of SCOP and SCOPe .',\n",
       "  'SCOP database in 2002: refinements accommodate structural genomics. The SCOP (Structural Classification of Proteins) database is a comprehensive ordering of all proteins of known structure, according to their evolutionary and structural relationships. Protein domains in SCOP are grouped into species and hierarchically classified into families, superfamilies, folds and classes. Recently, we introduced a new set of features with the aim of standardizing access to the database, and providing a solid basis to manage the increasing number of experimental structures expected from structural genomics projects. These features include: a new set of identifiers, which uniquely identify each entry in the hierarchy; a compact representation of protein domain classification; a new set of parseable files, which fully describe all domains in SCOP and the hierarchy itself. These new features are reflected in the ASTRAL compendium. The SCOP search engine has also been updated, and a set of links to external resources added at the level of domain entries. SCOP can be accessed at http://scop.mrc-lmb.cam.ac.uk/scop.',\n",
       "  'The ASTRAL compendium for protein structure and sequence analysis. The ASTRAL compendium provides several databases and tools to aid in the analysis of protein structures, particularly through the use of their sequences. The SPACI scores included in the system summarize the overall characteristics of a protein structure. A structural alignments database indicates residue equivalencies in superimposed protein domain structures. The PDB sequence-map files provide a linkage between the amino acid sequence of the molecule studied (SEQRES records in a database entry) and the sequence of the atoms experimentally observed in the structure (ATOM records). These maps are combined with information in the SCOPdatabase to provide sequences of protein domains. Selected subsets of the domain database, with varying degrees of similarity measured in several different ways, are also available. ASTRALmay be accessed at http://astral.stanford.edu/'],\n",
       " ['Hundreds of additional entries are added to SCOPe each month, because after at least one structure from a SCOPe family has been classified by a human curator, most other structures from that family may be added automatically by our rigorously validated software pipeline .',\n",
       "  'SCOPe: Structural Classification of Proteins—extended, integrating SCOP and ASTRAL data and classification of new structures. Structural Classification of Proteins—extended (SCOPe, http://scop.berkeley.edu) is a database of protein structural relationships that extends the SCOP database. SCOP is a manually curated ordering of domains from the majority of proteins of known structure in a hierarchy according to structural and evolutionary relationships. Development of the SCOP 1.x series concluded with SCOP 1.75. The ASTRAL compendium provides several databases and tools to aid in the analysis of the protein structures classified in SCOP, particularly through the use of their sequences. SCOPe extends version 1.75 of the SCOP database, using automated curation methods to classify many structures released since SCOP 1.75. We have rigorously benchmarked our automated methods to ensure that they are as accurate as manual curation, though there are many proteins to which our methods cannot be applied. SCOPe is also partially manually curated to correct some errors in SCOP. SCOPe aims to be backward compatible with SCOP, providing the same parseable files and a history of changes between all stable SCOP and SCOPe releases. SCOPe also incorporates and updates the ASTRAL database. The latest release of SCOPe, 2.03, contains 59 514 Protein Data Bank (PDB) entries, increasing the number of structures classified in SCOP by 55% and including more than 65% of the protein structures in the PDB.',\n",
       "  'The Ensembl Variant Effect Predictor. The Ensembl Variant Effect Predictor is a powerful toolset for the analysis, annotation, and prioritization of genomic variants in coding and non-coding regions. It provides access to an extensive collection of genomic annotation, with a variety of interfaces to suit different requirements, and simple options for configuring and extending analysis. It is open source, free to use, and supports full reproducibility of results. The Ensembl Variant Effect Predictor can simplify and accelerate variant interpretation in a wide range of study designs.'],\n",
       " ['According to previous studies on trust in e-commerce, trust in an online shopping mall is the belief that the shopping mall will perform to the extent the buyer expects .',\n",
       "  'Interpreting Dimensions of Consumer Trust in E-Commerce. ',\n",
       "  'User Acceptance of Information Technology: Toward a Unified View. Information technology (IT) acceptance research has yielded many competing models, each with different sets of acceptance determinants. In this paper, we (1) review user acceptance literature and discuss eight prominent models, (2) empirically compare the eight models and their extensions, (3) formulate a unified model that integrates elements across the eight models, and (4) empirically validate the unified model. The eight models reviewed are the theory of reasoned action, the technology acceptance model, the motivational model, the theory of planned behavior, a model combining the technology acceptance model and the theory of planned behavior, the model of PC utilization, the innovation diffusion theory, and the social cognitive theory. Using data from four organizations over a six-month period with three points of measurement, the eight models explained between 17 percent and 53 percent of the variance in user intentions to use information technology. Next, a unified model, called the Unified Theory of Acceptance and Use of Technology (UTAUT), was formulated, with four core determinants of intention and usage, and up to four moderators of key relationships. UTAUT was then tested using the original data and found to outperform the eight individual models (adjusted R2 of 69 percent). UTAUT was then confirmed with data from two new organizations with similar results (adjusted R2 of 70 percent). UTAUT thus provides a useful tool for managers needing to assess the likelihood of success for new technology introductions and helps them understand the drivers of acceptance in order to proactively design interventions (including training, marketing, etc.) targeted at populations of users that may be less inclined to adopt and use new systems. The paper also makes several recommendations for future research including developing a deeper understanding of the dynamic influences studied here, refining measurement of the core constructs used in UTAUT, and understanding the organizational outcomes associated with new technology use.'],\n",
       " ['Effort expectancy 4 The degree to which consumers believe that the mobile application and related processes are easy to use .',\n",
       "  'User Acceptance of Information Technology: Toward a Unified View. Information technology (IT) acceptance research has yielded many competing models, each with different sets of acceptance determinants. In this paper, we (1) review user acceptance literature and discuss eight prominent models, (2) empirically compare the eight models and their extensions, (3) formulate a unified model that integrates elements across the eight models, and (4) empirically validate the unified model. The eight models reviewed are the theory of reasoned action, the technology acceptance model, the motivational model, the theory of planned behavior, a model combining the technology acceptance model and the theory of planned behavior, the model of PC utilization, the innovation diffusion theory, and the social cognitive theory. Using data from four organizations over a six-month period with three points of measurement, the eight models explained between 17 percent and 53 percent of the variance in user intentions to use information technology. Next, a unified model, called the Unified Theory of Acceptance and Use of Technology (UTAUT), was formulated, with four core determinants of intention and usage, and up to four moderators of key relationships. UTAUT was then tested using the original data and found to outperform the eight individual models (adjusted R2 of 69 percent). UTAUT was then confirmed with data from two new organizations with similar results (adjusted R2 of 70 percent). UTAUT thus provides a useful tool for managers needing to assess the likelihood of success for new technology introductions and helps them understand the drivers of acceptance in order to proactively design interventions (including training, marketing, etc.) targeted at populations of users that may be less inclined to adopt and use new systems. The paper also makes several recommendations for future research including developing a deeper understanding of the dynamic influences studied here, refining measurement of the core constructs used in UTAUT, and understanding the organizational outcomes associated with new technology use.',\n",
       "  'Exploring consumers perceived risk and trust for mobile shopping: A theoretical framework and empirical study.'],\n",
       " ['We, thus, conclude that the model fit was satisfactory .',\n",
       "  \"An Empirical Investigation of the Factors Affecting Data Warehousing Success. The IT implementation literature suggests that various implementation factors play critical roles in the success of an information system; however, there is little empirical research about the implementation of data warehousing projects. Data warehousing has unique characteristics that may impact the importance of factors that apply to it. In this study, a cross-sectional survey investigated a model of data warehousing success. Data warehousing managers and data suppliers from 111 organizations completed paired mail questionnaires on implementation factors and the success of the warehouse. The results from a Partial Least Squares analysis of the data identified significant relationships between the system quality and data quality factors and perceived net benefits. It was found that management support and resources help to address organizational issues that arise during warehouse implementations; resources, user participation, and highly-skilled project team members increase the likelihood that warehousing projects will finish on-time, on-budget, with the right functionality; and diverse, unstandardized source systems and poor development technology will increase the technical issues that project teams must overcome. The implementation's success with organizational and project issues, in turn, influence the system quality of the data warehouse; however, data quality is best explained by factors not included in the research model.\",\n",
       "  'Causes and consequences of ‘order online pick up in-store’ shopping behavior.'],\n",
       " ['Mosquera et al studied the impact of smartphones in omni-channel shopping using the UTAUT model.',\n",
       "  'Key Factors for In-Store Smartphone Use in an Omnichannel Experience: Millennials vs. Nonmillennials. The in-store use of smartphones is revolutionizing the customer journey and has the potential to become an important driver in the omnichannel context. This paper aims at identifying the key factors that influence customers’ intentions to use smartphones in-store and their actual behavior and to test the moderating effect of age, differentiating between millennials and nonmillennials, as millennials are considered digital natives and early adopters of new technologies. We applied the UTAUT2 model to a sample of 1043 Spanish customers, tested it using structural equations, and performed a multigroup analysis to compare the results between the two groups. The results show that the model explains both the behavioral intention to use a smartphone in a brick-and-mortar store and use behavior. The UTAUT2 predictors found to be most important were habit, performance expectancy, and hedonic motivation. However, the study shows that the only difference between millennials and nonmillennials with regard to the use of smartphones in-store is the effects of behavioral intention and habit on use behavior. The study adds to the existing knowledge by providing evidence in support of the validity of UTAUT2 as an appropriate theoretical basis to explain effectively behavioral intention, specifically the in-store use of smartphones.',\n",
       "  'Shops offer the e-tail experience. E-commerce has transformed retail over the last 15 years, but the widely held presumption that online purchasing would spell the demise of bricks-and-mortar shopping has now been supplanted by a more nuanced vision of our shopping future. Recent developments in advanced interactive technology have enabled shrewd retailers to mine the idea that online/offline shopping is not an either/or proposition, and that there is a chance to engage customers in new ways on the physical shopfloor. Indeed, rather than diminishing the traditional shopping experience, techniques that have been the preserve of the online shop are to some extent now informing the new in-store retail technology.'],\n",
       " ['Unlike previous studies, effort expectancy and price value were found to have no significant effect .',\n",
       "  'Research Commentary - Digital Natives and Ubiquitous Information Systems. Most information systems research until now has focused on information systems in organizations and their use by digital immigrants. Digital immigrants are those who were not born into the digital world---they learnt to use information systems at some stage in their adult lives. An underlying assumption of much of this research is that users “resist” technology or at least have some difficulty in accepting it. Digital natives, conversely, are those who have grown up in a world where the use of information and communications technology is pervasive and ubiquitous. These ubiquitous technologies, networks, and associated systems have proliferated and have woven themselves into the very fabric of everyday life. This article suggests that the rise of the digital native, along with the growth of ubiquitous information systems (UIS), potentially represents a fundamental shift in our “paradigm” for IS research. We propose a research agenda that focuses on digital natives and UIS.',\n",
       "  'An Empirical Study of an Online Travel Purchase Intention Model.'],\n",
       " ['Retail companies have realized in recent years that integrated channels can increase consumer value and operational efficiency, prompting them to focus on omni-channel retailing .',\n",
       "  'Omnichannel Retail Operations with Buy-Online-and-Pickup-in-Store. Many retailers have recently started to offer customers the option to buy online and pick up in store (BOPS). We study the impact of the BOPS initiative on store operations. We build a stylized model where a retailer operates both online and offline channels. Consumers strategically make channel choices. The BOPS option affects consumer choice in two ways: by providing real-time information about inventory availability and by reducing the hassle cost of shopping. We obtain three findings. First, not all products are well-suited for in-store pickup; specifically, it may not be profitable to implement BOPS on products that sell well in stores. Second, BOPS enables retailers to reach new customers, but for existing customers, the shift from online fulfillment to store fulfillment may decrease profit margins when the latter is less cost effective. Finally, in a decentralized retail system where store and online channels are managed separately, BOPS revenue can be shared across channels to alleviate incentive conflicts; it is rarely efficient to allocate all the revenue to a single channel.',\n",
       "  \"Shopping orientation and online travel shopping: The role of travel experience. This paper investigates how consumers' shopping orientation toward travel shopping influences their tendency to shop for travel products on the Internet. The paper also looks into the role of consumers' travel experience. A conceptual model is developed and a number of hypotheses are forwarded and tested by structural equation modeling, using data from 256 respondents. The results support several of the stated hypotheses. Shopping orientation and travel experience both influence travelers' intentions to shop travel products online. Implications for management practice and further research are discussed at the end of the paper. Copyright © 2011 John Wiley & Sons, Ltd.\"],\n",
       " ['In this manner, trust influences behavioural intentions and use behaviour .',\n",
       "  'Examining multi-dimensional trust and multi-faceted risk in initial acceptance of emerging technologies: An empirical study of mobile banking services. ',\n",
       "  'The impact of personalization and compatibility with past experience on e-banking usage. Purpose          Banks and financial services providers are increasingly delivering their services via electronic banking, also known as e-banking. Yet even though this type of delivery is now common, the degree of personalization in the services provided via this channel exhibit considerable variation. The purpose of this paper is to examine the impact of service personalization on consumer reaction to the e-banking service. Based on research of information and communication technology (ICT) service innovation and the Unified Theory of Acceptance and Use of Technology (UTAUT) model, this study further examines one contingent factor, compatibility with previous experience with e-banking. This study focuses on the interactions effect of personalization and technology compatibility on customer e-banking service usage.          Design/methodology/approach          A survey was conducted to investigate the impacts of personalization on e-banking usage decision process and the interactions between personalization and compatibility with past e-banking experience. Quota sampling was applied and different type of customers were approached in 30 branches of the commercial bank. Data were collected from a sample of 181 banking customers in a metropolitan region in southern China.          Findings          The results indicated that personalization leads to increased performance expectancy and decreased effort expectancy, which in turn lead to increasing intention to continue to use e-banking services. In addition, compatibility with previous e-banking experience and personalization produces an interaction effect on both performance expectancy and effort expectancy.          Research limitations/implications          The theoretical contribution of this study is to demonstrate how the contingent factor of compatibility moderates the impact of personalization, thus extending the UTAUT model in the area of e-banking service adoption. Implications are twofold: personalization influences evaluations of both utility and ease of use, and the effect is magnified when compatibility with prior e-banking experience is factored into the model. This is an important extension and future research should examine whether the same relationship holds in other industries using new technologies to deliver services. The UTAUT model, after extension by including the moderating impact of compatibility, works well in demonstrating the impact of various factors on the adoption of a new technological delivery system for a service.          Practical implications          This study has two significant implications for managerial practices. First, the study sheds lights on the segmentation of e-banking customers. Modern marketers know that the best way to engage with consumers is through personal messaging strategies and should make great efforts to identify customers before trying to reach them. In the e-banking realm, consumer banking preferences keep changing. With a clear understanding of the different consumer banker segments, financial institutions can identify which channels appeal to them. For example, some users are more likely than average to use e-banking. Second, this study helps e-banking service provider design different personalized e-banking service for different customers.          Social implications          This study sheds light on social value of personalization, particularly among those new to a delivery platform.          Originality/value          This study provides evidence demonstrating that personalization increases customer perceptions of performance expectancy and decreases effort expectancy, and that the effect is most profound for customers with limited level of perceived compatibility with past experience with e-banking. This paper extended the UTAUT model and research on ICT service innovation by providing more insights on the impacts of e-banking service personalization and the contingency impact of user’s background in e-banking context.'],\n",
       " ['In our study, hedonic motivation was defined as the extent to which consumers enjoy a BOPIS system .',\n",
       "  'Factors influencing adoption of mobile banking by Jordanian bank customers: Extending UTAUT2 with trust. ',\n",
       "  \"Shopping orientation and online travel shopping: The role of travel experience. This paper investigates how consumers' shopping orientation toward travel shopping influences their tendency to shop for travel products on the Internet. The paper also looks into the role of consumers' travel experience. A conceptual model is developed and a number of hypotheses are forwarded and tested by structural equation modeling, using data from 256 respondents. The results support several of the stated hypotheses. Shopping orientation and travel experience both influence travelers' intentions to shop travel products online. Implications for management practice and further research are discussed at the end of the paper. Copyright © 2011 John Wiley & Sons, Ltd.\"],\n",
       " ['Behavioural intention 3 The intention to use BOPIS continuously .',\n",
       "  'Consumer Acceptance and Use of Information Technology: Extending the Unified Theory of Acceptance and Use of Technology. This paper extends the unified theory of acceptance and use of technology (UTAUT) to study acceptance and use of technology in a consumer context. Our proposed UTAUT2 incorporates three constructs into UTAUT: hedonic motivation, price value, and habit. Individual differences--namely, age, gender, and experience--are hypothesized to moderate the effects of these constructs on behavioral intention and technology use. Results from a two-stage online survey, with technology use data collected four months after the first survey, of 1,512 mobile Internet consumers supported our model. Compared to UTAUT, the extensions proposed in UTAUT2 produced a substantial improvement in the variance explained in behavioral intention (56 percent to 74 percent) and technology use (40 percent to 52 percent). The theoretical and managerial implications of these results are discussed.',\n",
       "  'Causes and consequences of ‘order online pick up in-store’ shopping behavior.'],\n",
       " ['MacCarthy, Zhang, and Muyldermans, found that order and pickup time management were significant factors of BOPIS.',\n",
       "  'Best Performance Frontiers for Buy-Online-Pickup-in-Store order fulfilment. ',\n",
       "  \"User Acceptance of Computer Technology: A Comparison of Two Theoretical Models. Computer systems cannot improve organizational performance if they aren't used. Unfortunately, resistance to end-user systems by managers and professionals is a widespread problem. To better predict, explain, and increase user acceptance, we need to better understand why people accept or reject computers. This research addresses the ability to predict peoples' computer acceptance from a measure of their intentions, and the ability to explain their intentions in terms of their attitudes, subjective norms, perceived usefulness, perceived ease of use, and related variables. In a longitudinal study of 107 users, intentions to use a specific system, measured after a one-hour introduction to the system, were correlated 0.35 with system use 14 weeks later. The intention-usage correlation was 0.63 at the end of this time period. Perceived usefulness strongly influenced peoples' intentions, explaining more than half of the variance in intentions at the end of 14 weeks. Perceived ease of use had a small but significant effect on intentions as well, although this effect subsided over time. Attitudes only partially mediated the effects of these beliefs on intentions. Subjective norms had no effect on intentions. These results suggest the possibility of simple but powerful models of the determinants of user acceptance, with practical value for evaluating systems and guiding managerial interventions aimed at reducing the problem of underutilized computer technology.\"],\n",
       " ['Social influence 3 The word-of-mouth activity in the BOPIS service, in which friends, colleagues, and family members express appreciation for the convenience of BOPIS to one another .',\n",
       "  'User Acceptance of Information Technology: Toward a Unified View. Information technology (IT) acceptance research has yielded many competing models, each with different sets of acceptance determinants. In this paper, we (1) review user acceptance literature and discuss eight prominent models, (2) empirically compare the eight models and their extensions, (3) formulate a unified model that integrates elements across the eight models, and (4) empirically validate the unified model. The eight models reviewed are the theory of reasoned action, the technology acceptance model, the motivational model, the theory of planned behavior, a model combining the technology acceptance model and the theory of planned behavior, the model of PC utilization, the innovation diffusion theory, and the social cognitive theory. Using data from four organizations over a six-month period with three points of measurement, the eight models explained between 17 percent and 53 percent of the variance in user intentions to use information technology. Next, a unified model, called the Unified Theory of Acceptance and Use of Technology (UTAUT), was formulated, with four core determinants of intention and usage, and up to four moderators of key relationships. UTAUT was then tested using the original data and found to outperform the eight individual models (adjusted R2 of 69 percent). UTAUT was then confirmed with data from two new organizations with similar results (adjusted R2 of 70 percent). UTAUT thus provides a useful tool for managers needing to assess the likelihood of success for new technology introductions and helps them understand the drivers of acceptance in order to proactively design interventions (including training, marketing, etc.) targeted at populations of users that may be less inclined to adopt and use new systems. The paper also makes several recommendations for future research including developing a deeper understanding of the dynamic influences studied here, refining measurement of the core constructs used in UTAUT, and understanding the organizational outcomes associated with new technology use.',\n",
       "  'Factors influencing adoption of mobile banking by Jordanian bank customers: Extending UTAUT2 with trust.'],\n",
       " ['of Questions Definition in This Study and References Performance expectancy 4 The degree to which consumers believe that the use of the BOPIS service will provide them with better benefits in purchasing activities .',\n",
       "  'Consumer Acceptance and Use of Information Technology: Extending the Unified Theory of Acceptance and Use of Technology. This paper extends the unified theory of acceptance and use of technology (UTAUT) to study acceptance and use of technology in a consumer context. Our proposed UTAUT2 incorporates three constructs into UTAUT: hedonic motivation, price value, and habit. Individual differences--namely, age, gender, and experience--are hypothesized to moderate the effects of these constructs on behavioral intention and technology use. Results from a two-stage online survey, with technology use data collected four months after the first survey, of 1,512 mobile Internet consumers supported our model. Compared to UTAUT, the extensions proposed in UTAUT2 produced a substantial improvement in the variance explained in behavioral intention (56 percent to 74 percent) and technology use (40 percent to 52 percent). The theoretical and managerial implications of these results are discussed.',\n",
       "  'Exploring consumers perceived risk and trust for mobile shopping: A theoretical framework and empirical study.'],\n",
       " ['Millennial cohorts are digital natives and shopping leaders .',\n",
       "  'The influence of Facebook advertising on cognitive attitudes amid Generation Y. Social media has irrevocably transformed the manner in which society communicates and has altered perceptions and attitudes. The proliferation of Facebook usage has connected consumers to each other, to marketers and to brands in a manner that is as inventive, and has altered the world as we know it. However, research on Facebook is still in its infancy in an emerging country such as South Africa. Generation Y is a significant consumer group and, consequently, their attitudes towards social media advertising are of significant importance to marketers. This has triggered the question whether the largest social medium’s marketing communications effectively reaches young adults in South Africa, and what influence it has on their cognitive attitudes, which leads to their buying behavior. The results suggest that Facebook advertising has a favorable effect on the awareness and knowledge hierarchy-of-effects model levels amongst Generation Y in South Africa. The research is beneficial to marketers who intend to make use of this powerful conduit to target this vacillating cohort.',\n",
       "  'Big TAM in Oman: Exploring the promise of on-line banking, its adoption by customers and the challenges of banking in Oman.'],\n",
       " ['Factors that induce service choices are important in the consumer experience .',\n",
       "  'Demystifying the benefits and risks of Lean service innovation: a banking case study. Purpose – The purpose of this paper is to demystify Lean for service innovation by investigating its benefits and risks. Lean innovation is a relatively new approach which is advocated in management literature. Little scientific work about its practice exists in the field. Although the Lean innovation principles are clear, there is limited evidence about their impact on service innovation processes. Design/methodology/approach – From the knowledge-based view (KBV) of the firm, a framework for understanding Lean innovation is developed. Using this framework, the benefits and risks of Lean innovation are analyzed in a case study. Findings – The case study not only shows that Lean service innovation can have many advantages, but also draws attention to the risks. The risks might result in the inability to follow Lean principles and might hamper the realization of the benefits. Using the case studies, study mitigation mechanisms are identified. Originality/value – This research offers a new knowledge perspect...',\n",
       "  \"An Empirical Investigation of the Factors Affecting Data Warehousing Success. The IT implementation literature suggests that various implementation factors play critical roles in the success of an information system; however, there is little empirical research about the implementation of data warehousing projects. Data warehousing has unique characteristics that may impact the importance of factors that apply to it. In this study, a cross-sectional survey investigated a model of data warehousing success. Data warehousing managers and data suppliers from 111 organizations completed paired mail questionnaires on implementation factors and the success of the warehouse. The results from a Partial Least Squares analysis of the data identified significant relationships between the system quality and data quality factors and perceived net benefits. It was found that management support and resources help to address organizational issues that arise during warehouse implementations; resources, user participation, and highly-skilled project team members increase the likelihood that warehousing projects will finish on-time, on-budget, with the right functionality; and diverse, unstandardized source systems and poor development technology will increase the technical issues that project teams must overcome. The implementation's success with organizational and project issues, in turn, influence the system quality of the data warehouse; however, data quality is best explained by factors not included in the research model.\"],\n",
       " ['In the extant literature, scholars have argued that effort expectancy has a strong effect on behavioural intention .',\n",
       "  'Consumer Acceptance and Use of Information Technology: Extending the Unified Theory of Acceptance and Use of Technology. This paper extends the unified theory of acceptance and use of technology (UTAUT) to study acceptance and use of technology in a consumer context. Our proposed UTAUT2 incorporates three constructs into UTAUT: hedonic motivation, price value, and habit. Individual differences--namely, age, gender, and experience--are hypothesized to moderate the effects of these constructs on behavioral intention and technology use. Results from a two-stage online survey, with technology use data collected four months after the first survey, of 1,512 mobile Internet consumers supported our model. Compared to UTAUT, the extensions proposed in UTAUT2 produced a substantial improvement in the variance explained in behavioral intention (56 percent to 74 percent) and technology use (40 percent to 52 percent). The theoretical and managerial implications of these results are discussed.',\n",
       "  'Integrating TTF and UTAUT to explain mobile banking user adoption.'],\n",
       " ['Previous studies have argued that hedonic motivation in the information and communications technology (ICT) environment accelerates the behavioural intention to purchase products using new systems .',\n",
       "  'Understanding online purchase intentions: contributions from technology and trust perspectives. ',\n",
       "  \"An Empirical Investigation of the Factors Affecting Data Warehousing Success. The IT implementation literature suggests that various implementation factors play critical roles in the success of an information system; however, there is little empirical research about the implementation of data warehousing projects. Data warehousing has unique characteristics that may impact the importance of factors that apply to it. In this study, a cross-sectional survey investigated a model of data warehousing success. Data warehousing managers and data suppliers from 111 organizations completed paired mail questionnaires on implementation factors and the success of the warehouse. The results from a Partial Least Squares analysis of the data identified significant relationships between the system quality and data quality factors and perceived net benefits. It was found that management support and resources help to address organizational issues that arise during warehouse implementations; resources, user participation, and highly-skilled project team members increase the likelihood that warehousing projects will finish on-time, on-budget, with the right functionality; and diverse, unstandardized source systems and poor development technology will increase the technical issues that project teams must overcome. The implementation's success with organizational and project issues, in turn, influence the system quality of the data warehouse; however, data quality is best explained by factors not included in the research model.\"],\n",
       " ['The UTAUT2 model compared with the TAM model demonstrated an improvement in the variance explained for behavioural intention (from 56% to 74%) and use behaviour (from 40% to 52%) .',\n",
       "  'Consumer Acceptance and Use of Information Technology: Extending the Unified Theory of Acceptance and Use of Technology. This paper extends the unified theory of acceptance and use of technology (UTAUT) to study acceptance and use of technology in a consumer context. Our proposed UTAUT2 incorporates three constructs into UTAUT: hedonic motivation, price value, and habit. Individual differences--namely, age, gender, and experience--are hypothesized to moderate the effects of these constructs on behavioral intention and technology use. Results from a two-stage online survey, with technology use data collected four months after the first survey, of 1,512 mobile Internet consumers supported our model. Compared to UTAUT, the extensions proposed in UTAUT2 produced a substantial improvement in the variance explained in behavioral intention (56 percent to 74 percent) and technology use (40 percent to 52 percent). The theoretical and managerial implications of these results are discussed.',\n",
       "  'Shops offer the e-tail experience. E-commerce has transformed retail over the last 15 years, but the widely held presumption that online purchasing would spell the demise of bricks-and-mortar shopping has now been supplanted by a more nuanced vision of our shopping future. Recent developments in advanced interactive technology have enabled shrewd retailers to mine the idea that online/offline shopping is not an either/or proposition, and that there is a chance to engage customers in new ways on the physical shopfloor. Indeed, rather than diminishing the traditional shopping experience, techniques that have been the preserve of the online shop are to some extent now informing the new in-store retail technology.'],\n",
       " ['The Moderating Effect of Generation Venkatesh et al argued that gender, age, and experience differentiated the effects on behavioural intention.',\n",
       "  'Consumer Acceptance and Use of Information Technology: Extending the Unified Theory of Acceptance and Use of Technology. This paper extends the unified theory of acceptance and use of technology (UTAUT) to study acceptance and use of technology in a consumer context. Our proposed UTAUT2 incorporates three constructs into UTAUT: hedonic motivation, price value, and habit. Individual differences--namely, age, gender, and experience--are hypothesized to moderate the effects of these constructs on behavioral intention and technology use. Results from a two-stage online survey, with technology use data collected four months after the first survey, of 1,512 mobile Internet consumers supported our model. Compared to UTAUT, the extensions proposed in UTAUT2 produced a substantial improvement in the variance explained in behavioral intention (56 percent to 74 percent) and technology use (40 percent to 52 percent). The theoretical and managerial implications of these results are discussed.',\n",
       "  'The effect of perceived trust on electronic commerce: Shopping online for tourism products and services in South Korea.'],\n",
       " ['Apart from these constructs, previous studies also found that innovativeness and trust have an effect .',\n",
       "  'Perceived Usefulness, Perceived Ease of Use, and User Acceptance of Information Technology. Valid measurement scales for predicting user acceptance of computers are in short supply. Most subjective measures used in practice are unvalidated, and their relationship to system usage is unknown. The present research develops and validates new scales for two specific variables, perceived usefulness and perceived ease of use, which are hypothesized to be fundamental determinants of user acceptance. Definitions of these two variables were used to develop scale items that were pretested for content validity and then tested for reliability and construct validity in two studies involving a total of 152 users and four application programs. The measures were refined and streamlined, resulting in two six-item scales with reliabilities of .98 for usefulness and .94 for ease of use. The scales exhibited hgih convergent, discriminant, and factorial validity. Perceived usefulness was significnatly correlated with both self-reported current usage r = .63, Study 1) and self-predicted future usage r = .85, Study 2). Perceived ease of use was also significantly correlated with current usage r = .45, Study 1) and future usage r = .59, Study 2). In both studies, usefulness had a signficnatly greater correaltion with usage behavior than did ease of use. Regression analyses suggest that perceived ease of use may actually be a causal antecdent to perceived usefulness, as opposed to a parallel, direct determinant of system usage. Implications are drawn for future research on user acceptance.',\n",
       "  'Unplugged: Exploring the costs and benefits of constant connection.'],\n",
       " ['The main consumers of BOPIS shopping are millennials .',\n",
       "  'Key Factors for In-Store Smartphone Use in an Omnichannel Experience: Millennials vs. Nonmillennials. The in-store use of smartphones is revolutionizing the customer journey and has the potential to become an important driver in the omnichannel context. This paper aims at identifying the key factors that influence customers’ intentions to use smartphones in-store and their actual behavior and to test the moderating effect of age, differentiating between millennials and nonmillennials, as millennials are considered digital natives and early adopters of new technologies. We applied the UTAUT2 model to a sample of 1043 Spanish customers, tested it using structural equations, and performed a multigroup analysis to compare the results between the two groups. The results show that the model explains both the behavioral intention to use a smartphone in a brick-and-mortar store and use behavior. The UTAUT2 predictors found to be most important were habit, performance expectancy, and hedonic motivation. However, the study shows that the only difference between millennials and nonmillennials with regard to the use of smartphones in-store is the effects of behavioral intention and habit on use behavior. The study adds to the existing knowledge by providing evidence in support of the validity of UTAUT2 as an appropriate theoretical basis to explain effectively behavioral intention, specifically the in-store use of smartphones.',\n",
       "  'Exploring consumers perceived risk and trust for mobile shopping: A theoretical framework and empirical study.'],\n",
       " ['Millennial cohorts are information gathering social connecters, game changers, and trend setters .',\n",
       "  'Unplugged: Exploring the costs and benefits of constant connection. ',\n",
       "  'Perceived Usefulness, Perceived Ease of Use, and User Acceptance of Information Technology. Valid measurement scales for predicting user acceptance of computers are in short supply. Most subjective measures used in practice are unvalidated, and their relationship to system usage is unknown. The present research develops and validates new scales for two specific variables, perceived usefulness and perceived ease of use, which are hypothesized to be fundamental determinants of user acceptance. Definitions of these two variables were used to develop scale items that were pretested for content validity and then tested for reliability and construct validity in two studies involving a total of 152 users and four application programs. The measures were refined and streamlined, resulting in two six-item scales with reliabilities of .98 for usefulness and .94 for ease of use. The scales exhibited hgih convergent, discriminant, and factorial validity. Perceived usefulness was significnatly correlated with both self-reported current usage r = .63, Study 1) and self-predicted future usage r = .85, Study 2). Perceived ease of use was also significantly correlated with current usage r = .45, Study 1) and future usage r = .59, Study 2). In both studies, usefulness had a signficnatly greater correaltion with usage behavior than did ease of use. Regression analyses suggest that perceived ease of use may actually be a causal antecdent to perceived usefulness, as opposed to a parallel, direct determinant of system usage. Implications are drawn for future research on user acceptance.'],\n",
       " ['In some previous studies, effort expectations were not supported .',\n",
       "  'Integrating TTF and UTAUT to explain mobile banking user adoption. ',\n",
       "  'Web‐shoppers and non‐shoppers: compatibility, relative advantage and demographics. This study investigated differences between Web‐shoppers and non‐shoppers, in terms of compatibility, relative advantage and demographics. Stepwise discriminant analysis was applied on a sample of 165 personally interviewed consumers, which showed that compatibility and relative advantage were overall successful, whereas, demographics were unsuccessful, in distinguishing Web‐shoppers from non‐shoppers. Significant variables included three factors of compatibility (use of direct shopping; use of Web browsing activities at home; and use of Web browsing activities at the office), and two factors of relative advantage (motives; and impediments). Managerial implications for targeting prospective Web‐shoppers and designing better‐grounded consumer Web‐marketing strategies are also discussed, together with study limitations and directions for future research.'],\n",
       " ['Kim et al studied from the perspective of consumers, and they confirmed that the choice of BOPIS was influenced by location convenience and product types.',\n",
       "  'Determinants of the intention to use Buy-Online, Pickup In-Store (BOPS): The moderating effects of situational factors and product type. ',\n",
       "  'Research Commentary - Digital Natives and Ubiquitous Information Systems. Most information systems research until now has focused on information systems in organizations and their use by digital immigrants. Digital immigrants are those who were not born into the digital world---they learnt to use information systems at some stage in their adult lives. An underlying assumption of much of this research is that users “resist” technology or at least have some difficulty in accepting it. Digital natives, conversely, are those who have grown up in a world where the use of information and communications technology is pervasive and ubiquitous. These ubiquitous technologies, networks, and associated systems have proliferated and have woven themselves into the very fabric of everyday life. This article suggests that the rise of the digital native, along with the growth of ubiquitous information systems (UIS), potentially represents a fundamental shift in our “paradigm” for IS research. We propose a research agenda that focuses on digital natives and UIS.'],\n",
       " ['We analysed the effect of omni-channel BOPIS services on purchasing using the UTAUT2 model proposed by Venkatesh et al .',\n",
       "  'Consumer Acceptance and Use of Information Technology: Extending the Unified Theory of Acceptance and Use of Technology. This paper extends the unified theory of acceptance and use of technology (UTAUT) to study acceptance and use of technology in a consumer context. Our proposed UTAUT2 incorporates three constructs into UTAUT: hedonic motivation, price value, and habit. Individual differences--namely, age, gender, and experience--are hypothesized to moderate the effects of these constructs on behavioral intention and technology use. Results from a two-stage online survey, with technology use data collected four months after the first survey, of 1,512 mobile Internet consumers supported our model. Compared to UTAUT, the extensions proposed in UTAUT2 produced a substantial improvement in the variance explained in behavioral intention (56 percent to 74 percent) and technology use (40 percent to 52 percent). The theoretical and managerial implications of these results are discussed.',\n",
       "  'User Acceptance of Information Technology: Toward a Unified View. Information technology (IT) acceptance research has yielded many competing models, each with different sets of acceptance determinants. In this paper, we (1) review user acceptance literature and discuss eight prominent models, (2) empirically compare the eight models and their extensions, (3) formulate a unified model that integrates elements across the eight models, and (4) empirically validate the unified model. The eight models reviewed are the theory of reasoned action, the technology acceptance model, the motivational model, the theory of planned behavior, a model combining the technology acceptance model and the theory of planned behavior, the model of PC utilization, the innovation diffusion theory, and the social cognitive theory. Using data from four organizations over a six-month period with three points of measurement, the eight models explained between 17 percent and 53 percent of the variance in user intentions to use information technology. Next, a unified model, called the Unified Theory of Acceptance and Use of Technology (UTAUT), was formulated, with four core determinants of intention and usage, and up to four moderators of key relationships. UTAUT was then tested using the original data and found to outperform the eight individual models (adjusted R2 of 69 percent). UTAUT was then confirmed with data from two new organizations with similar results (adjusted R2 of 70 percent). UTAUT thus provides a useful tool for managers needing to assess the likelihood of success for new technology introductions and helps them understand the drivers of acceptance in order to proactively design interventions (including training, marketing, etc.) targeted at populations of users that may be less inclined to adopt and use new systems. The paper also makes several recommendations for future research including developing a deeper understanding of the dynamic influences studied here, refining measurement of the core constructs used in UTAUT, and understanding the organizational outcomes associated with new technology use.'],\n",
       " ['The average variance extracted versus shared variance method (AVE-SV) and the overlapping confidence intervals approach, are commonly used discriminant validity test methods in structural equation mode .',\n",
       "  'Research Commentary - Digital Natives and Ubiquitous Information Systems. Most information systems research until now has focused on information systems in organizations and their use by digital immigrants. Digital immigrants are those who were not born into the digital world---they learnt to use information systems at some stage in their adult lives. An underlying assumption of much of this research is that users “resist” technology or at least have some difficulty in accepting it. Digital natives, conversely, are those who have grown up in a world where the use of information and communications technology is pervasive and ubiquitous. These ubiquitous technologies, networks, and associated systems have proliferated and have woven themselves into the very fabric of everyday life. This article suggests that the rise of the digital native, along with the growth of ubiquitous information systems (UIS), potentially represents a fundamental shift in our “paradigm” for IS research. We propose a research agenda that focuses on digital natives and UIS.',\n",
       "  'The influence of Facebook advertising on cognitive attitudes amid Generation Y. Social media has irrevocably transformed the manner in which society communicates and has altered perceptions and attitudes. The proliferation of Facebook usage has connected consumers to each other, to marketers and to brands in a manner that is as inventive, and has altered the world as we know it. However, research on Facebook is still in its infancy in an emerging country such as South Africa. Generation Y is a significant consumer group and, consequently, their attitudes towards social media advertising are of significant importance to marketers. This has triggered the question whether the largest social medium’s marketing communications effectively reaches young adults in South Africa, and what influence it has on their cognitive attitudes, which leads to their buying behavior. The results suggest that Facebook advertising has a favorable effect on the awareness and knowledge hierarchy-of-effects model levels amongst Generation Y in South Africa. The research is beneficial to marketers who intend to make use of this powerful conduit to target this vacillating cohort.'],\n",
       " ['Millennials are trend setters, social connecters, and omni-channel shopping leaders .',\n",
       "  'Unplugged: Exploring the costs and benefits of constant connection. ',\n",
       "  'Exploring consumers perceived risk and trust for mobile shopping: A theoretical framework and empirical study.'],\n",
       " ['Thus, we reconfigured and applied UTAUT2, a model borrowed from Venkatesh et al .',\n",
       "  'Consumer Acceptance and Use of Information Technology: Extending the Unified Theory of Acceptance and Use of Technology. This paper extends the unified theory of acceptance and use of technology (UTAUT) to study acceptance and use of technology in a consumer context. Our proposed UTAUT2 incorporates three constructs into UTAUT: hedonic motivation, price value, and habit. Individual differences--namely, age, gender, and experience--are hypothesized to moderate the effects of these constructs on behavioral intention and technology use. Results from a two-stage online survey, with technology use data collected four months after the first survey, of 1,512 mobile Internet consumers supported our model. Compared to UTAUT, the extensions proposed in UTAUT2 produced a substantial improvement in the variance explained in behavioral intention (56 percent to 74 percent) and technology use (40 percent to 52 percent). The theoretical and managerial implications of these results are discussed.',\n",
       "  'Understanding the role of competition in video gameplay satisfaction.'],\n",
       " ['The advances in social media platforms have translated into a drastically larger influence of communities; social influence will, thus, have a strong effect on behavioural intention .',\n",
       "  'Factors influencing adoption of mobile banking by Jordanian bank customers: Extending UTAUT2 with trust. ',\n",
       "  'Big TAM in Oman: Exploring the promise of on-line banking, its adoption by customers and the challenges of banking in Oman.'],\n",
       " ['To create this model, Venkatesh et al reviewed and synthesised aspects of the following models: the theory of reasoned action (TRA), the technology acceptance model (TAM), the motivational model, the theory of planned behaviour (TPB), the decomposed theory of planned behaviour, the combined TAM-TBP model (OTAM-CBT), the model of personal computer utilisation (MPCU), the innovation diffusion theory (IDT), and the social cognitive theory (SCT).',\n",
       "  'Consumer Acceptance and Use of Information Technology: Extending the Unified Theory of Acceptance and Use of Technology. This paper extends the unified theory of acceptance and use of technology (UTAUT) to study acceptance and use of technology in a consumer context. Our proposed UTAUT2 incorporates three constructs into UTAUT: hedonic motivation, price value, and habit. Individual differences--namely, age, gender, and experience--are hypothesized to moderate the effects of these constructs on behavioral intention and technology use. Results from a two-stage online survey, with technology use data collected four months after the first survey, of 1,512 mobile Internet consumers supported our model. Compared to UTAUT, the extensions proposed in UTAUT2 produced a substantial improvement in the variance explained in behavioral intention (56 percent to 74 percent) and technology use (40 percent to 52 percent). The theoretical and managerial implications of these results are discussed.',\n",
       "  'From Multi-Channel Retailing to Omni-Channel Retailing: Introduction to the Special Issue on Multi-Channel Retailing.'],\n",
       " ['Use behaviour 4 The degree to which consumers actually use the BOPIS service .',\n",
       "  'User Acceptance of Information Technology: Toward a Unified View. Information technology (IT) acceptance research has yielded many competing models, each with different sets of acceptance determinants. In this paper, we (1) review user acceptance literature and discuss eight prominent models, (2) empirically compare the eight models and their extensions, (3) formulate a unified model that integrates elements across the eight models, and (4) empirically validate the unified model. The eight models reviewed are the theory of reasoned action, the technology acceptance model, the motivational model, the theory of planned behavior, a model combining the technology acceptance model and the theory of planned behavior, the model of PC utilization, the innovation diffusion theory, and the social cognitive theory. Using data from four organizations over a six-month period with three points of measurement, the eight models explained between 17 percent and 53 percent of the variance in user intentions to use information technology. Next, a unified model, called the Unified Theory of Acceptance and Use of Technology (UTAUT), was formulated, with four core determinants of intention and usage, and up to four moderators of key relationships. UTAUT was then tested using the original data and found to outperform the eight individual models (adjusted R2 of 69 percent). UTAUT was then confirmed with data from two new organizations with similar results (adjusted R2 of 70 percent). UTAUT thus provides a useful tool for managers needing to assess the likelihood of success for new technology introductions and helps them understand the drivers of acceptance in order to proactively design interventions (including training, marketing, etc.) targeted at populations of users that may be less inclined to adopt and use new systems. The paper also makes several recommendations for future research including developing a deeper understanding of the dynamic influences studied here, refining measurement of the core constructs used in UTAUT, and understanding the organizational outcomes associated with new technology use.',\n",
       "  'STRUCTURAL EQUATION MODELING IN PRACTICE: A REVIEW AND RECOMMENDED TWO-STEP APPROACH. In this article, we provide guidance for substantive researchers on the use of structural equation modeling in practice for theory testing and development. We present a comprehensive, two-step modeling approach that employs a series of nested models and sequential chi-square difference tests. We discuss the comparative advantages of this approach over a one-step approach. Considerations in specification, assessment of fit, and respecification of measurement models using confirmatory factor analysis are reviewed. As background to the two-step approach, the distinction between exploratory and confirmatory analysis, the distinction between complementary approaches for theory testing versus predictive application, and some developments in estimation methods also are discussed.'],\n",
       " ['Hedonic motivation 3 The extent to which consumers enjoy the BOPIS system .',\n",
       "  'Consumer Acceptance and Use of Information Technology: Extending the Unified Theory of Acceptance and Use of Technology. This paper extends the unified theory of acceptance and use of technology (UTAUT) to study acceptance and use of technology in a consumer context. Our proposed UTAUT2 incorporates three constructs into UTAUT: hedonic motivation, price value, and habit. Individual differences--namely, age, gender, and experience--are hypothesized to moderate the effects of these constructs on behavioral intention and technology use. Results from a two-stage online survey, with technology use data collected four months after the first survey, of 1,512 mobile Internet consumers supported our model. Compared to UTAUT, the extensions proposed in UTAUT2 produced a substantial improvement in the variance explained in behavioral intention (56 percent to 74 percent) and technology use (40 percent to 52 percent). The theoretical and managerial implications of these results are discussed.',\n",
       "  'User Acceptance of Information Technology: Toward a Unified View. Information technology (IT) acceptance research has yielded many competing models, each with different sets of acceptance determinants. In this paper, we (1) review user acceptance literature and discuss eight prominent models, (2) empirically compare the eight models and their extensions, (3) formulate a unified model that integrates elements across the eight models, and (4) empirically validate the unified model. The eight models reviewed are the theory of reasoned action, the technology acceptance model, the motivational model, the theory of planned behavior, a model combining the technology acceptance model and the theory of planned behavior, the model of PC utilization, the innovation diffusion theory, and the social cognitive theory. Using data from four organizations over a six-month period with three points of measurement, the eight models explained between 17 percent and 53 percent of the variance in user intentions to use information technology. Next, a unified model, called the Unified Theory of Acceptance and Use of Technology (UTAUT), was formulated, with four core determinants of intention and usage, and up to four moderators of key relationships. UTAUT was then tested using the original data and found to outperform the eight individual models (adjusted R2 of 69 percent). UTAUT was then confirmed with data from two new organizations with similar results (adjusted R2 of 70 percent). UTAUT thus provides a useful tool for managers needing to assess the likelihood of success for new technology introductions and helps them understand the drivers of acceptance in order to proactively design interventions (including training, marketing, etc.) targeted at populations of users that may be less inclined to adopt and use new systems. The paper also makes several recommendations for future research including developing a deeper understanding of the dynamic influences studied here, refining measurement of the core constructs used in UTAUT, and understanding the organizational outcomes associated with new technology use.']]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86934"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
