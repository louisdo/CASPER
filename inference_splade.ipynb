{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "expired-priority",
   "metadata": {},
   "source": [
    "# SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becoming-television",
   "metadata": {},
   "source": [
    "This notebook gives a minimal example usage of SPLADE.\n",
    "\n",
    "* We provide models via Hugging Face (https://huggingface.co/naver)\n",
    "* See [Naver Labs Europe website](https://europe.naverlabs.com/research/machine-learning-and-optimization/splade-models/) for other intermediate models.\n",
    "\n",
    "| model | MRR@10 (MS MARCO dev) | recall@1000 (MS MARCO dev) | expected FLOPS | ~ avg q length | ~ avg d length | \n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| `naver/splade_v2_max` (**v2** [HF](https://huggingface.co/naver/splade_v2_max)) | 34.0 | 96.5 | 1.32 | 18 | 92 |\n",
    "| `naver/splade_v2_distil` (**v2** [HF](https://huggingface.co/naver/splade_v2_distil)) | 36.8 | 97.9 | 3.82 | 25 | 232 |\n",
    "| `naver/splade-cocondenser-selfdistil` (**v2bis**, [HF](https://huggingface.co/naver/splade-cocondenser-selfdistil))| 37.6 | 98.4 | 2.32 | 56 | 134 |\n",
    "| `naver/splade-cocondenser-ensembledistil` (**v2bis**, [HF](https://huggingface.co/naver/splade-cocondenser-ensembledistil)) | 38.3 | 98.3  | 1.85 | 44 | 120 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "tight-milton",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lamdo/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch, os, string\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "from splade.models.transformer_rep import SpladeMaxSim, Splade\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bbbeba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "mighty-invention",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the dir for trained weights\n",
    "\n",
    "##### v2\n",
    "# model_type_or_dir = \"naver/splade_v2_max\"\n",
    "# model_type_or_dir = \"naver/splade_v2_distil\"\n",
    "\n",
    "### v2bis, directly download from Hugging Face\n",
    "# model_type_or_dir = \"naver/splade-cocondenser-selfdistil\"\n",
    "# model_type_or_dir = \"naver/splade-cocondenser-ensembledistil\"\n",
    "model_type_or_dir = \"/scratch/lamdo/phrase_splade_checkpoints/phrase_splade_38/debug/checkpoint/model\"\n",
    "# model_type_or_dir = \"/scratch/lamdo/splade_maxsim_ckpts/splade_maxsim_150k_lowregv3/debug/checkpoint/model\"\n",
    "# model_type_or_dir = 'lamdo/distilbert-base-uncased-phrase-16kaddedphrasesfroms2orc-mlm-150000steps-multiwords'\n",
    "# model_type_or_dir = \"/scratch/lamdo/splade_checkpoints/experiments_combined_references_v8-1/debug/checkpoint/model\"\n",
    "# model_type_or_dir = \"lamdo/distilbert-base-uncased-phrase-60kaddedphrasesfroms2orc-mlm-150000steps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "centered-watershed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79577"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading model and tokenizer\n",
    "\n",
    "model = Splade(model_type_or_dir, agg=\"max\")\n",
    "model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_type_or_dir)\n",
    "reverse_voc = {v: k for k, v in tokenizer.vocab.items()}\n",
    "\n",
    "len(reverse_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ce3573e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Splade(\n",
       "  (transformer_rep): TransformerRep(\n",
       "    (transformer): DistilBertForMaskedLM(\n",
       "      (activation): GELUActivation()\n",
       "      (distilbert): DistilBertModel(\n",
       "        (embeddings): Embeddings(\n",
       "          (word_embeddings): Embedding(79577, 768, padding_idx=0)\n",
       "          (position_embeddings): Embedding(512, 768)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (transformer): Transformer(\n",
       "          (layer): ModuleList(\n",
       "            (0-5): 6 x TransformerBlock(\n",
       "              (attention): DistilBertSdpaAttention(\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (ffn): FFN(\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (activation): GELUActivation()\n",
       "              )\n",
       "              (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (vocab_transform): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (vocab_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (vocab_projector): Linear(in_features=768, out_features=79577, bias=True)\n",
       "      (mlm_loss_fct): CrossEntropyLoss()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53e869ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_custom(tokens, model, is_q = False):\n",
    "    out = model.encode_(tokens, is_q)[\"logits\"]  # shape (bs, pad_len, voc_size)\n",
    "    out = torch.log(1 + torch.relu(out)) * tokens[\"attention_mask\"].unsqueeze(-1)\n",
    "\n",
    "    # mask = ~torch.isin(tokens[\"input_ids\"], PUNCID)\n",
    "    # out = out * mask.unsqueeze(-1)\n",
    "\n",
    "    res = torch.zeros_like(out)\n",
    "    res = res.to(out.device)\n",
    "\n",
    "    out, token_indices = torch.max(out, dim = 1)\n",
    "\n",
    "\n",
    "    res.scatter_(1, token_indices.unsqueeze(1), out.unsqueeze(1))\n",
    "    return res\n",
    "\n",
    "\n",
    "PUNCID = torch.tensor([tokenizer.vocab[punc] for punc in string.punctuation])\n",
    "def encode_custom_mask_punc(tokens, model, is_q = False):\n",
    "    out = model.encode_(tokens, is_q)[\"logits\"]  # shape (bs, pad_len, voc_size)\n",
    "    out = torch.log(1 + torch.relu(out)) * tokens[\"attention_mask\"].unsqueeze(-1)\n",
    "\n",
    "    mask = ~torch.isin(tokens[\"input_ids\"], PUNCID)\n",
    "    out = out * mask.unsqueeze(-1)\n",
    "\n",
    "    res = torch.zeros_like(out)\n",
    "    res = res.to(out.device)\n",
    "\n",
    "    out, token_indices = torch.max(out, dim = 1)\n",
    "\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forward-movement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example document from MS MARCO passage collection (doc_id = 8003157)\n",
    "\n",
    "# doc = \"\"\"ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction. Neural information retrieval (IR) has greatly advanced search and other knowledge-intensive language tasks. While many neural IR methods encode queries and documents into single-vector representations, late interaction models produce multi-vector representations at the granularity of each token and decompose relevance modeling into scalable token-level computations. This decomposition has been shown to make late interaction more effective, but it inflates the space footprint of these models by an order of magnitude. In this work, we introduce ColBERTv2, a retriever that couples an aggressive residual compression mechanism with a denoised supervision strategy to simultaneously improve the quality and space footprint of late interaction. We evaluate ColBERTv2 across a wide range of benchmarks, establishing state-of-the-art quality within and outside the training domain while reducing the space footprint of late interaction models by 6--10×.\"\"\"\n",
    "\n",
    "# doc = \"\"\"Supplementing Remote Sensing of Ice: Deep Learning-Based Image Segmentation System for Automatic Detection and Localization of Sea-ice Formations From Close-Range Optical Images. This paper presents a three-stage approach for the automated analysis of close-range optical images containing ice objects. The proposed system is based on an ensemble of deep learning models and conditional random field postprocessing. The following surface ice formations were considered: Icebergs, Deformed ice, Level ice, Broken ice, Ice floes, Floebergs, Floebits, Pancake ice, and Brash ice. Additionally, five non-surface ice categories were considered: Sky, Open water, Shore, Underwater ice, and Melt ponds. To find input parameters for the approach, the performance of 12 different neural network architectures was explored and evaluated using a 5-fold cross-validation scheme. The best performance was achieved using an ensemble of models having pyramid pooling layers (PSPNet, PSPDenseNet, DeepLabV3+, and UPerNet) and convolutional conditional random field postprocessing with a mean intersection over union score of 0.799, and this outperformed the best single-model approach. The results of this study show that when per-class performance was considered, the Sky was the easiest class to predict, followed by Deformed ice and Open water. Melt pond was the most challenging class to predict. Furthermore, we have extensively explored the strengths and weaknesses of our approach and, in the process, discovered the types of scenes that pose a more significant challenge to the underlying neural networks. When coupled with optical sensors and AIS, the proposed approach can serve as a supplementary source of large-scale ‘ground truth’ data for validation of satellite-based sea-ice products. We have provided an implementation of the approach at https://github.com/panchinabil/sea_ice_segmentation .\"\"\"\n",
    "\n",
    "\n",
    "# doc = \"\"\"A comprehensive survey of graph embedding: Problems, techniques, and applications. Graph is an important data representation which appears in a wide diversity of real-world scenarios. Effective graph analytics provides users a deeper understanding of what is behind the data, and thus can benefit a lot of useful applications such as node classification, node recommendation, link prediction, etc. However, most graph analytics methods suffer the high computation and space cost. Graph embedding is an effective yet efficient way to solve the graph analytics problem. It converts the graph data into a low dimensional space in which the graph structural information and graph properties are maximumly preserved. In this survey, we conduct a comprehensive review of the literature in graph embedding. We first introduce the formal definition of graph embedding as well as the related concepts. After that, we propose two taxonomies of graph embedding which correspond to what challenges exist in different [MASK] [MASK]\"\"\"\n",
    "\n",
    "# doc = \"\"\"Attention Is All You Need. The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\"\"\"\n",
    "\n",
    "# doc = \"ERU-KG: Efficient Reference-aligned Unsupervised Keyphrase Generation\"\n",
    "\n",
    "# doc = \"\"\"ERU-KG: Efficient Reference-aligned Unsupervised Keyphrase Generation. Unsupervised keyphrase prediction has gained growing interest in recent years. However, existing methods typically rely on heuristically defined importance scores, which may lead to inaccurate informativeness estimation. In addition, they lack consideration for time efficiency. To solve these problems, we propose ERU-KG, an unsupervised keyphrase generation (UKG) model that consists of a phraseness and an informativeness module. The former generate candidates, while the latter estimate their relevance. The informativeness module innovates by learning to model informativeness through references (e.g., queries, citation contexts, and titles) and at the term-level, thereby 1) capturing how the key concepts of the document are perceived in different contexts and 2) estimate informativeness of phrases more efficiently by aggregating term informativeness, removing the need for explicit modeling of the candidates. ERU-KG demonstrates its effectiveness on keyphrase generation benchmarks by outperforming unsupervised baselines and achieving on average 89% of the performance of a supervised baseline for top 10 predictions. Additionally, to highlight its practical utility, we evaluate the model on text retrieval tasks and show that keyphrases generated by ERU-KG are effective when employed as query and document expansions. Finally, inference speed tests reveal that ERU-KG is the fastest among baselines of similar model sizes.\"\"\"\n",
    "\n",
    "# doc = \"\"\"SPLADE v2: Sparse Lexical and Expansion Model for Information Retrieval. In neural Information Retrieval (IR), ongoing research is directed towards improving the first retriever in ranking pipelines. Learning dense embeddings to conduct retrieval using efficient approximate nearest neighbors methods has proven to work well. Meanwhile, there has been a growing interest in learning \\emph{sparse} representations for documents and queries, that could inherit from the desirable properties of bag-of-words models such as the exact matching of terms and the efficiency of inverted indexes. Introduced recently, the SPLADE model provides highly sparse representations and competitive results with respect to state-of-the-art dense and sparse approaches. In this paper, we build on SPLADE and propose several significant improvements in terms of effectiveness and/or efficiency. More specifically, we modify the pooling mechanism, benchmark a model solely based on document expansion, and introduce models trained with distillation. We also report results on the BEIR benchmark. Overall, SPLADE is considerably improved with more than 9\\% gains on NDCG@10 on TREC DL 2019, leading to state-of-the-art results on the BEIR benchmark.\"\"\"\n",
    "\n",
    "# doc = \"\"\"The author uses 3 096 sample households in 15 counties from the year 1995 to 2006 to analyze the impact of PFPs on rural households' income inequality by income inequality decomposition.The research indicates that:(1) the percentage of subsidy income generated from PFPs has increased 8.03% during the period from 1995 to 2006;(2) the contribution of subsidy income generated from PFPs has been up from 0.330 7% in 1995 to 3.794 1% in 2006;(3) the policy-caused subsidy income inequality is more prominent than that caused by the planned regions of PFPs.Therefore a rational policy adjustment of PFPs will contribute more to poverty reduction in China's rural areas.\"\"\"\n",
    "\n",
    "# doc = \" | But much of the responsibility of the social inequity that leads to different health outcomes lies elsewhere. Health is affected by policies in other sectors, such as education, taxation, transport, and agriculture too.\"\n",
    "\n",
    "\n",
    "# doc = \"Gapped BLAST and PSI-BLAST: a new generation of protein database search programs. The BLAST programs are widely used tools for searching protein and DNA databases for sequence similarities. For protein comparisons, a variety of definitional, algorithmic and statistical refinements described here permits the execution time of the BLAST programs to be decreased substantially while enhancing their sensitivity to weak similarities. A new criterion for triggering the extension of word hits, combined with a new heuristic for generating gapped alignments, yields a gapped BLAST program that runs at approximately three times the speed of the original. In addition, a method is introduced for automatically combining statistically significant alignments produced by BLAST into a position-specific score matrix, and searching the database using this matrix. The resulting Position-Specific Iterated BLAST (PSIBLAST) program runs at approximately the same speed per iteration as gapped BLAST, but in many cases is much more sensitive to weak but biologically relevant sequence similarities. PSI-BLAST is used to uncover several new and interesting members of the BRCT superfamily.\"\n",
    "\n",
    "# doc = \"Generative Image Dynamics. We present an approach to modeling an image-space prior on scene motion. Our prior is learned from a collection of motion trajectories extracted from real video sequences depicting natural, oscillatory dynamics such as trees, flowers, candles, and clothes swaying in the wind. We model this dense, long-term motion prior in the Fourier domain:given a single image, our trained model uses a frequency-coordinated diffusion sampling process to predict a spectral volume, which can be converted into a motion texture that spans an entire video. Along with an image-based rendering module, these trajectories can be used for a number of downstream applications, such as turning still images into seamlessly looping videos, or allowing users to realistically interact with objects in real pictures by interpreting the spectral volumes as image-space modal bases, which approximate object dynamics.\"\n",
    "\n",
    "# doc = \"Rich Human Feedback for Text-to-Image Generation. Recent Text-to-Image (T2I) generation models such as Stable Diffusion and Imagen have made significant progress in generating high-resolution images based on text descriptions. However, many generated images still suffer from issues such as artifacts/implausibility, misalignment with text descriptions, and low aesthetic quality. Inspired by the success of Reinforcement Learning with Human Feedback (RLHF) for large language models, prior works collected human-provided scores as feedback on generated images and trained a reward model to improve the T2I generation. In this paper, we enrich the feedback signal by (i) marking image regions that are implausible or misaligned with the text, and (ii) annotating which words in the text prompt are misrepresented or missing on the image. We collect such rich human feedback on 18K generated images (RichHF-18K) and train a multimodal transformer to predict the rich feedback automatically. We show that the predicted rich human feedback can be leveraged to improve image generation, for example, by selecting high-quality training data to finetune and improve the generative models, or by creating masks with predicted heatmaps to inpaint the problematic regions. Notably, the improvements generalize to models (Muse) beyond those used to generate the images on which human feedback data were collected (Stable Diffusion variants)\"\n",
    "\n",
    "# doc = \"MedYOLO: A Medical Image Object Detection Framework. Artificial intelligence-enhanced identification of organs, lesions, and other structures in medical imaging is typically done using convolutional neural networks (CNNs) designed to make voxel-accurate segmentations of the region of interest. However, the labels required to train these CNNs are time-consuming to generate and require attention from subject matter experts to ensure quality. For tasks where voxel-level precision is not required, object detection models offer a viable alternative that can reduce annotation effort. Despite this potential application, there are few options for general purpose object detection frameworks available for 3-D medical imaging. We report on MedYOLO, a 3-D object detection framework using the one-shot detection method of the YOLO family of models and designed for use with medical imaging. We tested this model on four different datasets: BRaTS, LIDC, an abdominal organ Computed Tomography (CT) dataset, and an ECG-gated heart CT dataset. We found our models achieve high performance on commonly present medium and large-sized structures such as the heart, liver, and pancreas even without hyperparameter tuning. However, the models struggle with very small or rarely present structures.\"\n",
    "\n",
    "# doc = \"A study of smoothing methods for language models applied to ad hoc information retrieval. Language modeling approaches to information retrieval are attractive and promising because they connect the problem of retrieval with that of language model estimation, which has been studied extensively in other application areas such as speech recognition. The basic idea of these approaches is to estimate a language model for each document, and then rank documents by the likelihood of the query according to the estimated language model. A core problem in language model estimation is smoothing, which adjusts the maximum likelihood estimator so as to correct the inaccuracy due to data sparseness. In this paper, we study the problem of language model smoothing and its influence on retrieval performance. We examine the sensitivity of retrieval performance to the smoothing parameters and compare several popular smoothing methods on different test collection.\"\n",
    "\n",
    "# doc = \"Big data: astronomical or genomical? Genomics is a Big Data science and is going to get much bigger, very soon, but it is not known whether the needs of genomics will exceed other Big Data domains. Projecting to the year 2025, we compared genomics with three other major generators of Big Data: astronomy, YouTube, and Twitter. Our estimates show that genomics is a “four-headed beast”—it is either on par with or the most demanding of the domains analyzed here in terms of data acquisition, storage, distribution, and analysis. We discuss aspects of new technologies that will need to be developed to rise up and meet the computational challenges that genomics poses for the near future. Now is the time for concerted, community-wide planning for the “genomical” challenges of the next decade.\"\n",
    "\n",
    "# doc = \"Topic sentiment mixture: modeling facets and opinions in weblogs. In this paper, we define the problem of topic-sentiment analysis on Weblogs and propose a novel probabilistic model to capture the mixture of topics and sentiments simultaneously. The proposed Topic-Sentiment Mixture (TSM) model can reveal the latent topical facets in a Weblog collection, the subtopics in the results of an ad hoc query, and their associated sentiments. It could also provide general sentiment models that are applicable to any ad hoc topics. With a specifically designed HMM structure, the sentiment models and topic models estimated with TSM can be utilized to extract topic life cycles and sentiment dynamics. Empirical experiments on different Weblog datasets show that this approach is effective for modeling the topic facets and sentiments and extracting their dynamics from Weblog collections.\"\n",
    "\n",
    "\n",
    "# doc = \"Deep Residual Learning for Image Recognition. Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.\"\n",
    "\n",
    "# doc = \"Fairness in Dead-Reckoning based Distributed Multi-Player Games. In a distributed multi-player game that uses dead-reckoning vectors to exchange movement information among players, there is inaccuracy in rendering the objects at the receiver due to network delay between the sender and the receiver. The object is placed at the receiver at the position indicated by the dead-reckoning vector, but by that time, the real position could have changed considerably at the sender. This inaccuracy would be tolerable if it is consistent among all players; that is, at the same physical time, all players see inaccurate (with respect to the real position of the object) but the same position and trajectory for an object. But due to varying network delays between the sender and different receivers, the inaccuracy is different at different players as well. This leads to unfairness in game playing. In this paper, we first introduce an error measure for estimating this inaccuracy. Then we develop an algorithm for scheduling the sending of dead-reckoning vectors at a sender that strives to make this error equal at different receivers over time. This algorithm makes the game very fair at the expense of increasing the overall mean error of all players. To mitigate this effect, we propose a budget based algorithm that provides improved fairness without increasing the mean error thereby maintaining the accuracy of game playing. We have implemented both the scheduling algorithm and the budget based algorithm as part of BZFlag, a popular distributed multi-player game. We show through experiments that these algorithms provide fairness among players in spite of widely varying network delays. An additional property of the proposed algorithms is that they require less number of DRs to be exchanged (compared to the current implementation of BZflag) to achieve the same level of accuracy in game playing.\"\n",
    "\n",
    "# doc = \"Evaluating Adaptive Resource Management for Distributed Real-Time Embedded Systems. A challenging problem faced by researchers and developers of distributed real-time and embedded (DRE) systems is devising and implementing effective adaptive resource management strategies that can meet end-to-end quality of service (QoS) requirements in varying operational conditions. This paper presents two contributions to research in adaptive resource management for DRE systems. First, we describe the structure and functionality of the Hybrid Adaptive Resourcemanagement Middleware (HyARM), which provides adaptive resource management using hybrid control techniques for adapting to workload fluctuations and resource availability. Second, we evaluate the adaptive behavior of HyARM via experiments on a DRE multimedia system that distributes video in real-time. Our results indicate that HyARM yields predictable, stable, and high system performance, even in the face of fluctuating workload and resource availability.\"\n",
    "\n",
    "# doc = \"Real World BCI: Cross-Domain Learning and Practical Applications\"\n",
    "\n",
    "# doc = \"lda topic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c66d32d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/lamdo/splade/splade/models/transformer_rep.py:89: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast() if self.fp16 else NullContextManager():\n",
      "/home/lamdo/splade/splade/models/transformer_rep.py:32: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast() if self.fp16 else NullContextManager():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(45.4078)\n",
      "torch.Size([79577])\n",
      "number of actual dimensions:  84\n",
      "('darwin', 1.85)\n",
      "('domain', 1.69)\n",
      "('bci', 1.67)\n",
      "('real world', 1.48)\n",
      "('cross', 1.36)\n",
      "('company', 1.32)\n",
      "('learning', 1.31)\n",
      "('practical application', 1.26)\n",
      "('brain-computer interface', 1.14)\n",
      "('indonesia', 0.99)\n",
      "('eeg', 0.98)\n",
      "('application', 0.97)\n",
      "('gem', 0.95)\n",
      "('brain', 0.95)\n",
      "('practical', 0.93)\n",
      "('practice', 0.92)\n",
      "('real life', 0.9)\n",
      "('motor imagery', 0.9)\n",
      "('lstm', 0.89)\n",
      "('real', 0.88)\n",
      "('domains', 0.85)\n",
      "('ontology', 0.78)\n",
      "('moore', 0.77)\n",
      "('emg', 0.77)\n",
      "('signal', 0.73)\n",
      "('heart rate', 0.72)\n",
      "('biofeedback', 0.67)\n",
      "('language learning', 0.62)\n",
      "('culture', 0.61)\n",
      "('robot', 0.61)\n",
      "('p300', 0.6)\n",
      "('fingerprint recognition', 0.58)\n",
      "('ross', 0.58)\n",
      "('case study', 0.56)\n",
      "('building', 0.56)\n",
      "('prototype', 0.53)\n",
      "('synthetic', 0.53)\n",
      "('magnetic resonance', 0.52)\n",
      "('brain computer interface', 0.52)\n",
      "('humanoid robot', 0.5)\n",
      "('reality', 0.49)\n",
      "('qsar', 0.46)\n",
      "('demonstration', 0.46)\n",
      "('classification', 0.45)\n",
      "('migraine', 0.43)\n",
      "('mems', 0.41)\n",
      "('usability', 0.4)\n",
      "('domain structure', 0.37)\n",
      "('chinese medicine', 0.36)\n",
      "('generalisation', 0.36)\n",
      "('ratchet', 0.34)\n",
      "('petri nets', 0.33)\n",
      "('generalization', 0.31)\n",
      "('gait', 0.29)\n",
      "('case', 0.29)\n",
      "('medical', 0.28)\n",
      "('data mining', 0.28)\n",
      "('dance', 0.28)\n",
      "('acoustics', 0.27)\n",
      "('matrix', 0.25)\n",
      "('dft', 0.22)\n",
      "('csr', 0.21)\n",
      "('rasch model', 0.2)\n",
      "('utah', 0.2)\n",
      "('algorithm', 0.19)\n",
      "('applied', 0.19)\n",
      "('domain analysis', 0.19)\n",
      "('international business', 0.15)\n",
      "('china', 0.12)\n",
      "('and', 0.12)\n",
      "('transfer', 0.11)\n",
      "('research', 0.07)\n",
      "('group', 0.07)\n",
      "('matching', 0.05)\n",
      "('here', 0.04)\n",
      "('psychological', 0.04)\n",
      "('training', 0.04)\n",
      "('tea', 0.04)\n",
      "('british', 0.03)\n",
      "('spin', 0.03)\n",
      "('remote sensing', 0.03)\n",
      "('fmri', 0.03)\n",
      "('practical use', 0.02)\n",
      "('some', 0.0)\n"
     ]
    }
   ],
   "source": [
    "# now compute the document representation\n",
    "for punc in string.punctuation:\n",
    "    doc = doc.replace(punc, \" \")\n",
    "    \n",
    "doc_tokens = tokenizer(doc, max_length = 256, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    doc_rep = model(d_kwargs=doc_tokens)[\"d_rep\"].squeeze()  # (sparse) doc rep in voc space, shape (30522,)\n",
    "    print(torch.sum(doc_rep))\n",
    "    # doc_rep = encode_custom_mask_punc(doc_tokens, model).squeeze()\n",
    "print(doc_rep.shape)\n",
    "# get the number of non-zero dimensions in the rep:\n",
    "col = torch.nonzero(doc_rep).squeeze().cpu().tolist()\n",
    "print(\"number of actual dimensions: \", len(col))\n",
    "\n",
    "# now let's inspect the bow representation:\n",
    "weights = doc_rep[col].cpu().tolist()\n",
    "d = {k: v for k, v in zip(col, weights)}\n",
    "sorted_d = {k: v for k, v in sorted(d.items(), key=lambda item: item[1], reverse=True)}\n",
    "bow_rep = []\n",
    "for k, v in sorted_d.items():\n",
    "    print((reverse_voc[k], round(v, 2)))\n",
    "    bow_rep.append((reverse_voc[k], round(v, 2)))\n",
    "# print(\"SPLADE BOW rep:\\n\", bow_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "116d09cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.tokenize(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "rolled-canon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 233, 79577])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer(doc, return_tensors=\"pt\")\n",
    "out = encode_custom(tokens, model = model, is_q = False)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30d358f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_str = [reverse_voc[int(idx)] for idx in tokens[\"input_ids\"][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9f4971b",
   "metadata": {},
   "outputs": [],
   "source": [
    "row, col = torch.nonzero(out[0][:], as_tuple = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3f331e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_mapper = [[tokens_str[j], Counter()] for j in range(max(row) + 1)]\n",
    "for r,c in zip(row, col):\n",
    "    r_token_id = int(tokens[\"input_ids\"][0][r])\n",
    "    r_token_str = reverse_voc[r_token_id]\n",
    "\n",
    "    temp = {}\n",
    "\n",
    "    c_token_id = int(c)\n",
    "    c_token_str = reverse_voc[c_token_id]\n",
    "    if c_token_str not in temp:\n",
    "        temp[c_token_str] = float(out[0][r, c])\n",
    "\n",
    "    token_mapper[r][0] = r_token_str\n",
    "    token_mapper[r][1].update(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53d6a61e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['[CLS]',\n",
       "  Counter({'du': 0.4605824947357178,\n",
       "           'retired': 0.31978070735931396,\n",
       "           'skins': 0.2823777496814728,\n",
       "           'digital model': 0.23964962363243103,\n",
       "           'the': 0.2150944620370865,\n",
       "           'growth hormone': 0.15866422653198242,\n",
       "           'applications': 0.1541413515806198,\n",
       "           '##cher': 0.13340166211128235,\n",
       "           '2000': 0.13193988800048828,\n",
       "           'draft': 0.10153615474700928,\n",
       "           'vibrational spectroscopy': 0.07756754010915756,\n",
       "           'encephalopathy': 0.06918830424547195,\n",
       "           'e-health': 0.065287746489048,\n",
       "           '##hia': 0.03401720151305199,\n",
       "           'xanes': 0.03271549567580223,\n",
       "           'agriculture': 0.020593401044607162,\n",
       "           'emphasize': 0.007241904269903898,\n",
       "           'black': 0.0021865288726985455})],\n",
       " ['med', Counter({'med': 1.4884090423583984, 'clinic': 0.49059081077575684})],\n",
       " ['##yo', Counter()],\n",
       " ['##lo',\n",
       "  Counter({'patch': 0.6255648136138916,\n",
       "           '##lot': 0.32982271909713745,\n",
       "           'cleveland': 0.13886724412441254})],\n",
       " [':', Counter({'through': 0.02313060127198696})],\n",
       " ['a',\n",
       "  Counter({'temple': 0.27766653895378113,\n",
       "           'gene expression': 0.26604750752449036,\n",
       "           'military': 0.26098623871803284,\n",
       "           'mission': 0.2501056492328644,\n",
       "           'using': 0.11575782299041748,\n",
       "           'algorithms': 0.039118655025959015,\n",
       "           'intraocular pressure': 0.030305348336696625,\n",
       "           'wavelengths': 0.028172671794891357,\n",
       "           'diet': 0.015459113754332066,\n",
       "           'orbiting': 0.015221976675093174})],\n",
       " ['medical image',\n",
       "  Counter({'medical image': 1.2108694314956665,\n",
       "           'ct images': 0.6560217142105103,\n",
       "           'human action': 0.46326062083244324,\n",
       "           'medical': 0.45542776584625244,\n",
       "           'image edge': 0.35236939787864685,\n",
       "           'diagnosis': 0.2343769669532776,\n",
       "           'medical treatment': 0.20228387415409088,\n",
       "           'tv': 0.1951696276664734,\n",
       "           'tumor marker': 0.164454385638237,\n",
       "           'clinical trial': 0.15126216411590576,\n",
       "           'chinese medicine': 0.10691803693771362,\n",
       "           'modern': 0.09898516535758972,\n",
       "           'wi': 0.06722421199083328,\n",
       "           'health service': 0.06222696974873543,\n",
       "           'hyperspectral image': 0.04080352932214737,\n",
       "           'patient': 0.04079117253422737,\n",
       "           'registered': 0.025939147919416428})],\n",
       " ['object detection', Counter()],\n",
       " ['framework',\n",
       "  Counter({'framework': 1.193688154220581,\n",
       "           'workflow': 0.0012687736889347434})],\n",
       " ['.',\n",
       "  Counter({'disperse': 0.3514325022697449,\n",
       "           'premiere': 0.28791481256484985,\n",
       "           'rnas': 0.21888703107833862,\n",
       "           '.': 0.07015018165111542,\n",
       "           'keloid': 0.05514519661664963})],\n",
       " ['artificial intelligence',\n",
       "  Counter({'artificial intelligence': 0.5962511301040649,\n",
       "           'image': 0.536231517791748,\n",
       "           'image processing': 0.48501265048980713,\n",
       "           'intelligent': 0.0956893265247345,\n",
       "           'image analysis': 0.02956363558769226})],\n",
       " ['-',\n",
       "  Counter({'relevance': 0.30723854899406433,\n",
       "           'assessment tools': 0.1591303050518036,\n",
       "           'cloud service': 0.10229471325874329,\n",
       "           'pluralism': 0.06746281683444977})],\n",
       " ['enhanced',\n",
       "  Counter({'intensified': 0.5696448087692261,\n",
       "           'enhancement': 0.47911322116851807,\n",
       "           'enhanced': 0.24200522899627686,\n",
       "           'enhance': 0.18122565746307373})],\n",
       " ['identification',\n",
       "  Counter({'identification': 0.7090984582901001,\n",
       "           'recognition': 0.550685465335846,\n",
       "           'classification': 0.27667585015296936})],\n",
       " ['of',\n",
       "  Counter({'lu': 0.3757442831993103,\n",
       "           'for': 0.08576171845197678,\n",
       "           'representation': 0.02160000056028366})],\n",
       " ['organs',\n",
       "  Counter({'patients': 1.1151995658874512,\n",
       "           'tissue': 1.0230047702789307,\n",
       "           'tissues': 0.6635903716087341,\n",
       "           'blood vessel': 0.5383237600326538,\n",
       "           'human tissue': 0.47685158252716064,\n",
       "           'organs': 0.43501871824264526,\n",
       "           'limbs': 0.39006248116493225,\n",
       "           'anatomical structure': 0.3879886865615845,\n",
       "           'organ': 0.3822923004627228,\n",
       "           'biological tissue': 0.13478818535804749})],\n",
       " [',', Counter()],\n",
       " ['lesions',\n",
       "  Counter({'lesions': 0.29561173915863037,\n",
       "           'neck': 0.2291925847530365,\n",
       "           'diseases': 0.21834632754325867,\n",
       "           '##ben': 0.04820775240659714,\n",
       "           'stenosis': 0.0228939950466156})],\n",
       " [',',\n",
       "  Counter({'sockets': 0.30842331051826477,\n",
       "           'commissioners': 0.16430459916591644})],\n",
       " ['and', Counter()],\n",
       " ['other', Counter()],\n",
       " ['structures',\n",
       "  Counter({'reorganisation': 0.34129002690315247,\n",
       "           'arteries': 0.2423500269651413,\n",
       "           'morphological': 0.20563282072544098,\n",
       "           'materials': 0.14126014709472656,\n",
       "           'stroma': 0.13615645468235016})],\n",
       " ['in', Counter()],\n",
       " ['medical imaging',\n",
       "  Counter({'health': 0.670630931854248,\n",
       "           'ultrasound images': 0.1890268474817276,\n",
       "           'thorax': 0.14277710020542145})],\n",
       " ['is', Counter()],\n",
       " ['typically', Counter()],\n",
       " ['done',\n",
       "  Counter({'##yal': 0.26903143525123596,\n",
       "           'hyperspectral imaging': 0.018305763602256775})],\n",
       " ['using', Counter({'primary culture': 0.022185122594237328})],\n",
       " ['convolution', Counter({'convolution': 0.6936124563217163})],\n",
       " ['al', Counter()],\n",
       " ['neural network',\n",
       "  Counter({'neural network': 0.5465563535690308,\n",
       "           'machine learning': 0.13006606698036194})],\n",
       " ['s', Counter()],\n",
       " ['(', Counter()],\n",
       " ['cnn',\n",
       "  Counter({'networks': 0.6067525744438171,\n",
       "           'cnn': 0.5624194145202637,\n",
       "           'deep': 0.15540288388729095,\n",
       "           'deep learning': 0.1208733543753624})],\n",
       " ['##s', Counter()],\n",
       " [')', Counter()],\n",
       " ['designed', Counter()],\n",
       " ['to', Counter()],\n",
       " ['make',\n",
       "  Counter({'port': 0.3483373820781708, 'carving': 0.09522522985935211})],\n",
       " ['voxel', Counter()],\n",
       " ['-',\n",
       "  Counter({'worthy': 0.28111401200294495,\n",
       "           'gestures': 0.16884075105190277,\n",
       "           'postseason': 0.1591734141111374,\n",
       "           'yet': 0.06756655871868134,\n",
       "           'protection': 0.006851868238300085})],\n",
       " ['accurate', Counter({'accurate': 0.31830382347106934})],\n",
       " ['segmentation',\n",
       "  Counter({'segmentation': 0.9809404015541077,\n",
       "           'segment': 0.6610848307609558})],\n",
       " ['s', Counter({'maps': 0.3373008072376251})],\n",
       " ['of', Counter({'photogrammetry': 0.07783953100442886})],\n",
       " ['the', Counter()],\n",
       " ['region', Counter()],\n",
       " ['of', Counter({'##lative': 0.031086726114153862})],\n",
       " ['interest',\n",
       "  Counter({'study': 0.530454695224762,\n",
       "           'enlarged': 0.5022931098937988,\n",
       "           'interest': 0.07116687297821045,\n",
       "           'consideration': 0.024659791961312294})],\n",
       " ['.', Counter()],\n",
       " ['however', Counter()],\n",
       " [',', Counter()],\n",
       " ['the', Counter({'graphics': 0.01826069876551628})],\n",
       " ['labels',\n",
       "  Counter({'labels': 0.26424503326416016, 'label': 0.22182509303092957})],\n",
       " ['required', Counter()],\n",
       " ['to', Counter()],\n",
       " ['train', Counter()],\n",
       " ['these', Counter({'configured': 0.15867124497890472})],\n",
       " ['cnn', Counter()],\n",
       " ['##s', Counter({'sentiment analysis': 0.04176187515258789})],\n",
       " ['are', Counter()],\n",
       " ['time', Counter({'sirt': 0.2942736744880676})],\n",
       " ['-', Counter({'legislatures': 0.4092714488506317})],\n",
       " ['consuming', Counter()],\n",
       " ['to', Counter()],\n",
       " ['generate', Counter()],\n",
       " ['and', Counter({'conductors': 0.016446618363261223})],\n",
       " ['require', Counter()],\n",
       " ['attention', Counter({'attention': 0.450965940952301})],\n",
       " ['from', Counter()],\n",
       " ['subject matter',\n",
       "  Counter({'subject matter': 0.9567538499832153,\n",
       "           'topic': 0.4306587278842926,\n",
       "           'area': 0.38957247138023376,\n",
       "           'field': 0.38881486654281616})],\n",
       " ['experts',\n",
       "  Counter({'experts': 1.0025957822799683, 'expert': 0.1463383287191391})],\n",
       " ['to', Counter({'negotiations': 0.0019900058396160603})],\n",
       " ['ensure', Counter()],\n",
       " ['quality', Counter({'subjective quality': 0.274534672498703})],\n",
       " ['.', Counter({'combined heat': 0.43447691202163696})],\n",
       " ['for', Counter({'executions': 0.604387104511261})],\n",
       " ['tasks',\n",
       "  Counter({'adenoma': 0.4802824854850769,\n",
       "           'tasks': 0.32338616251945496,\n",
       "           'image retrieval': 0.08492196351289749})],\n",
       " ['where',\n",
       "  Counter({'authorization': 0.6744040250778198,\n",
       "           'yi': 0.036754705011844635,\n",
       "           'transportation': 0.031604766845703125})],\n",
       " ['voxel',\n",
       "  Counter({'voxel': 1.4355257749557495,\n",
       "           'point cloud': 0.4751839339733124,\n",
       "           'volume': 0.33786606788635254,\n",
       "           'volumetric': 0.323566734790802,\n",
       "           'point': 0.2668687105178833,\n",
       "           'pyramid': 0.20756085216999054,\n",
       "           'scene': 0.014986192807555199})],\n",
       " ['-',\n",
       "  Counter({'lay': 0.24572747945785522,\n",
       "           '##nded': 0.1365564912557602,\n",
       "           'carriage': 0.10582330077886581,\n",
       "           'weathered': 0.031066618859767914})],\n",
       " ['level',\n",
       "  Counter({'level': 0.7843330502510071,\n",
       "           'scale': 0.14593301713466644,\n",
       "           'levels': 0.11336913704872131,\n",
       "           'color': 0.06459970027208328})],\n",
       " ['precision',\n",
       "  Counter({'precision': 1.1704427003860474, 'accuracy': 0.24355240166187286})],\n",
       " ['is', Counter()],\n",
       " ['not', Counter({'2011': 0.06925883144140244})],\n",
       " ['required', Counter()],\n",
       " [',',\n",
       "  Counter({'docked': 0.49387532472610474,\n",
       "           'convicted': 0.39276322722435,\n",
       "           'firm': 0.3437481224536896})],\n",
       " ['object detection', Counter({'saliency': 0.030203569680452347})],\n",
       " ['models', Counter()],\n",
       " ['offer', Counter({'##lity': 0.16254663467407227})],\n",
       " ['a', Counter()],\n",
       " ['viable alternative',\n",
       "  Counter({'marginal': 0.4064711928367615,\n",
       "           'recommendation': 0.2307010143995285,\n",
       "           'viable alternative': 0.13553965091705322})],\n",
       " ['tha', Counter()],\n",
       " ['t c', Counter()],\n",
       " ['an', Counter({'##bbled': 0.5608344674110413})],\n",
       " ['reduce', Counter({'consumption': 0.05595114827156067})],\n",
       " ['annotation',\n",
       "  Counter({'annotation': 1.2094762325286865, 'sr': 0.2038140594959259})],\n",
       " ['effort', Counter({'content': 0.015352996066212654})],\n",
       " ['.', Counter({'lit': 0.5316305160522461})],\n",
       " ['despite', Counter()],\n",
       " ['this', Counter()],\n",
       " ['potential application', Counter({'implementation': 0.3361585736274719})],\n",
       " [',', Counter()],\n",
       " ['there', Counter()],\n",
       " ['are', Counter({'hotels': 0.012743468396365643})],\n",
       " ['few', Counter()],\n",
       " ['options', Counter()],\n",
       " ['for', Counter()],\n",
       " ['general', Counter()],\n",
       " ['purpose',\n",
       "  Counter({'purpose': 0.5390470623970032, 'sar': 0.28579849004745483})],\n",
       " ['object detection',\n",
       "  Counter({'object detection': 1.2835052013397217,\n",
       "           'detectors': 0.1506229192018509})],\n",
       " ['frameworks',\n",
       "  Counter({'frameworks': 0.6709259152412415,\n",
       "           'technologies': 0.5089192390441895})],\n",
       " ['available', Counter()],\n",
       " ['f', Counter({'dod': 0.3522401452064514, 'bulb': 0.2551153004169464})],\n",
       " ['##o', Counter({'##o': 0.3737483322620392})],\n",
       " ['r 3', Counter({'r 3': 0.07198065519332886})],\n",
       " ['-', Counter({'behalf': 0.43205392360687256})],\n",
       " ['d', Counter({'d': 0.619036853313446, '3d': 0.46631038188934326})],\n",
       " ['medical imaging',\n",
       "  Counter({'medical imaging': 0.6591327786445618,\n",
       "           'cancer': 0.500771164894104,\n",
       "           'mammography': 0.409626841545105,\n",
       "           '##py': 0.3735639750957489,\n",
       "           'research': 0.30502018332481384,\n",
       "           'cancers': 0.2956089973449707,\n",
       "           'texture analysis': 0.28991684317588806,\n",
       "           'health monitoring': 0.24484199285507202,\n",
       "           'medical diagnosis': 0.2093634307384491,\n",
       "           'clinical diagnosis': 0.17492759227752686,\n",
       "           'radiologists': 0.08160331100225449,\n",
       "           'radiology': 0.056434836238622665,\n",
       "           'cancer diagnosis': 0.018423739820718765})],\n",
       " ['.', Counter()],\n",
       " ['we', Counter()],\n",
       " ['report', Counter()],\n",
       " ['on', Counter()],\n",
       " ['med', Counter({'mud': 0.41014912724494934, 'robot': 0.08629966527223587})],\n",
       " ['##yo', Counter({'##yo': 1.456749677658081})],\n",
       " ['##lo',\n",
       "  Counter({'por': 0.37526455521583557,\n",
       "           'halo': 0.2409176379442215,\n",
       "           '##los': 0.20836159586906433,\n",
       "           'particle swarm optimization': 0.17629052698612213,\n",
       "           'cloud': 0.1387326717376709})],\n",
       " [',', Counter()],\n",
       " ['a', Counter({'opponents': 0.04918332025408745})],\n",
       " ['3',\n",
       "  Counter({'3': 0.8734519481658936,\n",
       "           '7': 0.23666316270828247,\n",
       "           'three': 0.13825491070747375,\n",
       "           '2013': 0.1219353899359703,\n",
       "           '13': 0.04835621640086174})],\n",
       " ['-', Counter()],\n",
       " ['d', Counter()],\n",
       " ['object detection',\n",
       "  Counter({'objects': 0.6213831305503845,\n",
       "           'detector': 0.3432706594467163,\n",
       "           'vehicle detection': 0.01093396358191967})],\n",
       " ['framework', Counter({'platform': 0.3793894350528717})],\n",
       " ['using', Counter({'pistols': 0.6611608266830444})],\n",
       " ['the', Counter()],\n",
       " ['one', Counter({'single': 0.0018354489002376795})],\n",
       " ['-', Counter()],\n",
       " ['shot',\n",
       "  Counter({'shot': 1.1363990306854248, 'person': 0.18490727245807648})],\n",
       " ['detection method',\n",
       "  Counter({'detection': 1.0355899333953857,\n",
       "           'detection method': 0.7580026388168335})],\n",
       " ['of', Counter()],\n",
       " ['the', Counter()],\n",
       " ['yo',\n",
       "  Counter({'yo': 1.784651279449463,\n",
       "           'you': 0.5309121012687683,\n",
       "           'y': 0.1339133381843567,\n",
       "           'by': 0.10298753529787064,\n",
       "           'ya': 0.0872839018702507})],\n",
       " ['##lo',\n",
       "  Counter({'##lo': 1.379321575164795,\n",
       "           'o': 0.49889761209487915,\n",
       "           'stuffed': 0.13409027457237244,\n",
       "           'community': 0.04290969669818878})],\n",
       " ['family', Counter({'gale': 0.20060425996780396})],\n",
       " ['of', Counter({'luminous': 0.13649502396583557})],\n",
       " ['models', Counter({'models': 0.6857348680496216})],\n",
       " ['and', Counter()],\n",
       " ['designed', Counter()],\n",
       " ['for', Counter()],\n",
       " ['use',\n",
       "  Counter({'guangzhou': 0.59950852394104,\n",
       "           'china': 0.09380703419446945,\n",
       "           'users': 0.05005747452378273,\n",
       "           'uplift': 0.04873653128743172})],\n",
       " ['with', Counter()],\n",
       " ['medical imaging',\n",
       "  Counter({'sara': 0.14880435168743134, 'residents': 0.10378734022378922})],\n",
       " ['.', Counter({'pump power': 0.46889281272888184})],\n",
       " ['we', Counter()],\n",
       " ['tested', Counter({'##val': 0.19311004877090454})],\n",
       " ['this', Counter()],\n",
       " ['model',\n",
       "  Counter({'process model': 0.2066716104745865, 'lake': 0.06614604592323303})],\n",
       " ['on', Counter({'econometric model': 0.043967220932245255})],\n",
       " ['four',\n",
       "  Counter({'##ellant': 0.7490181922912598,\n",
       "           '##nted': 0.4542648196220398,\n",
       "           '10': 0.031112149357795715})],\n",
       " ['different', Counter()],\n",
       " ['dataset',\n",
       "  Counter({'dataset': 0.3435879051685333, 'choroid': 0.29134419560432434})],\n",
       " ['s', Counter()],\n",
       " [':', Counter()],\n",
       " ['brat', Counter({'brat': 1.5045253038406372, 'pu': 0.07313709706068039})],\n",
       " ['##s', Counter({'sensors': 0.5513510704040527})],\n",
       " [',', Counter()],\n",
       " ['l', Counter({'l': 0.5655130743980408})],\n",
       " ['idc', Counter({'idc': 0.8937520980834961, 'emr': 0.2836954593658447})],\n",
       " [',', Counter()],\n",
       " ['an', Counter()],\n",
       " ['abdominal',\n",
       "  Counter({'abdominal': 0.10296258330345154,\n",
       "           'abdomen': 0.003390043042600155})],\n",
       " ['organ', Counter()],\n",
       " ['computed', Counter()],\n",
       " ['tomography', Counter({'tomographic imaging': 0.01634635217487812})],\n",
       " ['(', Counter({'railway': 0.006253556348383427})],\n",
       " ['ct', Counter()],\n",
       " [')', Counter()],\n",
       " ['dataset', Counter()],\n",
       " [',', Counter({'cap': 0.43521368503570557})],\n",
       " ['and', Counter()],\n",
       " ['an', Counter({'warship': 0.34164416790008545})],\n",
       " ['ecg',\n",
       "  Counter({'ecg': 0.5620979070663452, 'heart rate': 0.12203879654407501})],\n",
       " ['-', Counter({'hua': 0.5572530627250671, 'sponsors': 0.049755360931158066})],\n",
       " ['gate', Counter({'gate': 0.8553041219711304})],\n",
       " ['##d', Counter({'vibrating': 0.3654100298881531})],\n",
       " ['hear',\n",
       "  Counter({'hear': 1.0436153411865234,\n",
       "           'hearing': 0.5751321911811829,\n",
       "           'cochlea': 0.1698734015226364,\n",
       "           'call': 0.1605127900838852})],\n",
       " ['t c', Counter({'farmers': 0.21686050295829773})],\n",
       " ['t', Counter({'mechanical vibration': 0.10708855837583542})],\n",
       " ['dataset', Counter()],\n",
       " ['.',\n",
       "  Counter({'social problem': 0.23950664699077606,\n",
       "           'neutron activation': 0.11150483042001724})],\n",
       " ['we', Counter()],\n",
       " ['found', Counter()],\n",
       " ['our', Counter()],\n",
       " ['models', Counter({'olfactory bulb': 0.014287673868238926})],\n",
       " ['achieve', Counter()],\n",
       " ['high performance', Counter()],\n",
       " ['on',\n",
       "  Counter({'safety assessment': 0.3205028176307678,\n",
       "           'leaks': 0.05613374337553978})],\n",
       " ['commonly', Counter({'bern': 0.9380689859390259})],\n",
       " ['present', Counter()],\n",
       " ['medium', Counter({'medium': 0.10612373799085617})],\n",
       " ['and', Counter()],\n",
       " ['large', Counter({'large': 0.07811199873685837})],\n",
       " ['-',\n",
       "  Counter({'battery capacity': 0.18491362035274506,\n",
       "           'tibia': 0.11043848097324371})],\n",
       " ['sized', Counter()],\n",
       " ['structures',\n",
       "  Counter({'cells': 0.552078902721405,\n",
       "           'vibration response': 0.11305468529462814})],\n",
       " ['such', Counter()],\n",
       " ['as', Counter()],\n",
       " ['the', Counter()],\n",
       " ['heart',\n",
       "  Counter({'heart': 0.6897061467170715, 'larynx': 0.023402320221066475})],\n",
       " [',', Counter()],\n",
       " ['liver', Counter({'liver': 0.006273102946579456})],\n",
       " [',',\n",
       "  Counter({'visual function': 0.38157588243484497,\n",
       "           'reform': 0.307701975107193})],\n",
       " ['and', Counter()],\n",
       " ['pancreas',\n",
       "  Counter({'pancreas': 0.6979376673698425, 'blood': 0.18930506706237793})],\n",
       " ['even', Counter()],\n",
       " ['without', Counter({'doubt': 0.10417507588863373})],\n",
       " ['hyper', Counter({'hyper': 0.9155217409133911})],\n",
       " ['parameter tuning',\n",
       "  Counter({'tune': 1.0070111751556396,\n",
       "           'parameter tuning': 0.9919692277908325,\n",
       "           'tuning': 0.7213059663772583,\n",
       "           'optimization': 0.27579644322395325,\n",
       "           'enrichment': 0.2660672664642334,\n",
       "           'parameter optimization': 0.21344317495822906,\n",
       "           'search': 0.09104599803686142,\n",
       "           'forced': 0.055653855204582214,\n",
       "           'metabonomics': 0.041733063757419586})],\n",
       " ['.', Counter({'##loaded': 0.5572824478149414})],\n",
       " ['however', Counter()],\n",
       " [',', Counter()],\n",
       " ['the', Counter()],\n",
       " ['models', Counter()],\n",
       " ['struggle',\n",
       "  Counter({'sustainable development': 0.7677465677261353,\n",
       "           '##lag': 0.5665844082832336,\n",
       "           'peril': 0.07551871240139008,\n",
       "           'frequency stability': 0.04825228452682495})],\n",
       " ['with', Counter()],\n",
       " ['very', Counter({'pinus': 0.604762077331543, 'foods': 0.07326462119817734})],\n",
       " ['small', Counter()],\n",
       " ['or', Counter()],\n",
       " ['rarely', Counter({'life': 0.18357737362384796})],\n",
       " ['present', Counter()],\n",
       " ['structures',\n",
       "  Counter({'structure': 0.8309844732284546,\n",
       "           'structures': 0.6626634001731873,\n",
       "           'defects': 0.1663130819797516,\n",
       "           'membrane structure': 0.08809908479452133})],\n",
       " ['.',\n",
       "  Counter({'settle': 0.5311797261238098,\n",
       "           'driving method': 0.37176424264907837,\n",
       "           'qualification': 0.1257787048816681})],\n",
       " ['[SEP]',\n",
       "  Counter({'##dant': 0.7886182069778442,\n",
       "           '##pressed': 0.6304970979690552,\n",
       "           'embodied': 0.34603351354599,\n",
       "           'optical system': 0.25744539499282837,\n",
       "           '##holders': 0.20754534006118774,\n",
       "           'holders': 0.1507696509361267,\n",
       "           'vernacular': 0.0708838403224945,\n",
       "           '##iard': 0.05963713675737381,\n",
       "           'magnetic studies': 0.054324593394994736,\n",
       "           'values': 0.05069338530302048,\n",
       "           'aromatic': 0.014379098080098629})]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00dadfe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', 'these', 'rb', '##a', 'subsidiaries', 'were', 'involved', 'in', 'br']\n",
      "dict_keys(['by', '...', 'albert', 'this', 'martin', 'was', 'radio', ')', '\"'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({'...': 1.4506860971450806,\n",
       "         'was': 1.3149594068527222,\n",
       "         'radio': 1.3119200468063354,\n",
       "         '\"': 1.0273680686950684,\n",
       "         'by': 0.9747397899627686,\n",
       "         'this': 0.7900874018669128,\n",
       "         ')': 0.694354772567749,\n",
       "         'albert': 0.03176310285925865,\n",
       "         'martin': 0.013809965923428535})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_index =24\n",
    "end_index = 33\n",
    "print([item[0] for item in token_mapper[start_index:end_index]])\n",
    "test = Counter()\n",
    "for item in token_mapper[start_index:end_index]:\n",
    "    test.update(item[1])\n",
    "\n",
    "print(test.keys())\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "630a4268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in reverse\n",
    "\n",
    "token_mapper = {}\n",
    "for r,c in zip(row, col):\n",
    "    r_token_id = int(tokens[\"input_ids\"][0][r])\n",
    "    r_token_str = reverse_voc[r_token_id]\n",
    "\n",
    "    c_token_id = int(c)\n",
    "    c_token_str = reverse_voc[c_token_id]\n",
    "\n",
    "    if c_token_str not in token_mapper: token_mapper[c_token_str] = []\n",
    "    score = float(out[0][r, c])\n",
    "\n",
    "    token_mapper[c_token_str].append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abc90c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model [1.86, 1.85, 1.8, 1.7, 1.61, 1.48, 0.84, 0.78, 0.57, 0.41]\n",
      "efficiency [2.18, 2.14, 1.99, 1.94, 1.88, 1.88, 1.61, 1.38, 1.31, 1.17]\n",
      "efficient [1.5, 0.6, 0.55, 0.46, 0.38, 0.38, 0.16]\n",
      "research [0.58, 0.09, 0.01]\n",
      "study [1.2]\n",
      "test [0.91, 0.57]\n",
      "assessment [0.22, 0.04]\n",
      "sp [1.97, 1.93, 1.91, 1.89]\n",
      "##lad [1.38, 1.31, 1.29, 1.23]\n",
      "##e [1.03, 0.99, 0.86, 0.78]\n",
      "late [1.76, 1.65, 1.57]\n",
      "##nce [0.37, 0.26, 0.11]\n",
      "##ncy [1.6, 1.54, 1.41]\n",
      "issue [0.29]\n",
      "problem [0.33]\n",
      "important [1.06, 0.17]\n",
      "considered [0.15, 0.07]\n",
      "overlooked [1.09, 1.07]\n",
      "evaluate [0.39]\n",
      "ir [1.52]\n",
      "based [0.59]\n",
      "pre [0.96]\n",
      "##train [0.99]\n",
      "##ed [0.64]\n",
      "language [0.89]\n",
      "pl [1.39]\n",
      "##m [0.93, 0.68, 0.37]\n",
      "##ms [1.02, 0.77]\n",
      "reason [0.01]\n",
      "multiple [0.42]\n",
      "hardware [0.6]\n",
      "software [0.51]\n",
      "part [0.07]\n",
      "system [0.84, 0.75]\n",
      "paper [0.0]\n",
      "good [0.19, 0.04]\n",
      "better [0.59, 0.14]\n",
      "improve [1.4, 1.2, 0.86, 0.53]\n",
      "improvement [0.72, 0.42]\n",
      "achieved [0.2]\n",
      "state [0.49, 0.07]\n",
      "zero [0.95]\n",
      "shot [0.52]\n",
      "performance [0.66, 0.57, 0.53]\n",
      "competitive [1.09]\n",
      "result [0.34]\n",
      "tre [1.57]\n",
      "##c [0.54]\n",
      "collection [0.66]\n",
      "can [0.21, 0.16]\n",
      "regular [1.15, 1.09, 0.87, 0.82]\n",
      "##ization [0.76, 0.75, 0.62]\n",
      "factor [0.68]\n",
      "control [0.33]\n",
      "effective [0.75]\n",
      "reduce [0.54]\n",
      "gap [1.13]\n",
      "traditional [0.98, 0.72]\n",
      "conventional [0.25, 0.18]\n",
      "retrieval [1.14]\n",
      "method [1.1, 0.1, 0.03]\n",
      "we [0.05, 0.02]\n",
      "propose [0.74, 0.57]\n",
      "type [0.09]\n",
      "l [0.81]\n",
      "##1 [0.73]\n",
      "for [0.19]\n",
      "que [0.86]\n",
      "en [1.08, 0.89]\n",
      "flop [0.45]\n",
      "##ized [1.1]\n",
      "middle [0.66]\n",
      "training [0.01]\n",
      "bench [0.58]\n",
      "significantly [0.34]\n",
      "drastically [0.24]\n",
      "increase [0.58, 0.15]\n",
      "while [0.65, 0.36]\n",
      "metric [0.75]\n",
      "domain [1.17, 0.96]\n",
      "data [1.04, 0.97]\n",
      "our [0.02]\n",
      "first [0.63]\n",
      "neural [0.51, 0.19]\n",
      "under [0.37]\n",
      "same [1.36, 0.41, 0.25]\n",
      "different [0.07]\n",
      "computing [0.13]\n",
      "constraint [0.17]\n",
      "ex [1.23, 0.88]\n",
      "##ti [1.44, 1.4]\n",
      "##t [0.99, 0.95]\n",
      "{ [0.08]\n",
      "achieve [0.67]\n",
      "similar [1.01, 0.88]\n",
      "difference [0.22]\n",
      "b [1.04]\n",
      "##25 [0.65]\n",
      "speed [0.01]\n",
      "mr [1.06]\n",
      "##r [0.75]\n",
      "reduction [0.99]\n",
      "art [0.1]\n",
      "single [0.97]\n",
      "stage [0.86]\n",
      "brain [0.31]\n"
     ]
    }
   ],
   "source": [
    "for k in token_mapper:\n",
    "    scores = list(sorted(token_mapper[k], reverse=True))\n",
    "    print(k, [round(item, 2) for item in scores[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0ed9353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 15756, 12850,  2869,  2241,  2006,  9742, 15066,  4117,  2007,\n",
       "         15796,  7205, 10638,  3945,  2031,  3728,  2363,  1037,  2843,  1997,\n",
       "          3086,  1010, 11427,  2037,  3112,  2000,  4487, 16643, 20382,  1998,\n",
       "          1013,  2030,  2488, 16227,  1997,  4973,  2005,  2731,  1011,  1011,\n",
       "          2096,  2145, 18345,  2006,  1996,  2168, 21505,  4294,  1012,  1999,\n",
       "          1996, 12507,  1010, 20288,  6630,  4083, 17999,  2011,  3151, 20037,\n",
       "          5950,  2075,  5461,  2038,  2464,  1037,  3652,  3037,  1010, 22490,\n",
       "          2075,  2013, 16166, 20868,  3188,  2015,  2107,  2004, 13216, 16105,\n",
       "          9289,  9844,  1012,  2096,  2070,  6549, 10176,  2031,  2042,  3818,\n",
       "          1010,  1037,  8276,  3947,  2038,  2042,  2404,  1999,  1996,  2731,\n",
       "          1997,  2107,  4275,  1012,  1999,  2023,  2147,  1010,  2057,  3857,\n",
       "          2006, 11867, 27266,  2063,  1011,  1011,  1037, 20288,  4935,  1011,\n",
       "          2241, 12850,  2099,  1011,  1011,  1998,  2265,  2000,  2029,  6698,\n",
       "          2009,  2003,  2583,  2000,  5770,  2013,  1996,  2168,  2731,  8377,\n",
       "          2004,  9742,  4275,  1010,  2011,  5702,  1996,  3466,  1997,  4487,\n",
       "         16643, 20382,  1010,  2524,  1011,  4997,  5471,  2004,  2092,  2004,\n",
       "          1996,  3653,  1011,  4738,  2653,  2944,  3988,  3989,  1012,  2057,\n",
       "          7297,  2817,  1996,  4957,  2090, 12353,  1998,  8122,  1010,  2006,\n",
       "          1999,  1011,  5884,  1998,  5717,  1011,  2915, 10906,  1010,  2877,\n",
       "          2000,  2110,  1011,  1997,  1011,  1996,  1011,  2396,  3463,  1999,\n",
       "          2119, 16820,  2005, 12949, 22570,  4275,  1012,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6442c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/lamdo/splade/inference_splade.ipynb Cell 18\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bosprey2.csl.illinois.edu/home/lamdo/splade/inference_splade.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bosprey2.csl.illinois.edu/home/lamdo/splade/inference_splade.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     batch_doc_rep, batch_doc_token_indices, batch_doc_pad_len \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mencode(tokenizer([doc, doc], return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m), is_q \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m)  \u001b[39m# (sparse) doc rep in voc space, shape (30522,)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bosprey2.csl.illinois.edu/home/lamdo/splade/inference_splade.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(batch_doc_rep\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bosprey2.csl.illinois.edu/home/lamdo/splade/inference_splade.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     doc_rep \u001b[39m=\u001b[39m batch_doc_rep[i]\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    batch_doc_rep, batch_doc_token_indices, batch_doc_pad_len = model.encode(tokenizer([doc, doc], return_tensors=\"pt\"), is_q = False)  # (sparse) doc rep in voc space, shape (30522,)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(batch_doc_rep.size(0)):\n",
    "    doc_rep = batch_doc_rep[i]\n",
    "    doc_token_indices = batch_doc_token_indices[i]\n",
    "\n",
    "    # get the number of non-zero dimensions in the rep:\n",
    "    col = torch.nonzero(doc_rep).squeeze().cpu().tolist()\n",
    "    print(\"number of actual dimensions: \", len(col))\n",
    "\n",
    "    # now let's inspect the bow representation:\n",
    "    weights = doc_rep[col].cpu().tolist()\n",
    "    _indices = doc_token_indices[col].cpu().tolist()\n",
    "    d = {k: v for k, v in zip(col, weights)}\n",
    "    d_indices = {reverse_voc[k]: v for k, v in zip(col, _indices)}\n",
    "    sorted_d = {reverse_voc[k]: v for k, v in sorted(d.items(), key=lambda item: item[1], reverse=True)}\n",
    "    print(d_indices, \"\\n\", sorted_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b343054e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd833238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 30522]), torch.Size([2, 30522]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[0].shape, temp[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2f8aa202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['er',\n",
       " '##u',\n",
       " '-',\n",
       " 'kg',\n",
       " ':',\n",
       " 'efficient',\n",
       " 'reference',\n",
       " '-',\n",
       " 'aligned',\n",
       " 'un',\n",
       " '##su',\n",
       " '##per',\n",
       " '##vis',\n",
       " '##ed',\n",
       " 'key',\n",
       " '##ph',\n",
       " '##rase',\n",
       " 'generation',\n",
       " '.',\n",
       " 'un',\n",
       " '##su',\n",
       " '##per',\n",
       " '##vis',\n",
       " '##ed',\n",
       " 'key',\n",
       " '##ph',\n",
       " '##rase',\n",
       " 'prediction',\n",
       " 'has',\n",
       " 'gained',\n",
       " 'growing',\n",
       " 'interest',\n",
       " 'in',\n",
       " 'recent',\n",
       " 'years',\n",
       " '.',\n",
       " 'however',\n",
       " ',',\n",
       " 'existing',\n",
       " 'methods',\n",
       " 'typically',\n",
       " 'rely',\n",
       " 'on',\n",
       " 'he',\n",
       " '##uri',\n",
       " '##stic',\n",
       " '##ally',\n",
       " 'defined',\n",
       " 'importance',\n",
       " 'scores',\n",
       " ',',\n",
       " 'which',\n",
       " 'may',\n",
       " 'lead',\n",
       " 'to',\n",
       " 'inaccurate',\n",
       " 'inform',\n",
       " '##ative',\n",
       " '##ness',\n",
       " 'estimation',\n",
       " '.',\n",
       " 'in',\n",
       " 'addition',\n",
       " ',',\n",
       " 'the',\n",
       " 'y la',\n",
       " 'ck',\n",
       " 'consideration',\n",
       " 'for',\n",
       " 'time',\n",
       " 'efficiency',\n",
       " '.',\n",
       " 'to',\n",
       " 'solve',\n",
       " 'these',\n",
       " 'problems',\n",
       " ',',\n",
       " 'we',\n",
       " 'propose',\n",
       " 'er',\n",
       " '##u',\n",
       " '-',\n",
       " 'kg',\n",
       " ',',\n",
       " 'an',\n",
       " 'un',\n",
       " '##su',\n",
       " '##per',\n",
       " '##vis',\n",
       " '##ed',\n",
       " 'key',\n",
       " '##ph',\n",
       " '##rase',\n",
       " 'generation',\n",
       " '(',\n",
       " 'uk',\n",
       " '##g',\n",
       " ')',\n",
       " 'model',\n",
       " 'that',\n",
       " 'consists',\n",
       " 'of',\n",
       " 'a',\n",
       " 'phrase',\n",
       " '##ness',\n",
       " 'and',\n",
       " 'an',\n",
       " 'inform',\n",
       " '##ative',\n",
       " '##ness',\n",
       " 'module',\n",
       " '.',\n",
       " 'the',\n",
       " 'former',\n",
       " 'generate',\n",
       " 'candidates',\n",
       " ',',\n",
       " 'while',\n",
       " 'the',\n",
       " 'latter',\n",
       " 'estimate',\n",
       " 'their',\n",
       " 'relevance',\n",
       " '.',\n",
       " 'the',\n",
       " 'inform',\n",
       " '##ative',\n",
       " '##ness',\n",
       " 'module',\n",
       " 'inn',\n",
       " '##ova',\n",
       " '##tes',\n",
       " 'by',\n",
       " 'learning',\n",
       " 'to',\n",
       " 'model',\n",
       " 'inform',\n",
       " '##ative',\n",
       " '##ness',\n",
       " 'through',\n",
       " 'references',\n",
       " '(',\n",
       " 'e',\n",
       " '.',\n",
       " 'g',\n",
       " '.',\n",
       " ',',\n",
       " 'queries',\n",
       " ',',\n",
       " 'citation',\n",
       " 'contexts',\n",
       " ',',\n",
       " 'and',\n",
       " 'titles',\n",
       " ')',\n",
       " 'and',\n",
       " 'at',\n",
       " 'the',\n",
       " 'term',\n",
       " '-',\n",
       " 'level',\n",
       " ',',\n",
       " 'thereby',\n",
       " '1',\n",
       " ')',\n",
       " 'capturing',\n",
       " 'how',\n",
       " 'the',\n",
       " 'key',\n",
       " 'concepts',\n",
       " 'of',\n",
       " 'the',\n",
       " 'document',\n",
       " 'are',\n",
       " 'perceived',\n",
       " 'in',\n",
       " 'different',\n",
       " 'contexts',\n",
       " 'and',\n",
       " '2',\n",
       " ')',\n",
       " 'estimate',\n",
       " 'inform',\n",
       " '##ative',\n",
       " '##ness',\n",
       " 'of',\n",
       " 'phrases',\n",
       " 'more',\n",
       " 'efficiently',\n",
       " 'by',\n",
       " 'ag',\n",
       " '##gre',\n",
       " '##gating',\n",
       " 'term',\n",
       " 'inform',\n",
       " '##ative',\n",
       " '##ness',\n",
       " ',',\n",
       " 'removing',\n",
       " 'the',\n",
       " 'need',\n",
       " 'for',\n",
       " 'explicit',\n",
       " 'modeling',\n",
       " 'of',\n",
       " 'the',\n",
       " 'candidates',\n",
       " '.',\n",
       " 'er',\n",
       " '##u',\n",
       " '-',\n",
       " 'kg',\n",
       " 'demonstrates',\n",
       " 'its',\n",
       " 'effectiveness',\n",
       " 'on',\n",
       " 'key',\n",
       " '##ph',\n",
       " '##rase',\n",
       " 'generation',\n",
       " 'benchmark',\n",
       " 's',\n",
       " 'by',\n",
       " 'out',\n",
       " '##per',\n",
       " '##form',\n",
       " '##ing',\n",
       " 'un',\n",
       " '##su',\n",
       " '##per',\n",
       " '##vis',\n",
       " '##ed',\n",
       " 'baseline',\n",
       " '##s',\n",
       " 'and',\n",
       " 'achieving',\n",
       " 'on',\n",
       " 'average',\n",
       " '89',\n",
       " '%',\n",
       " 'of',\n",
       " 'the',\n",
       " 'performance',\n",
       " 'of',\n",
       " 'a',\n",
       " 'supervised',\n",
       " 'baseline',\n",
       " 'for',\n",
       " 'top',\n",
       " '10',\n",
       " 'predictions',\n",
       " '.',\n",
       " 'additionally',\n",
       " ',',\n",
       " 'to',\n",
       " 'highlight',\n",
       " 'its',\n",
       " 'practical',\n",
       " 'utility',\n",
       " ',',\n",
       " 'we',\n",
       " 'evaluate',\n",
       " 'the',\n",
       " 'model',\n",
       " 'on',\n",
       " 'text',\n",
       " 'retrieval',\n",
       " 'tasks',\n",
       " 'and',\n",
       " 'show',\n",
       " 'that',\n",
       " 'key',\n",
       " '##ph',\n",
       " '##rase',\n",
       " '##s',\n",
       " 'generated',\n",
       " 'by',\n",
       " 'er',\n",
       " '##u',\n",
       " '-',\n",
       " 'kg',\n",
       " 'are',\n",
       " 'effective',\n",
       " 'when',\n",
       " 'employed',\n",
       " 'as',\n",
       " 'query',\n",
       " 'and',\n",
       " 'document',\n",
       " 'expansion',\n",
       " '##s',\n",
       " '.',\n",
       " 'finally',\n",
       " ',',\n",
       " 'inference',\n",
       " 'speed',\n",
       " 'tests',\n",
       " 'reveal',\n",
       " 'that',\n",
       " 'er',\n",
       " '##u',\n",
       " '-',\n",
       " 'kg',\n",
       " 'is',\n",
       " 'the',\n",
       " 'fastest',\n",
       " 'among',\n",
       " 'baseline',\n",
       " '##s',\n",
       " 'of',\n",
       " 'similar',\n",
       " 'model',\n",
       " 'sizes',\n",
       " '.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "832edf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8b568a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['attention', 'is', 'all', 'you', 'need', '.', 'the', 'dominant', 'sequence', 'trans', '##duction', 'models', 'are', 'based', 'on', 'complex', 'recurrent', 'or', 'convolution', 'al', 'neural networks', 'in', 'an', 'en', '##code', '##r', '-', 'decoder', 'configuration', '.', 'the', 'best', 'performing', 'models', 'also', 'connect', 'the', 'en', '##code', '##r', 'and', 'decoder', 'through', 'an', 'attention mechanism', '.', 'we', 'propose', 'a', 'new', 'simple', 'network architecture', ',', 'the', 'transform', '##er', ',', 'based', 'solely', 'on', 'attention mechanism', 's', ',', 'di', '##sp', '##ens', '##ing', 'with', 'recurrence', 'and', 'convolution', 's', 'entirely', '.', 'experiments', 'on', 'two', 'machine translation', 'tasks', 'show', 'these', 'models', 'to', 'be', 'superior', 'in', 'quality', 'while', 'being', 'more', 'parallel', '##iza', '##ble', 'and', 'requiring', 'significantly', 'less', 'time', 'to', 'train', '.', 'our', 'model', 'achieve', '##s', '28', '.', '4', 'b', '##le', '##u', 'on', 'the', 'w', '##mt', '2014', 'english', '-', 'to', '-', 'german', 'translation', 'task', ',', 'improving', 'over', 'the', 'existing', 'best', 'results', ',', 'including', 'ensembles', 'by', 'over', '2', 'b', '##le', '##u', '.', 'on', 'the', 'w', '##mt', '2014', 'english', '-', 'to', '-', 'french', 'translation', 'task', ',', 'our', 'model', 'establishes', 'a', 'new', 'single', '-', 'model', 'state', '-', 'of', '-', 'the', '-', 'art', 'b', '##le', '##u', 'score', 'of', '41', '.', '8', 'after', 'training', 'for', '3', '.', '5', 'days', 'on', 'eight', 'gpus', ',', 'a', 'small', 'fraction', 'of', 'the', 'training', 'costs', 'of', 'the', 'best', 'models', 'from', 'the', 'literature', '.', 'we', 'show', 'that', 'the', 'transform', '##er', 'general', '##izes', 'well', 'to', 'other', 'tasks', 'by', 'applying', 'it', 'successfully', 'to', 'english', 'constituency', 'parsing', 'both', 'with', 'large', 'and', 'limited', 'training data', '.']\n",
      "\n",
      "['attention', 'is', 'all', 'you', 'need', '.', 'the', 'dominant', 'sequence', 'trans', '##duction', 'models', 'are', 'based', 'on', 'complex', 'rec', '##urrent', 'or', 'con', '##vo', '##lu', '##tion', '##al', 'neural', 'networks', 'in', 'an', 'en', '##code', '##r', '-', 'deco', '##der', 'configuration', '.', 'the', 'best', 'performing', 'models', 'also', 'connect', 'the', 'en', '##code', '##r', 'and', 'deco', '##der', 'through', 'an', 'attention', 'mechanism', '.', 'we', 'propose', 'a', 'new', 'simple', 'network', 'architecture', ',', 'the', 'transform', '##er', ',', 'based', 'solely', 'on', 'attention', 'mechanisms', ',', 'di', '##sp', '##ens', '##ing', 'with', 'rec', '##ur', '##rence', 'and', 'con', '##vo', '##lu', '##tions', 'entirely', '.', 'experiments', 'on', 'two', 'machine', 'translation', 'tasks', 'show', 'these', 'models', 'to', 'be', 'superior', 'in', 'quality', 'while', 'being', 'more', 'parallel', '##iza', '##ble', 'and', 'requiring', 'significantly', 'less', 'time', 'to', 'train', '.', 'our', 'model', 'achieve', '##s', '28', '.', '4', 'b', '##le', '##u', 'on', 'the', 'w', '##mt', '2014', 'english', '-', 'to', '-', 'german', 'translation', 'task', ',', 'improving', 'over', 'the', 'existing', 'best', 'results', ',', 'including', 'ensembles', 'by', 'over', '2', 'b', '##le', '##u', '.', 'on', 'the', 'w', '##mt', '2014', 'english', '-', 'to', '-', 'french', 'translation', 'task', ',', 'our', 'model', 'establishes', 'a', 'new', 'single', '-', 'model', 'state', '-', 'of', '-', 'the', '-', 'art', 'b', '##le', '##u', 'score', 'of', '41', '.', '8', 'after', 'training', 'for', '3', '.', '5', 'days', 'on', 'eight', 'gp', '##us', ',', 'a', 'small', 'fraction', 'of', 'the', 'training', 'costs', 'of', 'the', 'best', 'models', 'from', 'the', 'literature', '.', 'we', 'show', 'that', 'the', 'transform', '##er', 'general', '##izes', 'well', 'to', 'other', 'tasks', 'by', 'applying', 'it', 'successfully', 'to', 'english', 'constituency', 'par', '##sing', 'both', 'with', 'large', 'and', 'limited', 'training', 'data', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(doc))\n",
    "print()\n",
    "print(original_tokenizer.tokenize(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "35680bd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['as', '##db', '##ais', '##b', '##d']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_tokenizer.tokenize(\"asdbaisbd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "715d4526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39b95171",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/scratch/lamdo/doris-mae/DORIS-MAE_dataset_v1.json\") as f:\n",
    "    ds = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd890a29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'masked_abstract': \"Machine learning systems often experience a distribution shift between training and testing . In this paper , we introduce a simple variational objective whose optima are exactly the set of all representations on which risk minimizers are guaranteed to be robust to any distribution shift that preserves the Bayes predictor , * , covariate shifts . Our objective has two components . First , a representation must remain discriminative for the task , i.e. , some predictor must be able to simultaneously minimize the source and target risk . Second , the representation 's marginal support needs to be the same across source and target . We make this practical by designing self-supervised objectives that only use unlabelled data and augmentations to train robust representations . Our objectives give insights into the robustness of * , and further improve * 's representations to achieve * results on * .\",\n",
       " 'original_abstract': \"Machine learning systems often experience a distribution shift between\\ntraining and testing. In this paper, we introduce a simple variational\\nobjective whose optima are exactly the set of all representations on which risk\\nminimizers are guaranteed to be robust to any distribution shift that preserves\\nthe Bayes predictor, e.g., covariate shifts. Our objective has two components.\\nFirst, a representation must remain discriminative for the task, i.e., some\\npredictor must be able to simultaneously minimize the source and target risk.\\nSecond, the representation's marginal support needs to be the same across\\nsource and target. We make this practical by designing self-supervised\\nobjectives that only use unlabelled data and augmentations to train robust\\nrepresentations. Our objectives give insights into the robustness of CLIP, and\\nfurther improve CLIP's representations to achieve SOTA results on DomainBed.\",\n",
       " 'title': 'Optimal_Representations_for_Covariate_Shift',\n",
       " 'url': 'http://arxiv.org/abs/2201.00057v2',\n",
       " 'primary_category': 'cs.LG',\n",
       " 'categories': ['cs.LG', 'cs.AI', 'cs.IT', 'math.IT', 'stat.ML'],\n",
       " 'incoming_citations': [11669],\n",
       " 'ss_id': '5382d9bc17aabfd47b7c7d9873d2b64fdde48305',\n",
       " 'outgoing_citations': [11783,\n",
       "  40729,\n",
       "  42817,\n",
       "  53464,\n",
       "  77928,\n",
       "  98358,\n",
       "  114487,\n",
       "  114823,\n",
       "  115360,\n",
       "  137625,\n",
       "  139100,\n",
       "  148709,\n",
       "  155339,\n",
       "  175834,\n",
       "  177371,\n",
       "  182326,\n",
       "  183874,\n",
       "  185954,\n",
       "  195912,\n",
       "  197714,\n",
       "  199022,\n",
       "  207239,\n",
       "  224762,\n",
       "  243653,\n",
       "  253776,\n",
       "  262064,\n",
       "  263051,\n",
       "  265970,\n",
       "  272564,\n",
       "  272675,\n",
       "  277021,\n",
       "  278987,\n",
       "  281135,\n",
       "  288929,\n",
       "  292142,\n",
       "  317764],\n",
       " 'abstract_id': 10}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"Corpus\"][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25658ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "machine learning is fun\n",
    "\n",
    "-> [\"machine\" \"learning\" \"machine learning\" \"is\" \"fun\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
