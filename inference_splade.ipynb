{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "expired-priority",
   "metadata": {},
   "source": [
    "# SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becoming-television",
   "metadata": {},
   "source": [
    "This notebook gives a minimal example usage of SPLADE.\n",
    "\n",
    "* We provide models via Hugging Face (https://huggingface.co/naver)\n",
    "* See [Naver Labs Europe website](https://europe.naverlabs.com/research/machine-learning-and-optimization/splade-models/) for other intermediate models.\n",
    "\n",
    "| model | MRR@10 (MS MARCO dev) | recall@1000 (MS MARCO dev) | expected FLOPS | ~ avg q length | ~ avg d length | \n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| `naver/splade_v2_max` (**v2** [HF](https://huggingface.co/naver/splade_v2_max)) | 34.0 | 96.5 | 1.32 | 18 | 92 |\n",
    "| `naver/splade_v2_distil` (**v2** [HF](https://huggingface.co/naver/splade_v2_distil)) | 36.8 | 97.9 | 3.82 | 25 | 232 |\n",
    "| `naver/splade-cocondenser-selfdistil` (**v2bis**, [HF](https://huggingface.co/naver/splade-cocondenser-selfdistil))| 37.6 | 98.4 | 2.32 | 56 | 134 |\n",
    "| `naver/splade-cocondenser-ensembledistil` (**v2bis**, [HF](https://huggingface.co/naver/splade-cocondenser-ensembledistil)) | 38.3 | 98.3  | 1.85 | 44 | 120 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "tight-milton",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lamdo/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch, os, string\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "from splade.models.transformer_rep import SpladeMaxSim, Splade, PhraseSpladev3, PhraseSpladev4, PhraseSpladev5\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bbbeba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "mighty-invention",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the dir for trained weights\n",
    "\n",
    "##### v2\n",
    "# model_type_or_dir = \"naver/splade_v2_max\"\n",
    "# model_type_or_dir = \"naver/splade_v2_distil\"\n",
    "\n",
    "### v2bis, directly download from Hugging Face\n",
    "# model_type_or_dir = \"naver/splade-cocondenser-selfdistil\"\n",
    "# model_type_or_dir = \"naver/splade-cocondenser-ensembledistil\"\n",
    "model_type_or_dir = \"/scratch/lamdo/phrase_splade_checkpoints/phrase_splade_71/debug/checkpoint/model\"\n",
    "# model_type_or_dir = \"/scratch/lamdo/splade_maxsim_ckpts/splade_maxsim_150k_lowregv3/debug/checkpoint/model\"\n",
    "# model_type_or_dir = 'lamdo/distilbert-base-uncased-phrase-16kaddedphrasesfroms2orc-mlm-150000steps-multiwords'\n",
    "# model_type_or_dir = \"/scratch/lamdo/splade_checkpoints/experiments_combined_references_v8-1/debug/checkpoint/model\"\n",
    "# model_type_or_dir = \"lamdo/distilbert-base-uncased-phrase-60kaddedphrasesfroms2orc-mlm-150000steps\"\n",
    "# model_type_or_dir = \"/scratch/lamdo/phrase_splade_checkpoints/splade_max_1/debug/checkpoint/model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "centered-watershed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59419"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading model and tokenizer\n",
    "\n",
    "model = PhraseSpladev3(model_type_or_dir, agg=\"max\")\n",
    "model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_type_or_dir)\n",
    "reverse_voc = {v: k for k, v in tokenizer.vocab.items()}\n",
    "\n",
    "len(reverse_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ce3573e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhraseSpladev3(\n",
       "  (transformer_rep): TransformerRep(\n",
       "    (transformer): DistilBertForMaskedLM(\n",
       "      (activation): GELUActivation()\n",
       "      (distilbert): DistilBertModel(\n",
       "        (embeddings): Embeddings(\n",
       "          (word_embeddings): Embedding(59419, 768, padding_idx=0)\n",
       "          (position_embeddings): Embedding(512, 768)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (transformer): Transformer(\n",
       "          (layer): ModuleList(\n",
       "            (0-5): 6 x TransformerBlock(\n",
       "              (attention): DistilBertSdpaAttention(\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (ffn): FFN(\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (activation): GELUActivation()\n",
       "              )\n",
       "              (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (vocab_transform): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (vocab_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (vocab_projector): Linear(in_features=768, out_features=59419, bias=True)\n",
       "      (mlm_loss_fct): CrossEntropyLoss()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53e869ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_custom(tokens, model, is_q = False):\n",
    "    out = model.encode_(tokens, is_q)[\"logits\"]  # shape (bs, pad_len, voc_size)\n",
    "\n",
    "    # if hasattr(model, \"create_token_phrase_mask\"):\n",
    "    #     print(\"yoyo\")\n",
    "    #     token_phrase_mask = model.create_token_phrase_mask(tokens)\n",
    "    #     out = out * token_phrase_mask\n",
    "\n",
    "    out = torch.log(1 + torch.relu(out)) * tokens[\"attention_mask\"].unsqueeze(-1)\n",
    "\n",
    "    # out = encode_custom_mask_punc(tokens, model, is_q, False)\n",
    "    # mask = ~torch.isin(tokens[\"input_ids\"], PUNCID)\n",
    "    # out = out * mask.unsqueeze(-1)\n",
    "\n",
    "    return out\n",
    "\n",
    "    res = torch.zeros_like(out)\n",
    "    res = res.to(out.device)\n",
    "\n",
    "    out, token_indices = torch.max(out, dim = 1)\n",
    "\n",
    "\n",
    "    res.scatter_(1, token_indices.unsqueeze(1), out.unsqueeze(1))\n",
    "    return res\n",
    "\n",
    "def splade_pooling_v3(out, tokens):\n",
    "    out_tokens = out[..., :30522] # shape (bs, pad_len, original_bert_vocab_size)\n",
    "    out_phrases = out[..., 30522:] # shape (bs, pad_len, vocab_size - original_bert_vocab_size)\n",
    "    values_tokens, _ = torch.max(torch.log(1 + torch.relu(out_tokens)) * tokens[\"attention_mask\"].unsqueeze(-1), dim=1) # shape (bs, original_bert_vocab_size)\n",
    "    values_phrases = torch.sum(torch.log(1 + torch.relu(out_phrases)) * tokens[\"attention_mask\"].unsqueeze(-1), dim=1) # shape (bs, vocab_size - original_bert_vocab_size)\n",
    "\n",
    "    values = torch.cat([values_tokens, values_phrases], dim = -1)\n",
    "    return values\n",
    "    # 0 masking also works with max because all activations are positive\n",
    "\n",
    "\n",
    "PUNCID = torch.tensor([tokenizer.vocab[punc] for punc in string.punctuation] + [tokenizer.vocab[\"[SEP]\"], tokenizer.vocab[\"[CLS]\"]])\n",
    "# PUNCID = torch.tensor([tokenizer.vocab[\"[SEP]\"], tokenizer.vocab[\"[CLS]\"]])\n",
    "def encode_custom_mask_punc(tokens, model, is_q = False, pooling = True):\n",
    "    out = model.encode_(tokens, is_q)[\"logits\"]  # shape (bs, pad_len, voc_size)\n",
    "    out = torch.log(1 + torch.relu(out)) * tokens[\"attention_mask\"].unsqueeze(-1)\n",
    "\n",
    "    mask = ~torch.isin(tokens[\"input_ids\"], PUNCID)\n",
    "    out = out * mask.unsqueeze(-1)\n",
    "\n",
    "    res = torch.zeros_like(out)\n",
    "    res = res.to(out.device)\n",
    "\n",
    "    # out, token_indices = torch.max(out, dim = 1)\n",
    "\n",
    "    if pooling: return splade_pooling_v3(out, tokens)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "forward-movement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example document from MS MARCO passage collection (doc_id = 8003157)\n",
    "\n",
    "# doc = \"\"\"ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction. Neural information retrieval (IR) has greatly advanced search and other knowledge-intensive language tasks. While many neural IR methods encode queries and documents into single-vector representations, late interaction models produce multi-vector representations at the granularity of each token and decompose relevance modeling into scalable token-level computations. This decomposition has been shown to make late interaction more effective, but it inflates the space footprint of these models by an order of magnitude. In this work, we introduce ColBERTv2, a retriever that couples an aggressive residual compression mechanism with a denoised supervision strategy to simultaneously improve the quality and space footprint of late interaction. We evaluate ColBERTv2 across a wide range of benchmarks, establishing state-of-the-art quality within and outside the training domain while reducing the space footprint of late interaction models by 6--10×.\"\"\"\n",
    "\n",
    "# doc = \"\"\"Supplementing Remote Sensing of Ice: Deep Learning-Based Image Segmentation System for Automatic Detection and Localization of Sea-ice Formations From Close-Range Optical Images. This paper presents a three-stage approach for the automated analysis of close-range optical images containing ice objects. The proposed system is based on an ensemble of deep learning models and conditional random field postprocessing. The following surface ice formations were considered: Icebergs, Deformed ice, Level ice, Broken ice, Ice floes, Floebergs, Floebits, Pancake ice, and Brash ice. Additionally, five non-surface ice categories were considered: Sky, Open water, Shore, Underwater ice, and Melt ponds. To find input parameters for the approach, the performance of 12 different neural network architectures was explored and evaluated using a 5-fold cross-validation scheme. The best performance was achieved using an ensemble of models having pyramid pooling layers (PSPNet, PSPDenseNet, DeepLabV3+, and UPerNet) and convolutional conditional random field postprocessing with a mean intersection over union score of 0.799, and this outperformed the best single-model approach. The results of this study show that when per-class performance was considered, the Sky was the easiest class to predict, followed by Deformed ice and Open water. Melt pond was the most challenging class to predict. Furthermore, we have extensively explored the strengths and weaknesses of our approach and, in the process, discovered the types of scenes that pose a more significant challenge to the underlying neural networks. When coupled with optical sensors and AIS, the proposed approach can serve as a supplementary source of large-scale ‘ground truth’ data for validation of satellite-based sea-ice products. We have provided an implementation of the approach at https://github.com/panchinabil/sea_ice_segmentation .\"\"\"\n",
    "\n",
    "\n",
    "# doc = \"\"\"A comprehensive survey of graph embedding: Problems, techniques, and applications. Graph is an important data representation which appears in a wide diversity of real-world scenarios. Effective graph analytics provides users a deeper understanding of what is behind the data, and thus can benefit a lot of useful applications such as node classification, node recommendation, link prediction, etc. However, most graph analytics methods suffer the high computation and space cost. Graph embedding is an effective yet efficient way to solve the graph analytics problem. It converts the graph data into a low dimensional space in which the graph structural information and graph properties are maximumly preserved. In this survey, we conduct a comprehensive review of the literature in graph embedding. We first introduce the formal definition of graph embedding as well as the related concepts. After that, we propose two taxonomies of graph embedding which correspond to what challenges exist in different\"\"\"\n",
    "\n",
    "# doc = \"\"\"Attention Is All You Need. The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\"\"\"\n",
    "\n",
    "# doc = \"ERU-KG: Efficient Reference-aligned Unsupervised Keyphrase Generation\"\n",
    "\n",
    "# doc = \"\"\"ERU-KG: Efficient Reference-aligned Unsupervised Keyphrase Generation. Unsupervised keyphrase prediction has gained growing interest in recent years. However, existing methods typically rely on heuristically defined importance scores, which may lead to inaccurate informativeness estimation. In addition, they lack consideration for time efficiency. To solve these problems, we propose ERU-KG, an unsupervised keyphrase generation (UKG) model that consists of a phraseness and an informativeness module. The former generate candidates, while the latter estimate their relevance. The informativeness module innovates by learning to model informativeness through references (e.g., queries, citation contexts, and titles) and at the term-level, thereby 1) capturing how the key concepts of the document are perceived in different contexts and 2) estimate informativeness of phrases more efficiently by aggregating term informativeness, removing the need for explicit modeling of the candidates. ERU-KG demonstrates its effectiveness on keyphrase generation benchmarks by outperforming unsupervised baselines and achieving on average 89% of the performance of a supervised baseline for top 10 predictions. Additionally, to highlight its practical utility, we evaluate the model on text retrieval tasks and show that keyphrases generated by ERU-KG are effective when employed as query and document expansions. Finally, inference speed tests reveal that ERU-KG is the fastest among baselines of similar model sizes.\"\"\"\n",
    "\n",
    "# doc = \"\"\"SPLADE v2: Sparse Lexical and Expansion Model for Information Retrieval. In neural Information Retrieval (IR), ongoing research is directed towards improving the first retriever in ranking pipelines. Learning dense embeddings to conduct retrieval using efficient approximate nearest neighbors methods has proven to work well. Meanwhile, there has been a growing interest in learning \\emph{sparse} representations for documents and queries, that could inherit from the desirable properties of bag-of-words models such as the exact matching of terms and the efficiency of inverted indexes. Introduced recently, the SPLADE model provides highly sparse representations and competitive results with respect to state-of-the-art dense and sparse approaches. In this paper, we build on SPLADE and propose several significant improvements in terms of effectiveness and/or efficiency. More specifically, we modify the pooling mechanism, benchmark a model solely based on document expansion, and introduce models trained with distillation. We also report results on the BEIR benchmark. Overall, SPLADE is considerably improved with more than 9\\% gains on NDCG@10 on TREC DL 2019, leading to state-of-the-art results on the BEIR benchmark.\"\"\"\n",
    "\n",
    "# doc = \"\"\"The author uses 3 096 sample households in 15 counties from the year 1995 to 2006 to analyze the impact of PFPs on rural households' income inequality by income inequality decomposition.The research indicates that:(1) the percentage of subsidy income generated from PFPs has increased 8.03% during the period from 1995 to 2006;(2) the contribution of subsidy income generated from PFPs has been up from 0.330 7% in 1995 to 3.794 1% in 2006;(3) the policy-caused subsidy income inequality is more prominent than that caused by the planned regions of PFPs.Therefore a rational policy adjustment of PFPs will contribute more to poverty reduction in China's rural areas.\"\"\"\n",
    "\n",
    "# doc = \" | But much of the responsibility of the social inequity that leads to different health outcomes lies elsewhere. Health is affected by policies in other sectors, such as education, taxation, transport, and agriculture too.\"\n",
    "\n",
    "\n",
    "# doc = \"Gapped BLAST and PSI-BLAST: a new generation of protein database search programs. The BLAST programs are widely used tools for searching protein and DNA databases for sequence similarities. For protein comparisons, a variety of definitional, algorithmic and statistical refinements described here permits the execution time of the BLAST programs to be decreased substantially while enhancing their sensitivity to weak similarities. A new criterion for triggering the extension of word hits, combined with a new heuristic for generating gapped alignments, yields a gapped BLAST program that runs at approximately three times the speed of the original. In addition, a method is introduced for automatically combining statistically significant alignments produced by BLAST into a position-specific score matrix, and searching the database using this matrix. The resulting Position-Specific Iterated BLAST (PSIBLAST) program runs at approximately the same speed per iteration as gapped BLAST, but in many cases is much more sensitive to weak but biologically relevant sequence similarities. PSI-BLAST is used to uncover several new and interesting members of the BRCT superfamily.\"\n",
    "\n",
    "# doc = \"Generative Image Dynamics. We present an approach to modeling an image-space prior on scene motion. Our prior is learned from a collection of motion trajectories extracted from real video sequences depicting natural, oscillatory dynamics such as trees, flowers, candles, and clothes swaying in the wind. We model this dense, long-term motion prior in the Fourier domain:given a single image, our trained model uses a frequency-coordinated diffusion sampling process to predict a spectral volume, which can be converted into a motion texture that spans an entire video. Along with an image-based rendering module, these trajectories can be used for a number of downstream applications, such as turning still images into seamlessly looping videos, or allowing users to realistically interact with objects in real pictures by interpreting the spectral volumes as image-space modal bases, which approximate object dynamics.\"\n",
    "\n",
    "# doc = \"Rich Human Feedback for Text-to-Image Generation. Recent Text-to-Image (T2I) generation models such as Stable Diffusion and Imagen have made significant progress in generating high-resolution images based on text descriptions. However, many generated images still suffer from issues such as artifacts/implausibility, misalignment with text descriptions, and low aesthetic quality. Inspired by the success of Reinforcement Learning with Human Feedback (RLHF) for large language models, prior works collected human-provided scores as feedback on generated images and trained a reward model to improve the T2I generation. In this paper, we enrich the feedback signal by (i) marking image regions that are implausible or misaligned with the text, and (ii) annotating which words in the text prompt are misrepresented or missing on the image. We collect such rich human feedback on 18K generated images (RichHF-18K) and train a multimodal transformer to predict the rich feedback automatically. We show that the predicted rich human feedback can be leveraged to improve image generation, for example, by selecting high-quality training data to finetune and improve the generative models, or by creating masks with predicted heatmaps to inpaint the problematic regions. Notably, the improvements generalize to models (Muse) beyond those used to generate the images on which human feedback data were collected (Stable Diffusion variants)\"\n",
    "\n",
    "# doc = \"MedYOLO: A Medical Image Object Detection Framework. Artificial intelligence-enhanced identification of organs, lesions, and other structures in medical imaging is typically done using convolutional neural networks (CNNs) designed to make voxel-accurate segmentations of the region of interest. However, the labels required to train these CNNs are time-consuming to generate and require attention from subject matter experts to ensure quality. For tasks where voxel-level precision is not required, object detection models offer a viable alternative that can reduce annotation effort. Despite this potential application, there are few options for general purpose object detection frameworks available for 3-D medical imaging. We report on MedYOLO, a 3-D object detection framework using the one-shot detection method of the YOLO family of models and designed for use with medical imaging. We tested this model on four different datasets: BRaTS, LIDC, an abdominal organ Computed Tomography (CT) dataset, and an ECG-gated heart CT dataset. We found our models achieve high performance on commonly present medium and large-sized structures such as the heart, liver, and pancreas even without hyperparameter tuning. However, the models struggle with very small or rarely present structures.\"\n",
    "\n",
    "# doc = \"A study of smoothing methods for language models applied to ad hoc information retrieval. Language modeling approaches to information retrieval are attractive and promising because they connect the problem of retrieval with that of language model estimation, which has been studied extensively in other application areas such as speech recognition. The basic idea of these approaches is to estimate a language model for each document, and then rank documents by the likelihood of the query according to the estimated language model. A core problem in language model estimation is smoothing, which adjusts the maximum likelihood estimator so as to correct the inaccuracy due to data sparseness. In this paper, we study the problem of language model smoothing and its influence on retrieval performance. We examine the sensitivity of retrieval performance to the smoothing parameters and compare several popular smoothing methods on different test collection.\"\n",
    "\n",
    "# doc = \"Big data: astronomical or genomical? Genomics is a Big Data science and is going to get much bigger, very soon, but it is not known whether the needs of genomics will exceed other Big Data domains. Projecting to the year 2025, we compared genomics with three other major generators of Big Data: astronomy, YouTube, and Twitter. Our estimates show that genomics is a “four-headed beast”—it is either on par with or the most demanding of the domains analyzed here in terms of data acquisition, storage, distribution, and analysis. We discuss aspects of new technologies that will need to be developed to rise up and meet the computational challenges that genomics poses for the near future. Now is the time for concerted, community-wide planning for the “genomical” challenges of the next decade.\"\n",
    "\n",
    "# doc = \"Topic sentiment mixture: modeling facets and opinions in weblogs. In this paper, we define the problem of topic-sentiment analysis on Weblogs and propose a novel probabilistic model to capture the mixture of topics and sentiments simultaneously. The proposed Topic-Sentiment Mixture (TSM) model can reveal the latent topical facets in a Weblog collection, the subtopics in the results of an ad hoc query, and their associated sentiments. It could also provide general sentiment models that are applicable to any ad hoc topics. With a specifically designed HMM structure, the sentiment models and topic models estimated with TSM can be utilized to extract topic life cycles and sentiment dynamics. Empirical experiments on different Weblog datasets show that this approach is effective for modeling the topic facets and sentiments and extracting their dynamics from Weblog collections.\"\n",
    "\n",
    "\n",
    "# doc = \"Deep Residual Learning for Image Recognition. Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.\"\n",
    "\n",
    "# doc = \"Fairness in Dead-Reckoning based Distributed Multi-Player Games. In a distributed multi-player game that uses dead-reckoning vectors to exchange movement information among players, there is inaccuracy in rendering the objects at the receiver due to network delay between the sender and the receiver. The object is placed at the receiver at the position indicated by the dead-reckoning vector, but by that time, the real position could have changed considerably at the sender. This inaccuracy would be tolerable if it is consistent among all players; that is, at the same physical time, all players see inaccurate (with respect to the real position of the object) but the same position and trajectory for an object. But due to varying network delays between the sender and different receivers, the inaccuracy is different at different players as well. This leads to unfairness in game playing. In this paper, we first introduce an error measure for estimating this inaccuracy. Then we develop an algorithm for scheduling the sending of dead-reckoning vectors at a sender that strives to make this error equal at different receivers over time. This algorithm makes the game very fair at the expense of increasing the overall mean error of all players. To mitigate this effect, we propose a budget based algorithm that provides improved fairness without increasing the mean error thereby maintaining the accuracy of game playing. We have implemented both the scheduling algorithm and the budget based algorithm as part of BZFlag, a popular distributed multi-player game. We show through experiments that these algorithms provide fairness among players in spite of widely varying network delays. An additional property of the proposed algorithms is that they require less number of DRs to be exchanged (compared to the current implementation of BZflag) to achieve the same level of accuracy in game playing.\"\n",
    "\n",
    "# doc = \"Evaluating Adaptive Resource Management for Distributed Real-Time Embedded Systems. A challenging problem faced by researchers and developers of distributed real-time and embedded (DRE) systems is devising and implementing effective adaptive resource management strategies that can meet end-to-end quality of service (QoS) requirements in varying operational conditions. This paper presents two contributions to research in adaptive resource management for DRE systems. First, we describe the structure and functionality of the Hybrid Adaptive Resourcemanagement Middleware (HyARM), which provides adaptive resource management using hybrid control techniques for adapting to workload fluctuations and resource availability. Second, we evaluate the adaptive behavior of HyARM via experiments on a DRE multimedia system that distributes video in real-time. Our results indicate that HyARM yields predictable, stable, and high system performance, even in the face of fluctuating workload and resource availability.\"\n",
    "\n",
    "# doc = \"Real World BCI: Cross-Domain Learning and Practical Applications\"\n",
    "\n",
    "\n",
    "# doc = \"how long is german measles contagious\"\n",
    "\n",
    "# doc = \"Nonsense. In places where people have most kids, it doesn't matter how expensive baby care products are because most babies are born where they aren't used nearly as much, or at all. Sure, making them more expensive will help having less children in the first world, but that would change squat WRT the total number.\"\n",
    "\n",
    "# doc = \"lda\"\n",
    "\n",
    "# doc = \"The Reserve Bank of Australia (RBA) came into being on 14 January 1960 as Australia 's central bank and banknote issuing authority, when the Reserve Bank Act 1959 removed the central banking functions from the Commonwealth Bank. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion. Nearly 94% of the RBA's employees work at its headquarters in Sydney, New South Wales and at the Business Resumption Site.\"\n",
    "\n",
    "\n",
    "# doc = \"Here are the SEC requirements: The federal securities laws define the term accredited investor in Rule 501 of Regulation D as: a bank, insurance company, registered investment company, business development company, or small business investment company; an employee benefit plan, within the meaning of the Employee Retirement Income Security Act, if a bank, insurance company, or registered investment adviser makes the investment decisions, or if the plan has total assets in excess of $5 million; a charitable organization, corporation, or partnership with assets exceeding $5 million; a director, executive officer, or general partner of the company selling the securities; a business in which all the equity owners are accredited investors; a natural person who has individual net worth, or joint net worth with the person’s spouse, that exceeds $1 million at the time of the purchase, excluding the value of the primary residence of such person; a natural person with income exceeding $200,000 in each of the two most recent years or joint income with a spouse exceeding $300,000 for those years and a reasonable expectation of the same income level in the current year; or a trust with assets in excess of $5 million, not formed to acquire the securities offered, whose purchases a sophisticated person makes. No citizenship/residency requirements.\"\n",
    "\n",
    "# doc = \"where is steph currys home in nc\"\n",
    "\n",
    "# doc = \"was ronald reagan a democrat\"\n",
    "# doc = \"what slows down the flow of blood\"\n",
    "\n",
    "# doc = \"A comprehensive survey of graph embedding: Problems, techniques, and applications\"\n",
    "\n",
    "# doc = \"\"\"C-Pack: Packed Resources For General Chinese Embeddings. We introduce C-Pack, a package of resources that significantly advance the field of general Chinese embeddings. C-Pack includes three critical resources. 1) C-MTEB is a comprehensive benchmark for Chinese text embeddings covering 6 tasks and 35 datasets. 2) C-MTP is a massive text embedding dataset curated from labeled and unlabeled Chinese corpora for training embedding models. 3) C-TEM is a family of embedding models covering multiple sizes. Our models outperform all prior Chinese text embeddings on C-MTEB by up to +10% upon the time of the release. We also integrate and optimize the entire suite of training methods for C-TEM. Along with our resources on general Chinese embedding, we release our data and models for English text embeddings. The English models achieve state-of-the-art performance on MTEB benchmark; meanwhile, our released English data is 2 times larger than the Chinese data\"\"\"\n",
    "\n",
    "# doc = \"\"\"Generative Representational Instruction Tuning. All text-based language problems can be reduced to either generation or embedding. Current models only perform well at one or the other. We introduce generative representational instruction tuning (GRIT) whereby a large language model is trained to handle both generative and embedding tasks by distinguishing between them through instructions. Compared to other open models, our resulting GritLM 7B sets a new state of the art on the Massive Text Embedding Benchmark (MTEB) and outperforms all models up to its size on a range of generative tasks. By scaling up further, GritLM 8x7B outperforms all open generative language models that we tried while still being among the best embedding models. Notably, we find that GRIT matches training on only generative or embedding data, thus we can unify both at no performance loss. Among other benefits, the unification via GRIT speeds up Retrieval-Augmented Generation (RAG) by > 60% for long documents, by no longer requiring separate retrieval and generation models\"\"\"\n",
    "\n",
    "doc = \"I work in the field of medical text mining and I require a system that can accurately extract relevant information from medical documents. First of all, the system needs a named entity recognition module that is capable of recognizing biomedical entities. I am currently thinking about using deep learning based approaches for the named entity recognition module, so I can achieve optimal performance. However, since human annotations are expensive and difficult to obtain, I want to explore options where my deep learning module does not need much labeled data, instead, it could train on unlabeled data as well.  Since in real-life, there is a large volume of medical documents, I want to reduce the amount of training time. Currently, I think I could use some weight sharing or weight transfer to reduce training time. My final goal is to beat current state of the art models as well as pre-trained models. Overall, efficiency and low-cost are two important factors in my data collection, model training and inference pipeline.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c66d32d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([59419])\n",
      "number of actual dimensions:  388\n",
      "I work in the field of medical text mining and I require a system that can accurately extract relevant information from medical documents. First of all, the system needs a named entity recognition module that is capable of recognizing biomedical entities. I am currently thinking about using deep learning based approaches for the named entity recognition module, so I can achieve optimal performance. However, since human annotations are expensive and difficult to obtain, I want to explore options where my deep learning module does not need much labeled data, instead, it could train on unlabeled data as well.  Since in real-life, there is a large volume of medical documents, I want to reduce the amount of training time. Currently, I think I could use some weight sharing or weight transfer to reduce training time. My final goal is to beat current state of the art models as well as pre-trained models. Overall, efficiency and low-cost are two important factors in my data collection, model training and inference pipeline.\n",
      "('training method', 2.03)\n",
      "('information extraction', 1.9)\n",
      "('entity recognition', 1.75)\n",
      "('inferences', 1.59)\n",
      "('narrative text', 1.34)\n",
      "('text mining', 1.28)\n",
      "('automatic extraction', 1.18)\n",
      "('training time', 1.14)\n",
      "('selection method', 1.14)\n",
      "('bioinformatics', 1.14)\n",
      "('data collection', 1.12)\n",
      "('pipelines', 1.12)\n",
      "('deep learning', 1.11)\n",
      "('transfer process', 1.08)\n",
      "('biomedical engineering', 1.07)\n",
      "('tagging', 1.05)\n",
      "('new system', 1.03)\n",
      "('system architecture', 1.0)\n",
      "('large volume', 0.96)\n",
      "('sentiment analysis', 0.91)\n",
      "('efficiency improvement', 0.84)\n",
      "('new models', 0.83)\n",
      "('relevant information', 0.82)\n",
      "('human body', 0.78)\n",
      "('total cost', 0.77)\n",
      "('data acquisition', 0.77)\n",
      "('machine learning', 0.75)\n",
      "('recognition system', 0.73)\n",
      "('health care', 0.71)\n",
      "('training system', 0.68)\n",
      "('optimal design', 0.66)\n",
      "('document image', 0.65)\n",
      "('automatic generation', 0.65)\n",
      "('nlp', 0.65)\n",
      "('annotations', 0.64)\n",
      "('natural language processing', 0.63)\n",
      "('life cycle cost', 0.63)\n",
      "('text classification', 0.62)\n",
      "('annotation', 0.6)\n",
      "('human population', 0.59)\n",
      "('tweets', 0.58)\n",
      "('medical literature', 0.57)\n",
      "('statistical inference', 0.56)\n",
      "('extraction method', 0.56)\n",
      "('generation method', 0.55)\n",
      "('parsing', 0.54)\n",
      "('information quality', 0.54)\n",
      "('topic modeling', 0.53)\n",
      "('toolkit', 0.51)\n",
      "('transfer reactions', 0.5)\n",
      "('high accuracy', 0.5)\n",
      "('lexicon', 0.48)\n",
      "('learning algorithm', 0.47)\n",
      "('brand name', 0.45)\n",
      "('effective communication', 0.45)\n",
      "('human genome', 0.44)\n",
      "('new name', 0.43)\n",
      "('optimal algorithm', 0.43)\n",
      "('language model', 0.41)\n",
      "('machine translation', 0.41)\n",
      "('adapter', 0.4)\n",
      "('automatic recognition', 0.39)\n",
      "('emr', 0.39)\n",
      "('medical science', 0.38)\n",
      "('opinion mining', 0.38)\n",
      "('medical education', 0.37)\n",
      "('data mining', 0.37)\n",
      "('technical efficiency', 0.37)\n",
      "('chronic illness', 0.36)\n",
      "('context information', 0.36)\n",
      "('medical profession', 0.36)\n",
      "('ontology', 0.34)\n",
      "('medical image', 0.34)\n",
      "('cost model', 0.34)\n",
      "('plagiarism', 0.34)\n",
      "('knowledge representation', 0.34)\n",
      "('performance evaluation', 0.33)\n",
      "('task performance', 0.3)\n",
      "('dermatology', 0.29)\n",
      "('modularization', 0.29)\n",
      "('prosody', 0.29)\n",
      "('medical information', 0.28)\n",
      "('statistical information', 0.28)\n",
      "('medical knowledge', 0.27)\n",
      "('grammars', 0.27)\n",
      "('additional information', 0.27)\n",
      "('mapping method', 0.26)\n",
      "('gene expression', 0.25)\n",
      "('weight distribution', 0.25)\n",
      "('mining method', 0.24)\n",
      "('weighting', 0.24)\n",
      "('forecasting', 0.24)\n",
      "('temporal information', 0.24)\n",
      "('medical technology', 0.23)\n",
      "('deduction', 0.23)\n",
      "('partitioning', 0.23)\n",
      "('modular structure', 0.22)\n",
      "('more information', 0.22)\n",
      "('labelling', 0.22)\n",
      "('chinese herbal medicine', 0.21)\n",
      "('identification method', 0.21)\n",
      "('personalization', 0.21)\n",
      "('telemedicine', 0.21)\n",
      "('life science', 0.21)\n",
      "('profiling', 0.2)\n",
      "('choice behavior', 0.19)\n",
      "('nursing profession', 0.19)\n",
      "('training session', 0.19)\n",
      "('cancer treatment', 0.19)\n",
      "('causal explanation', 0.19)\n",
      "('summaries', 0.19)\n",
      "('family physician', 0.19)\n",
      "('pattern matching', 0.19)\n",
      "('chirality', 0.18)\n",
      "('hemp', 0.17)\n",
      "('semantic information', 0.17)\n",
      "('bayesian network', 0.17)\n",
      "('genetic testing', 0.17)\n",
      "('semantic similarity', 0.17)\n",
      "('modularity', 0.16)\n",
      "('performance prediction', 0.16)\n",
      "('melamine', 0.15)\n",
      "('information needs', 0.15)\n",
      "('automatic system', 0.14)\n",
      "('ring system', 0.14)\n",
      "('monitoring system', 0.14)\n",
      "('chinese medicine', 0.14)\n",
      "('new information', 0.14)\n",
      "('biosynthesis', 0.13)\n",
      "('optimal control', 0.13)\n",
      "('fault diagnosis', 0.13)\n",
      "('high density', 0.13)\n",
      "('event detection', 0.12)\n",
      "('prediction method', 0.11)\n",
      "('blogging', 0.11)\n",
      "('hypertext', 0.11)\n",
      "('breast cancer', 0.1)\n",
      "('large scale', 0.09)\n",
      "('optimality', 0.09)\n",
      "('optimization model', 0.08)\n",
      "('cloning', 0.08)\n",
      "('user preferences', 0.08)\n",
      "('natural language', 0.07)\n",
      "('feature extraction', 0.07)\n",
      "('medical treatment', 0.07)\n",
      "('selection process', 0.07)\n",
      "('prototype system', 0.06)\n",
      "('semantic analysis', 0.06)\n",
      "('transmission rate', 0.06)\n",
      "('visual information', 0.06)\n",
      "('training programs', 0.06)\n",
      "('structural information', 0.06)\n",
      "('optimal location', 0.05)\n",
      "('loss reduction', 0.05)\n",
      "('survey data', 0.05)\n",
      "('control system', 0.04)\n",
      "('text analysis', 0.04)\n",
      "('mfcc', 0.04)\n",
      "('xml document', 0.04)\n",
      "('document management', 0.03)\n",
      "('site selection', 0.03)\n",
      "('knowledge level', 0.02)\n",
      "('research papers', 0.02)\n",
      "('information network', 0.02)\n",
      "('expert knowledge', 0.02)\n",
      "('training model', 0.02)\n",
      "('sensor data', 0.02)\n",
      "('sampler', 0.02)\n",
      "('text categorization', 0.01)\n",
      "('association analysis', 0.01)\n",
      "('relative efficiency', 0.0)\n",
      "('j2me', 0.0)\n",
      "('web content', 0.0)\n",
      "('data distribution', 0.0)\n",
      "('veterinary medicine', 0.0)\n"
     ]
    }
   ],
   "source": [
    "# # now compute the document representation\n",
    "# for punc in string.punctuation:\n",
    "#     doc = doc.replace(punc, \" \")\n",
    "    \n",
    "doc_tokens = tokenizer(doc, max_length = 256, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    doc_rep = model(d_kwargs=doc_tokens)[\"d_rep\"].squeeze()  # (sparse) doc rep in voc space, shape (30522,)\n",
    "    # print(torch.sum(doc_rep))\n",
    "    # doc_rep = encode_custom_mask_punc(doc_tokens, model).squeeze()\n",
    "print(doc_rep.shape)\n",
    "# get the number of non-zero dimensions in the rep:\n",
    "col = torch.nonzero(doc_rep).squeeze().cpu().tolist()\n",
    "print(\"number of actual dimensions: \", len(col))\n",
    "\n",
    "# now let's inspect the bow representation:\n",
    "weights = doc_rep[col].cpu().tolist()\n",
    "d = {k: v for k, v in zip(col, weights) if k >= model.original_bert_vocab_size}\n",
    "sorted_d = {k: v for k, v in sorted(d.items(), key=lambda item: item[1], reverse=True)}\n",
    "bow_rep = []\n",
    "\n",
    "print(doc)\n",
    "for k, v in sorted_d.items():\n",
    "    print((reverse_voc[k], round(v, 2)))\n",
    "    bow_rep.append((reverse_voc[k], round(v, 2)))\n",
    "# print(\"SPLADE BOW rep:\\n\", bow_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "116d09cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "188"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.tokenize(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "rolled-canon",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lamdo/splade/splade/models/transformer_rep.py:32: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast() if self.fp16 else NullContextManager():\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 190, 59419])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer(doc, return_tensors=\"pt\")\n",
    "out = encode_custom(tokens, model = model, is_q = True)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "30d358f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_str = [reverse_voc[int(idx)] for idx in tokens[\"input_ids\"][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b9f4971b",
   "metadata": {},
   "outputs": [],
   "source": [
    "row, col = torch.nonzero(out[0][:], as_tuple = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d3f331e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_mapper = [[tokens_str[j], Counter()] for j in range(max(row) + 1)]\n",
    "for r,c in zip(row, col):\n",
    "    r_token_id = int(tokens[\"input_ids\"][0][r])\n",
    "    r_token_str = reverse_voc[r_token_id]\n",
    "\n",
    "    temp = {}\n",
    "\n",
    "    c_token_id = int(c)\n",
    "    c_token_str = reverse_voc[c_token_id]\n",
    "    if c_token_str not in temp:\n",
    "        temp[c_token_str] = round(float(out[0][r, c]), 2)\n",
    "\n",
    "    token_mapper[r][0] = r_token_str\n",
    "    token_mapper[r][1].update(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "53d6a61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] -> {',': 1.2, '.': 1.05, 'the': 1.18, 'and': 0.96, 'family': 0.52, 'language': 0.35, 'word': 1.07, 'deep': 0.5, 'image': 0.52, 'gene': 0.24, 'soap': 1.51, 'logic': 0.05, 'enzyme': 0.52, 'algorithm': 0.25}\n",
      "colbert -> {',': 1.07, '.': 0.7, 'the': 1.35, 'and': 1.0, 'with': 0.05, 'even': 0.01, 'food': 0.53, 'broadcast': 0.08, 'birds': 0.09, 'breeding': 0.12, 'prehistoric': 0.21, 'meteor': 0.08, 'colbert': 1.46, 'climate change': 0.14, 'fermentation': 0.26, 'zeolite': 0.71, 'microwave irradiation': 0.15, 'ammonium': 0.28, 'volatility': 0.18, 'fiscal policy': 0.56, 'catalysis': 0.23, 'caffeine': 0.02, 'fluoride': 0.16, 'microwave heating': 0.4, 'wheat flour': 0.14, 'west africa': 0.76, 'trna': 0.75, 'fluorine': 0.59, 'cryptanalysis': 0.32, 'distillation': 0.38, 'cyclization': 0.12, 'nh3': 0.38, 'malware': 0.0, 'natural selection': 0.18, 'asr': 0.42, 'breastfeeding': 0.18, 'smt': 0.32, 'neutralization': 0.38, 'thiol': 0.51, 'folic acid': 0.29, 'zircon': 0.35, 'gypsum': 0.49, 'soy protein': 0.58, 'rnase': 0.05, 'caching': 0.01, 'forage': 0.1, 'argumentation': 0.26, 'new product': 0.25, 'srb': 0.01, 'deadlock': 0.0, 'yttrium': 0.22, 'melamine': 0.51, 'urease': 0.04, 'auctions': 0.06, 'language model': 0.53, 'job search': 0.25, 'meteorite': 0.54, 'moderation': 0.44, 'mechanical activation': 0.3, 'natural disaster': 0.08, 'hymenoptera': 0.47, 'japanese language': 0.2, 'blogging': 0.17, 'msr': 0.17, 'jammer': 0.05, 'sieve': 0.16, 'masking': 0.37, 'nitrogenase': 0.32, 'janus': 0.32, 'ochratoxin': 0.22, 'miso': 0.26, 'spillover': 0.04, 'spam': 0.28, 'food chain': 0.0, 'ribonuclease': 0.03, 'evasion': 0.45, 'interference effect': 0.03, 'bacillus thuringiensis': 0.08, 'harry potter': 0.2, 'other people': 0.27, 'toddler': 0.19, 'hard work': 0.0, 'condensation': 0.08, 'fusarium': 0.68, 'euphemism': 0.21, 'maser': 0.1, 'ratchet': 0.36, 'u.s.': 0.05, 'last week': 0.32, 'mycotoxin': 0.49, 'oseltamivir': 0.16, 'saccharin': 0.81, 'hydrothermal': 0.41, 'millet': 0.15, 'termite': 0.84}\n",
      "##v -> {',': 1.32, '.': 0.95, 'v': 0.53, 'the': 1.32, 'and': 1.1, '##v': 0.56, 'virus': 0.88, 'vc': 0.44, 'vp': 0.43, 'vanadium': 0.81, 'toolkit': 0.03, 'neutralization': 0.14, 'svc': 0.69, 'factor viii': 0.22, 'dcc': 0.04, 'drm': 0.04, 'aav': 0.54, 'frc': 0.22, 'same-sex marriage': 0.06, 'cpv': 0.53, 'herpesvirus': 0.19, 'pvr': 0.04, 'streamer': 0.04}\n",
      "##2 -> {',': 1.31, '.': 1.08, '2': 0.57, 'the': 1.21, 'and': 1.07, 'family': 0.08, 'game': 0.01, '##2': 0.85, 'training': 0.04, 'brazil': 0.08, 'gene': 0.29, 'logic': 0.07, 'toolkit': 0.83, 'cryptanalysis': 0.04, 'model checking': 0.05, 'malware': 0.01, 'japonica': 0.0, 'generation method': 0.45, 'potassium channel': 0.29, 'adapter': 0.25}\n",
      ": -> {',': 1.15, '.': 1.06, 'the': 1.18, 'and': 0.84, 'family': 0.3, 'word': 0.16, 'deep': 0.4, 'soap': 1.37, 'chip': 0.08, 'algorithm': 0.14}\n",
      "effective -> {\"'\": 0.0, ',': 1.2, '.': 0.91, 'the': 1.33, 'and': 1.12, 'effective': 1.05, 'efficient': 0.43, 'efficiency': 0.31, 'algorithm': 0.27, 'effectiveness': 0.18, 'high efficiency': 0.12, 'efficiency improvement': 0.42}\n",
      "and -> {',': 0.98, '.': 0.92, 'the': 1.28, 'and': 1.05, 'soap': 1.09, 'enzyme': 0.22, 'algorithm': 0.38}\n",
      "efficient -> {',': 1.09, '.': 0.93, 'the': 1.33, 'and': 1.13, 'effective': 0.31, 'acid': 0.26, 'efficient': 0.84, 'efficiency': 0.6, 'enzyme': 0.12, 'algorithm': 0.46, 'efficiently': 0.0, 'high efficiency': 0.23, 'efficiency improvement': 0.5}\n",
      "retrieval -> {',': 1.28, '.': 1.19, 'the': 1.28, 'and': 1.15, 'for': 0.05, 'source': 0.04, 'memory': 0.11, 'online': 0.09, 'search': 0.65, 'searching': 0.17, 'algorithm': 0.38, 'fusion': 0.24, 'retrieve': 0.09, 'extraction': 0.15, 'prediction': 0.03, 'query': 0.0, 'retrieval': 1.29, 'new algorithm': 0.02, 'information retrieval': 0.6, 'decoding': 0.03, 'trna': 0.37, 'machine translation': 0.04, 'deep learning': 0.19, 'distillation': 0.06, 'information extraction': 0.19, 'data structure': 0.39, 'information fusion': 0.23, 'vector quantization': 0.23, 'extraction method': 0.31, 'semantic similarity': 0.02, 'storage system': 0.39, 'search algorithm': 0.3, 'tagging': 0.24, 'cbir': 0.13, 'automatic extraction': 0.57, 'indexing': 0.28, 'caching': 0.01, 'retrieval system': 0.84, 'language model': 0.22, 'learning algorithm': 0.46, 'pattern matching': 0.17, 'channel coding': 0.26, 'inversion method': 0.19, 'plagiarism': 0.02, 'vector space': 0.05, 'hash table': 0.37, 'collaborative filtering': 0.15, 'generation method': 0.03, 'retrieval method': 0.96}\n",
      "via -> {',': 0.99, '.': 1.01, 'the': 1.25, 'and': 0.83, 'magic': 0.0, 'soap': 1.2, 'algorithm': 0.4}\n",
      "lightweight -> {',': 1.23, '.': 1.01, 'the': 1.4, 'and': 1.16, 'with': 0.01, 'including': 0.04, 'using': 0.08, 'proposed': 0.31, 'considering': 0.17, 'warner': 0.14, 'lightweight': 1.41, 'soy protein': 0.24, 'high strength': 0.02, 'encapsulation': 0.21, 'bagging': 0.1, 'modularization': 0.03}\n",
      "late -> {',': 1.25, '.': 0.98, 'the': 1.36, 'and': 1.18, 'time': 0.06, 'early': 0.43, 'late': 1.32, 'trna': 0.52, 'higher order': 0.57, 'fluorine': 0.27, 'alkali': 0.03, 'asr': 0.35, 'smt': 0.49, 'zircon': 0.17, 'gypsum': 0.43, 'early stage': 0.03, 'language model': 0.45, 'masking': 0.26, 'miso': 0.11, 'mdi': 0.23, 'lexicon': 0.01, 'condensation': 0.1, 'fusarium': 0.06, 'pyrimidine': 0.02, 'euphemism': 0.09, 'polyol': 0.41, 'ratchet': 0.11, 'noun phrase': 0.11, 'oseltamivir': 0.0, 'long term': 0.07, 'saccharin': 0.36}\n",
      "interaction -> {',': 1.29, '.': 0.98, 'the': 1.22, 'and': 1.11, 'only': 0.03, 'game': 0.26, 'system': 0.03, 'association': 0.22, 'alone': 0.32, 'action': 0.09, 'network': 0.16, 'movement': 0.23, 'online': 0.43, 'exchange': 0.11, 'communication': 0.04, 'interaction': 1.28, 'interactions': 0.6, 'interact': 0.13, 'complex formation': 0.22, 'linkage': 0.11, 'social interaction': 0.26, 'hybridization': 0.38, 'hybrid system': 0.21, 'synergy': 0.01, 'interface design': 0.28, 'neutralization': 0.01, 'parallel mechanism': 0.15, 'interactivity': 0.23, 'interface structure': 0.02, 'dialogue system': 0.25, 'molecular docking': 0.34, 'fret': 0.29, 'interference effect': 0.12, 'interaction model': 1.14, 'interaction effects': 0.37, 'adapter': 0.55, 'dynamic interaction': 0.21}\n",
      ". -> {',': 1.37, '.': 1.23, 'the': 1.25, 'and': 1.2, 'magic': 0.22, 'soap': 0.34, 'enzyme': 0.04}\n",
      "neural -> {',': 1.3, '.': 1.13, 'the': 1.38, 'and': 1.15, 'such': 0.01, 'training': 0.3, 'word': 0.34, 'deep': 0.31, 'network': 0.42, 'brain': 0.8, 'smart': 0.07, 'neurons': 0.24, 'neural': 1.1, 'neural network': 0.98, 'artificial intelligence': 0.37, 'artificial neural network': 0.21, 'neural networks': 0.22, 'emg': 0.18, 'nervous system': 0.09, 'multilayer': 0.58, 'substance p': 0.05, 'nerve growth factor': 0.21, 'deep learning': 0.47, 'mouse brain': 0.27, 'facial nerve': 0.25, 'piaget': 0.11, 'electronic nose': 0.16, 'memristor': 0.4, 'neural stem cells': 0.23, 'hopfield neural network': 0.53, 'neurogenesis': 0.05, 'synapse': 0.46, 'back propagation': 0.41, 'gallium nitride': 0.18, 'auditory nerve': 0.39, 'perceptron': 0.01, 'cellular neural network': 0.05, 'piezoelectric': 0.31}\n",
      "information retrieval -> {',': 1.32, '.': 1.06, 'e': 0.18, 'the': 1.28, 'and': 1.21, 'in': 0.05, 'it': 0.15, 'or': 0.1, 'light': 0.2, 'information': 0.64, 'science': 0.13, 'word': 0.55, 'reading': 0.04, 'search': 0.77, 'acid': 0.04, 'chemistry': 0.21, 'acoustic': 0.15, 'searching': 0.05, 'em': 0.27, 'er': 0.43, 'ara': 0.22, 'query': 0.18, 'ur': 0.28, 'retrieval': 1.08, 'information retrieval': 1.25, 'organic chemistry': 0.07, 'trna': 0.38, 'information science': 0.22, 'information processing': 0.26, 'information theory': 0.45, 'information extraction': 0.58, 'indexing': 0.45, 'retrieval system': 0.68, 'information transfer': 0.54, 'document retrieval': 0.1, 'information retrieval system': 0.15, 'retrieval method': 0.49}\n",
      "( -> {',': 1.0, '.': 0.85, 'the': 1.3, 'and': 0.8, 'soap': 0.95, 'enzyme': 0.06, 'algorithm': 0.26}\n",
      "ir -> {',': 1.26, '.': 0.91, 'the': 1.29, 'and': 1.17, 'open': 0.02, 'er': 0.04, 'infrared': 0.79, 'ir': 1.17, 'irs': 0.02, 'iridium': 1.2, 'infrared radiation': 0.08, 'asr': 0.12, 'ir spectroscopy': 0.15, 'ihc': 0.13, 'erbium': 0.07, 'thermal imaging': 0.4, 'infrared imaging': 0.22, 'allium cepa': 0.21}\n",
      ") -> {',': 0.98, '.': 0.85, 'the': 1.37, 'and': 0.89, 'soap': 0.59, 'enzyme': 0.06, 'chip': 0.03, 'algorithm': 0.16}\n",
      "has -> {',': 1.01, '.': 0.85, 'the': 1.24, 'and': 0.85, 'soap': 0.98, 'enzyme': 0.04, 'algorithm': 0.3}\n",
      "greatly -> {',': 1.09, '.': 0.86, 'the': 1.35, 'and': 1.05}\n",
      "advanced -> {',': 1.1, '.': 0.86, 'the': 1.35, 'and': 1.02, 'advanced': 0.13}\n",
      "search -> {',': 1.22, '.': 0.98, 'the': 1.31, 'and': 1.09, 'search': 0.92, 'searching': 0.09}\n",
      "and -> {',': 0.87, '.': 0.67, 'the': 1.31, 'and': 0.97, 'soap': 0.63}\n",
      "other -> {',': 0.88, '.': 0.69, 'the': 1.32, 'and': 0.81, 'soap': 0.38}\n",
      "knowledge -> {',': 1.22, '.': 1.0, 'the': 1.3, 'and': 1.12, 'knowledge': 0.03}\n",
      "- -> {',': 0.92, '.': 0.74, 'the': 1.26, 'and': 0.79, 'soap': 0.47}\n",
      "intensive -> {',': 1.09, '.': 0.78, 'the': 1.28, 'and': 0.99, 'intensive': 0.3}\n",
      "language -> {',': 1.27, '.': 1.04, 'the': 1.36, 'and': 1.14, 'language': 0.87, 'word': 0.26, 'c language': 0.52}\n",
      "tasks -> {',': 1.23, '.': 1.08, 'the': 1.24, 'and': 1.04, 'soap': 0.09, 'tasks': 0.01}\n",
      ". -> {',': 1.37, '.': 1.23, 'the': 1.25, 'and': 1.2, 'magic': 0.2, 'soap': 0.26, 'enzyme': 0.03}\n",
      "while -> {',': 1.07, '.': 0.99, 'the': 1.31, 'and': 0.83, 'soap': 0.49}\n",
      "many -> {',': 0.87, '.': 0.74, 'the': 1.26, 'and': 0.68, 'soap': 0.9, 'enzyme': 0.19}\n",
      "neural -> {',': 1.03, '.': 0.83, 'the': 1.36, 'and': 0.94}\n",
      "ir -> {',': 1.0, '.': 0.46, 'the': 1.28, 'and': 0.94}\n",
      "methods -> {',': 1.25, '.': 1.04, 'the': 1.23, 'and': 1.04, 'method': 0.04, 'methods': 0.51, 'new methods': 0.45}\n",
      "en -> {',': 0.96, '.': 0.68, 'the': 1.26, 'and': 0.81, 'gene': 0.18, 'soap': 0.32, 'enzyme': 0.09}\n",
      "##code -> {',': 1.2, '.': 0.92, 'the': 1.32, 'and': 1.05, 'code': 0.39, '##code': 0.21}\n",
      "queries -> {',': 1.28, '.': 0.87, 'a': 0.18, 'the': 1.32, 'and': 1.08, 'in': 0.03, 'for': 0.06, 'only': 0.44, 'both': 0.01, 'water': 0.33, 'open': 0.1, 'query': 1.15, 'queries': 1.12}\n",
      "and -> {',': 1.05, '.': 0.82, 'the': 1.26, 'and': 1.11, 'word': 0.1, 'soap': 0.62}\n",
      "documents -> {',': 1.34, '.': 1.03, 'a': 0.03, 'the': 1.37, 'and': 1.2, 'also': 0.08, 'about': 0.02, 'only': 0.2, 'form': 0.17, 'documents': 0.82, 'document': 0.03, 'hypertext': 0.18, 'summaries': 0.26}\n",
      "into -> {',': 1.0, '.': 0.75, 'the': 1.32, 'and': 0.89}\n",
      "single -> {',': 1.17, '.': 0.93, 'the': 1.31, 'and': 1.07, 'single': 0.7}\n",
      "- -> {',': 0.86, '.': 0.67, 'the': 1.31, 'and': 0.76, 'soap': 0.58, 'enzyme': 0.03}\n",
      "vector -> {',': 1.21, '.': 0.93, 'the': 1.33, 'and': 1.07, 'vector': 1.11, 'vectors': 0.12, 'vector space': 0.55, 'sparse matrix': 0.17}\n",
      "representations -> {',': 1.27, '.': 1.04, 'the': 1.28, 'and': 1.04, 'representation': 0.2, 'representations': 0.75, 'basis functions': 0.13, 'grammars': 0.04, 'irreducible representations': 0.7, 'generation method': 0.12}\n",
      ", -> {',': 1.16, '.': 0.87, 'the': 1.32, 'and': 0.89, 'soap': 0.55, 'enzyme': 0.29}\n",
      "late -> {',': 1.01, '.': 0.78, 'the': 1.32, 'and': 0.92, 'late': 1.07}\n",
      "interaction model -> {',': 1.03, '.': 0.71, 'the': 1.18, 'and': 0.93, 'couple': 0.32, 'exchange': 0.23, 'fit': 0.7}\n",
      "s -> {',': 1.06, '.': 0.9, 's': 0.36, 'the': 1.26, 'and': 0.92, 'magic': 0.06, 'soap': 0.82, 'algorithm': 0.32}\n",
      "produce -> {',': 1.04, '.': 0.84, 'the': 1.3, 'and': 0.9}\n",
      "multi -> {',': 1.1, '.': 0.86, 'the': 1.3, 'and': 1.01, 'single': 0.09}\n",
      "- -> {',': 0.76, '.': 0.65, 'the': 1.32, 'and': 0.68, 'soap': 0.42}\n",
      "vector -> {',': 1.02, '.': 0.71, 'the': 1.32, 'and': 0.89, 'vector': 0.59}\n",
      "representations -> {',': 1.12, '.': 0.9, 'the': 1.27, 'and': 0.89, 'magic': 0.05, 'pictures': 0.03, 'representations': 0.56, 'irreducible representations': 0.15}\n",
      "at -> {',': 0.97, '.': 0.75, 'the': 1.29, 'and': 0.87, 'magic': 0.15, 'soap': 0.05}\n",
      "the -> {',': 0.85, '.': 0.7, 'the': 1.45, 'and': 0.78, 'soap': 0.46}\n",
      "gran -> {',': 1.2, '.': 0.96, 'the': 1.32, 'and': 1.08, 'gran': 0.96, 'rough set': 0.06, 'granule': 0.23}\n",
      "##ular -> {',': 1.13, '.': 0.86, 'the': 1.31, 'and': 0.94}\n",
      "##ity -> {',': 1.06, '.': 0.76, 'the': 1.32, 'and': 0.89}\n",
      "of -> {',': 0.9, '.': 0.63, 'the': 1.3, 'and': 0.8, 'soap': 0.35}\n",
      "each -> {',': 0.88, '.': 0.74, 'the': 1.31, 'and': 0.81, 'soap': 0.27}\n",
      "token -> {',': 1.32, '.': 0.95, 'the': 1.26, 'and': 1.19, 'token': 1.29, 'template': 0.06, 'keypad': 0.08}\n",
      "and -> {',': 1.16, '.': 1.02, 'the': 1.25, 'and': 1.14, 'soap': 0.6, 'enzyme': 0.04, 'algorithm': 0.26}\n",
      "deco -> {',': 1.23, '.': 1.04, 'the': 1.35, 'and': 1.13, 'deco': 0.53}\n",
      "##mp -> {',': 1.08, '.': 0.96, 'the': 1.32, 'and': 1.02, 'algorithm': 0.01}\n",
      "##ose -> {',': 0.99, '.': 0.88, 'the': 1.37, 'and': 0.84}\n",
      "relevance -> {',': 1.2, '-': 0.08, '.': 0.92, 'a': 0.1, 'the': 1.39, 'and': 1.17, 'for': 0.02, 'this': 0.07, 'some': 0.01, 'justice': 0.17, 'significance': 0.38, 'relevant': 0.32, 'evaluation': 0.36, 'relevance': 1.12, 'relative importance': 0.18, 'semantic similarity': 0.42}\n",
      "modeling -> {',': 1.21, '.': 1.01, 'the': 1.23, 'and': 1.02, 'model': 0.26, 'modeling method': 0.48}\n",
      "into -> {',': 0.93, '.': 0.78, 'the': 1.31, 'and': 0.81, 'soap': 0.14, 'enzyme': 0.14, 'algorithm': 0.3}\n",
      "scala -> {',': 1.23, '.': 1.06, 'the': 1.34, 'and': 1.1, 'scala': 0.9}\n",
      "##ble -> {',': 0.93, '.': 0.8, 'the': 1.34, 'and': 0.82, 'enzyme': 0.07}\n",
      "token -> {',': 1.25, '-': 0.07, '.': 0.91, 'the': 1.28, 'and': 1.2, 'all': 0.19, 'token': 1.27, 'dialogue system': 0.09, 'state machine': 0.2, 'keypad': 0.21}\n",
      "- -> {',': 0.95, '.': 0.89, 'the': 1.25, 'and': 0.77, 'magic': 0.06, 'soap': 0.96, 'algorithm': 0.33}\n",
      "level -> {',': 1.11, '.': 0.98, 'the': 1.3, 'and': 1.02, 'based': 0.16, 'level': 0.52}\n",
      "computations -> {',': 1.25, '.': 1.14, 'the': 1.28, 'and': 1.08, 'for': 0.03, 'magic': 0.49, 'calculations': 0.42, 'computation': 0.34, 'parallel computing': 0.25, 'computations': 1.34}\n",
      ". -> {',': 1.38, '.': 1.23, 'the': 1.25, 'and': 1.2, 'magic': 0.2, 'soap': 0.28, 'enzyme': 0.0}\n",
      "this -> {',': 0.85, '.': 0.8, 'the': 1.27, 'and': 0.67, 'soap': 0.92, 'enzyme': 0.31, 'algorithm': 0.23}\n",
      "decomposition -> {',': 1.22, '.': 1.01, 'the': 1.21, 'and': 1.11, 'decomposition': 1.13, 'decomposition method': 0.75, 'decompositions': 0.97, 'decomposition reaction': 0.44, 'generation method': 0.14}\n",
      "has -> {',': 0.83, '.': 0.69, 'the': 1.22, 'and': 0.76, 'soap': 0.83, 'algorithm': 0.31}\n",
      "been -> {',': 0.77, '.': 0.67, 'the': 1.31, 'and': 0.72, 'soap': 0.56, 'enzyme': 0.03, 'algorithm': 0.27}\n",
      "shown -> {',': 0.81, '.': 0.64, 'the': 1.33, 'and': 0.77}\n",
      "to -> {',': 0.72, '.': 0.65, 'the': 1.31, 'and': 0.57, 'soap': 0.88, 'enzyme': 0.21, 'algorithm': 0.33}\n",
      "make -> {',': 0.71, '.': 0.64, 'the': 1.37, 'and': 0.64, 'soap': 0.42, 'algorithm': 0.12}\n",
      "late -> {',': 0.82, '.': 0.46, 'the': 1.36, 'and': 0.79, 'late': 0.72}\n",
      "interaction -> {',': 0.73, '.': 0.45, 'the': 1.37, 'and': 0.74}\n",
      "more -> {',': 0.96, '.': 0.84, 'the': 1.33, 'and': 0.85, 'magic': 0.09, 'soap': 0.66, 'algorithm': 0.26}\n",
      "effective -> {',': 0.94, '.': 0.68, 'the': 1.28, 'and': 0.82}\n",
      ", -> {',': 1.27, '.': 0.95, 'the': 1.31, 'and': 0.93, 'magic': 0.09, 'soap': 0.63, 'enzyme': 0.23, 'algorithm': 0.02}\n",
      "but -> {',': 1.03, '.': 0.96, 'the': 1.3, 'and': 0.93, 'soap': 0.22, 'algorithm': 0.17}\n",
      "it -> {',': 0.87, '.': 0.78, 'the': 1.2, 'and': 0.77, 'soap': 1.15, 'enzyme': 0.28, 'algorithm': 0.5}\n",
      "in -> {',': 0.69, '.': 0.53, 'the': 1.31, 'and': 0.67}\n",
      "##fl -> {',': 1.13, '.': 1.01, 'the': 1.37, 'and': 1.04}\n",
      "##ates -> {',': 0.78, '.': 0.71, 'the': 1.31, 'and': 0.78, 'algorithm': 0.08}\n",
      "the -> {',': 0.86, '.': 0.76, 'the': 1.48, 'and': 0.77, 'soap': 0.86, 'enzyme': 0.13, 'algorithm': 0.44}\n",
      "space -> {',': 1.24, '.': 1.1, 'the': 1.45, 'and': 1.13, 'space': 0.58, 'total energy': 0.18, 'physical space': 0.08}\n",
      "footprint -> {',': 1.21, '.': 0.92, 'the': 1.2, 'and': 1.08, 'cost': 0.13, 'waste': 0.15, 'footprint': 1.01, 'footprints': 0.17}\n",
      "of -> {',': 0.89, '.': 0.81, 'the': 1.28, 'and': 0.74, 'word': 0.17, 'soap': 1.2, 'enzyme': 0.44, 'chip': 0.17, 'algorithm': 0.6}\n",
      "these -> {',': 0.72, '.': 0.66, 'the': 1.22, 'and': 0.69, 'soap': 1.02, 'enzyme': 0.13, 'algorithm': 0.15}\n",
      "models -> {',': 1.11, '.': 1.01, 'the': 1.31, 'and': 0.98, 'women': 0.09, 'model': 0.36, 'magic': 0.32, 'models': 0.44, 'new models': 0.01}\n",
      "by -> {',': 0.91, '.': 0.79, 'the': 1.33, 'and': 0.88, 'algorithm': 0.19}\n",
      "an -> {',': 0.65, '.': 0.52, 'the': 1.34, 'and': 0.71, 'soap': 0.38, 'enzyme': 0.2, 'algorithm': 0.08}\n",
      "order -> {',': 1.07, '.': 0.93, 'the': 1.39, 'and': 1.11}\n",
      "of -> {',': 0.67, '.': 0.56, 'the': 1.4, 'and': 0.76, 'soap': 0.42, 'enzyme': 0.29, 'algorithm': 0.19}\n",
      "magnitude -> {',': 1.15, '.': 1.0, 'the': 1.37, 'and': 1.12, 'magnitude': 0.71}\n",
      ". -> {',': 1.38, '.': 1.23, 'the': 1.26, 'and': 1.2, 'magic': 0.23, 'soap': 0.31, 'enzyme': 0.01}\n",
      "in -> {',': 0.73, '.': 0.61, 'the': 1.35, 'and': 0.6, 'soap': 0.92, 'enzyme': 0.27}\n",
      "this -> {',': 0.72, '.': 0.64, 'the': 1.34, 'and': 0.55, 'soap': 0.89, 'enzyme': 0.24}\n",
      "work -> {',': 0.79, '.': 0.59, 'the': 1.26, 'and': 0.68, 'soap': 0.83, 'enzyme': 0.16}\n",
      ", -> {',': 1.31, '.': 0.9, 'the': 1.28, 'and': 0.96, 'soap': 0.59, 'enzyme': 0.29}\n",
      "we -> {',': 0.86, '.': 0.74, 'the': 1.32, 'and': 0.65, 'soap': 1.01, 'enzyme': 0.04}\n",
      "introduce -> {',': 0.81, '.': 0.78, 'the': 1.23, 'and': 0.7, 'soap': 0.93, 'algorithm': 0.01}\n",
      "colbert -> {',': 0.89, '.': 0.3, 'the': 1.25, 'and': 0.69}\n",
      "##v -> {',': 0.89, '.': 0.43, 'the': 1.29, 'and': 0.86}\n",
      "##2 -> {',': 0.95, '.': 0.55, 'the': 1.21, 'and': 0.84, 'family': 0.11, 'gene': 0.04, 'soap': 0.2}\n",
      ", -> {',': 1.23, '.': 0.9, 'the': 1.17, 'and': 0.79, 'family': 0.21, 'word': 0.06, 'soap': 1.31, 'enzyme': 0.0, 'algorithm': 0.24}\n",
      "a -> {',': 0.97, '.': 0.86, 'the': 1.24, 'and': 0.74, 'family': 0.15, 'word': 0.2, 'deep': 0.1, 'soap': 1.35, 'enzyme': 0.12, 'algorithm': 0.21}\n",
      "retrieve -> {',': 1.09, '.': 0.81, 'the': 1.39, 'and': 1.02, 'free': 0.06, 'exchange': 0.13, 'tea': 0.12, 'retrieve': 0.88, 'retrieval': 0.23}\n",
      "##r -> {',': 1.05, '.': 0.85, 'the': 1.18, 'and': 0.84, '##r': 0.24, 'soap': 1.05, 'algorithm': 0.37}\n",
      "tha -> {',': 0.94, '.': 0.83, 'the': 1.19, 'and': 0.69, 'soap': 1.13, 'enzyme': 0.4, 'algorithm': 0.17}\n",
      "t c -> {',': 0.9, '.': 0.75, 'the': 1.32, 'and': 0.89}\n",
      "ou -> {',': 0.69, '.': 0.56, 'the': 1.27, 'and': 0.72}\n",
      "##ples -> {',': 0.87, '.': 0.61, 'the': 1.18, 'and': 0.83}\n",
      "an -> {',': 0.84, '.': 0.76, 'the': 1.27, 'and': 0.66, 'soap': 1.11, 'enzyme': 0.41, 'algorithm': 0.21}\n",
      "aggressive -> {',': 1.3, '.': 1.05, 'the': 1.32, 'and': 1.16, 'attack': 0.0, 'advanced': 0.03, 'aggressive': 0.96, 'aggressive behavior': 0.6}\n",
      "residual -> {',': 1.22, '.': 0.92, 'the': 1.33, 'and': 1.14, 'de': 0.6, 'remaining': 0.35, 'residual': 1.52, 'higher order': 0.19}\n",
      "compression -> {',': 1.24, '.': 1.0, 'the': 1.29, 'and': 1.1, 'compression': 1.36, 'compressed': 0.49, 'shrinkage': 0.08, 'compression ratio': 0.97, 'compressibility': 0.16, 'data compression': 0.74, 'lossless compression': 0.55, 'channel coding': 0.25, 'jpeg': 0.25, 'decompression': 0.46, 'compression algorithm': 0.82, 'arithmetic coding': 0.14}\n",
      "mechanism -> {',': 1.25, '.': 0.98, 'the': 1.26, 'and': 1.11, 'mechanism': 0.74, 'mechanisms': 0.04, 'new mechanism': 0.83}\n",
      "with -> {',': 1.06, '.': 0.84, 'the': 1.13, 'and': 1.04, 'soap': 0.5, 'enzyme': 0.1, 'algorithm': 0.37}\n",
      "a -> {',': 0.87, '.': 0.76, 'the': 1.28, 'and': 0.78, 'soap': 0.73, 'enzyme': 0.21, 'algorithm': 0.29}\n",
      "den -> {',': 1.23, '.': 1.04, 'the': 1.31, 'and': 1.14, 'den': 0.8}\n",
      "##oise -> {',': 1.19, '.': 0.95, 'the': 1.28, 'and': 1.09, '##oise': 0.44}\n",
      "##d -> {',': 0.87, '.': 0.65, 'the': 1.32, 'and': 0.81, 'soap': 0.08, 'enzyme': 0.26, 'algorithm': 0.11}\n",
      "supervision -> {',': 1.22, '.': 0.99, 'the': 1.28, 'and': 1.07, 'control': 0.09, 'scott': 0.07, 'supervision': 0.91, 'supervisory': 0.22, 'supervision system': 0.27}\n",
      "strategy -> {',': 1.2, '.': 0.96, 'the': 1.27, 'and': 1.04, 'strategy': 0.36}\n",
      "to -> {',': 1.0, '.': 0.93, 'the': 1.25, 'and': 0.83, 'soap': 0.92, 'enzyme': 0.33, 'algorithm': 0.46}\n",
      "simultaneously -> {',': 0.98, '.': 0.88, 'the': 1.32, 'and': 0.89}\n",
      "improve -> {',': 0.99, '.': 0.87, 'the': 1.31, 'and': 0.97}\n",
      "the -> {',': 0.91, '.': 0.84, 'the': 1.47, 'and': 0.75, 'soap': 1.0, 'enzyme': 0.0, 'algorithm': 0.32}\n",
      "quality -> {',': 1.25, '.': 1.04, 'the': 1.33, 'and': 1.09, 'quality': 0.64}\n",
      "and -> {',': 0.88, '.': 0.78, 'the': 1.38, 'and': 0.97, 'soap': 0.59, 'algorithm': 0.12}\n",
      "space -> {',': 1.1, '.': 0.99, 'the': 1.46, 'and': 1.0, 'land': 0.03, 'space': 0.47}\n",
      "footprint -> {',': 1.17, '.': 0.94, 'the': 1.23, 'and': 1.02, 'footprint': 0.99, 'footprints': 0.19}\n",
      "of -> {',': 0.98, '.': 0.92, 'the': 1.31, 'and': 0.75, 'training': 0.06, 'word': 0.58, 'image': 0.08, 'soap': 1.27, 'enzyme': 0.37, 'algorithm': 0.42}\n",
      "late -> {',': 0.86, '.': 0.69, 'the': 1.4, 'and': 0.88, 'late': 0.48}\n",
      "interaction -> {',': 0.78, '.': 0.48, 'the': 1.35, 'and': 0.82}\n",
      ". -> {',': 1.37, '.': 1.23, 'the': 1.24, 'and': 1.2, 'magic': 0.18, 'soap': 0.31, 'enzyme': 0.06}\n",
      "we -> {',': 0.93, '.': 0.78, 'the': 1.28, 'and': 0.79, 'soap': 0.89, 'enzyme': 0.27}\n",
      "evaluate -> {',': 1.11, '.': 1.01, 'the': 1.32, 'and': 1.03, 'test': 0.16, 'evaluation': 0.5, 'evaluate': 0.32}\n",
      "colbert -> {',': 0.92, '.': 0.38, 'the': 1.3, 'and': 0.83, 'food': 0.2}\n",
      "##v -> {',': 0.86, '.': 0.44, 'the': 1.41, 'and': 0.85}\n",
      "##2 -> {',': 0.89, '.': 0.57, 'the': 1.34, 'and': 0.79, 'gene': 0.12, 'soap': 0.47, 'algorithm': 0.07}\n",
      "across -> {',': 1.01, '.': 0.8, 'the': 1.24, 'and': 0.89, 'soap': 0.57}\n",
      "a -> {',': 0.77, '.': 0.61, 'the': 1.29, 'and': 0.69, 'soap': 0.96, 'enzyme': 0.03}\n",
      "wide range -> {',': 1.15, '.': 0.95, 'the': 1.26, 'and': 1.05, 'set': 0.17, 'wide': 0.35, 'wide range': 0.21}\n",
      "of -> {',': 0.73, '.': 0.55, 'the': 1.24, 'and': 0.68, 'soap': 1.02, 'enzyme': 0.31}\n",
      "benchmarks -> {',': 1.16, '.': 1.0, 'the': 1.24, 'and': 1.06, 'set': 0.24, 'competition': 0.03, 'bar': 0.44, 'benchmark': 1.03, 'benchmarking': 0.11, 'frameworks': 0.32, 'benchmarks': 1.12}\n",
      ", -> {',': 1.38, '.': 1.01, 'the': 1.28, 'and': 0.92, 'soap': 0.56, 'enzyme': 0.3}\n",
      "establishing -> {',': 1.06, '.': 0.97, 'the': 1.34, 'and': 1.02}\n",
      "state -> {',': 1.17, '.': 0.97, 'the': 1.37, 'and': 1.07, 'state': 0.45}\n",
      "- -> {',': 0.89, '.': 0.72, 'the': 1.26, 'and': 0.69, 'soap': 0.92, 'algorithm': 0.21}\n",
      "of -> {',': 0.7, '.': 0.43, 'the': 1.5, 'and': 0.83, 'soap': 0.41, 'enzyme': 0.39}\n",
      "- -> {',': 0.79, '.': 0.74, 'the': 1.31, 'and': 0.65, 'soap': 0.94, 'enzyme': 0.23, 'algorithm': 0.23}\n",
      "the -> {',': 0.72, '.': 0.47, 'the': 1.62, 'and': 0.81, 'soap': 0.57, 'enzyme': 0.38}\n",
      "- -> {',': 0.8, '.': 0.82, 'the': 1.37, 'and': 0.61, 'soap': 0.98, 'enzyme': 0.2, 'algorithm': 0.15}\n",
      "art -> {',': 1.0, '.': 0.79, 'the': 1.37, 'and': 0.97, 'art': 0.4}\n",
      "quality -> {',': 1.22, '.': 1.04, 'the': 1.31, 'and': 1.02, 'quality': 0.71}\n",
      "within -> {',': 1.12, '.': 0.88, 'the': 1.3, 'and': 1.05}\n",
      "and -> {',': 1.01, '.': 0.9, 'the': 1.35, 'and': 1.2, 'soap': 0.35, 'enzyme': 0.11}\n",
      "outside -> {',': 1.07, '.': 0.94, 'the': 1.35, 'and': 1.01}\n",
      "the -> {',': 0.9, '.': 0.84, 'the': 1.43, 'and': 0.74, 'training': 0.14, 'magic': 0.01, 'soap': 1.06, 'enzyme': 0.54, 'chip': 0.08, 'algorithm': 0.14}\n",
      "training -> {',': 1.19, '.': 1.1, 'the': 1.28, 'and': 1.04, 'training': 1.22, 'trained': 0.01, 'training system': 0.01, 'training method': 0.61}\n",
      "domain -> {',': 1.01, '.': 0.91, 'the': 1.3, 'and': 0.94}\n",
      "while -> {',': 1.07, '.': 1.01, 'the': 1.34, 'and': 0.99}\n",
      "reducing -> {',': 1.1, '.': 1.04, 'the': 1.33, 'and': 1.03}\n",
      "the -> {',': 0.92, '.': 0.85, 'the': 1.46, 'and': 0.81, 'soap': 0.69, 'enzyme': 0.02, 'chip': 0.02, 'algorithm': 0.36}\n",
      "space -> {',': 1.12, '.': 1.03, 'the': 1.46, 'and': 1.0, 'space': 0.34}\n",
      "footprint -> {',': 1.09, '.': 0.84, 'the': 1.2, 'and': 0.96}\n",
      "of -> {',': 0.96, '.': 0.88, 'the': 1.3, 'and': 0.78, 'word': 0.21, 'soap': 1.09, 'enzyme': 0.49, 'chip': 0.03, 'algorithm': 0.49}\n",
      "late -> {',': 0.88, '.': 0.69, 'the': 1.4, 'and': 0.84, 'late': 0.4}\n",
      "interaction model -> {',': 0.87, '.': 0.57, 'the': 1.19, 'and': 0.78, 'fit': 0.49}\n",
      "s -> {',': 0.93, '.': 0.79, 's': 0.25, 'the': 1.29, 'and': 0.78, 'soap': 0.87, 'algorithm': 0.36}\n",
      "by -> {',': 0.99, '.': 0.89, 'the': 1.31, 'and': 0.87, 'algorithm': 0.17}\n",
      "6 -> {',': 1.13, '.': 0.91, '6': 0.15, 'the': 1.33, 'and': 1.04}\n",
      "- -> {',': 0.93, '.': 0.83, 'the': 1.37, 'and': 0.97}\n",
      "- -> {',': 0.77, '.': 0.64, 'the': 1.35, 'and': 0.87}\n",
      "10 -> {',': 1.14, '.': 0.97, 'the': 1.35, 'and': 1.09, '10': 0.36}\n",
      "##× -> {',': 1.25, '.': 1.1, 'the': 1.29, 'and': 1.15, '##×': 0.82}\n",
      ". -> {',': 1.38, '.': 1.24, 'the': 1.25, 'and': 1.19, 'magic': 0.23, 'soap': 0.3}\n",
      "[SEP] -> {',': 1.21, '.': 1.15, 'the': 1.17, 'and': 0.93, 'family': 0.13, 'training': 0.09, 'word': 0.5, 'deep': 0.46, 'magic': 0.52, 'soap': 1.37, 'enzyme': 0.36, 'chip': 0.31, 'algorithm': 0.16}\n"
     ]
    }
   ],
   "source": [
    "for token, counter in token_mapper:\n",
    "    print(token, \"->\", dict(counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "00dadfe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['other', 'knowledge', '-', 'intensive', 'language', 'tasks', '.', 'while', 'many']\n",
      "dict_keys(['knowledge', 'intensive', 'language', 'c language', 'tasks'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({'language': 0.87,\n",
       "         'c language': 0.52,\n",
       "         'intensive': 0.3,\n",
       "         'knowledge': 0.03,\n",
       "         'tasks': 0.01})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_index =24\n",
    "end_index = 33\n",
    "print([item[0] for item in token_mapper[start_index:end_index]])\n",
    "test = Counter()\n",
    "for item in token_mapper[start_index:end_index]:\n",
    "    test.update(item[1])\n",
    "\n",
    "print(test.keys())\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630a4268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in reverse\n",
    "\n",
    "token_mapper = {}\n",
    "for r,c in zip(row, col):\n",
    "    r_token_id = int(tokens[\"input_ids\"][0][r])\n",
    "    r_token_str = reverse_voc[r_token_id]\n",
    "\n",
    "    c_token_id = int(c)\n",
    "    c_token_str = reverse_voc[c_token_id]\n",
    "\n",
    "    if c_token_str not in token_mapper: token_mapper[c_token_str] = []\n",
    "    score = float(out[0][r, c])\n",
    "\n",
    "    token_mapper[c_token_str].append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc90c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in token_mapper:\n",
    "    scores = list(sorted(token_mapper[k], reverse=True))\n",
    "    print(k, [round(item, 2) for item in scores[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ed9353",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6442c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    batch_doc_rep, batch_doc_token_indices, batch_doc_pad_len = model.encode(tokenizer([doc, doc], return_tensors=\"pt\"), is_q = False)  # (sparse) doc rep in voc space, shape (30522,)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(batch_doc_rep.size(0)):\n",
    "    doc_rep = batch_doc_rep[i]\n",
    "    doc_token_indices = batch_doc_token_indices[i]\n",
    "\n",
    "    # get the number of non-zero dimensions in the rep:\n",
    "    col = torch.nonzero(doc_rep).squeeze().cpu().tolist()\n",
    "    print(\"number of actual dimensions: \", len(col))\n",
    "\n",
    "    # now let's inspect the bow representation:\n",
    "    weights = doc_rep[col].cpu().tolist()\n",
    "    _indices = doc_token_indices[col].cpu().tolist()\n",
    "    d = {k: v for k, v in zip(col, weights)}\n",
    "    d_indices = {reverse_voc[k]: v for k, v in zip(col, _indices)}\n",
    "    sorted_d = {reverse_voc[k]: v for k, v in sorted(d.items(), key=lambda item: item[1], reverse=True)}\n",
    "    print(d_indices, \"\\n\", sorted_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b343054e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd833238",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp[0].shape, temp[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8aa202",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832edf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b568a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.tokenize(doc))\n",
    "print()\n",
    "print(original_tokenizer.tokenize(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35680bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_tokenizer.tokenize(\"asdbaisbd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715d4526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b95171",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/scratch/lamdo/doris-mae/DORIS-MAE_dataset_v1.json\") as f:\n",
    "    ds = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd890a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"Corpus\"][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25658ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "machine learning is fun\n",
    "\n",
    "-> [\"machine\" \"learning\" \"machine learning\" \"is\" \"fun\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
