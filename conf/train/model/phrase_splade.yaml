# @package _global_

# lamdo/distilbert-base-uncased-phrase-60kaddedphrasesfroms2orc-mlm-150000steps # mlm training, priortize phrase (85% of phrases are masked)
# lamdo/bert-base-uncased-phrase-60kaddedphrasesfroms2orc-mlm-150000steps # same as above, but with bert
# lamdo/distilbert-base-uncased-phrase-60kaddedphrasesfroms2orc-mlm-150000steps-v2 # mlm training, phrases and tokens are treated equally
# lamdo/distilbert-base-uncased-phrase-30kaddedphrasesfroms2orc-mlm-70000steps # mlm training, priortize phrase (85% of phrases are masked), batch size 64 (previously 32)
# lamdo/distilbert-base-uncased-phrase-30kaddedphrasesfrommsmarco-mlm-70000steps # mlm training, priortize phrase (85% of phrases are masked), batch size 64 (previously 32), using msmarco to create phrase vocab and do pretraining
# lamdo/distilbert-base-uncased-phrase-30kaddedphrasesfroms2orcfreqbased-mlm-70000steps # mlm training, priortize phrase (85% of phrases are masked), batch size 64 (previously 32). Phrase vocab is frequency based
# lamdo/distilbert-base-uncased-phrase-60kaddedphrasesfroms2orcfreqbased-mlm-150000steps # mlm training, priortize phrase (85% of phrases are masked), batch size 32. Phrase vocab is frequency based. Phrase vocab size is 60k
# lamdo/distilbert-base-uncased-phrase-15kaddedphrasesfroms2orc-mlm-70000steps # mlm training, priortize phrase (85% of phrases are masked), batch size 64. Phrase vocab is built via maximum coverage. Phrase vocab size is 15k
# lamdo/distilbert-base-uncased-phrase-15kaddedphrasesfroms2orcfreqbased-mlm-70000steps # mlm training, priortize phrase (85% of phrases are masked), batch size 64. Phrase vocab is frequency based. Phrase vocab size is 15k
# lamdo/distilbert-base-uncased-phrase-30kaddedphrasesfroms2orc_cs-mlm-26000steps # mlm training, priortize phrase (85% of phrases are masked), batch size 64. Train with s2orc CS corpus. Phrase vocab is built via maximum coverage. Phrase vocab size is 30k

# lamdo/distilbert-base-uncased-phrase-60kaddedphrasesfroms2orc-mlm-70000steps # mlm training, priortize phrase (85% of phrases are masked), batch size 48. Phrase vocab is built via maximum coverage. Phrase vocab size is 60k
# lamdo/distilbert-base-uncased-phrase-5kaddedphrasesfroms2orc-mlm-70000steps # mlm training, priortize phrase (85% of phrases are masked), batch size 64. Phrase vocab is built via maximum coverage. Phrase vocab size is 5k

# lamdo/scibert-base-uncased-phrase-30kaddedphrasesfroms2orc_cs-mlm-26000steps  # mlm training, priortize phrase (85% of phrases are masked), batch size 64. Train with s2orc CS corpus. Phrase vocab is built via maximum coverage. Phrase vocab size is 30k. Using SciBERT instead of distilbert
# lamdo/scibert-base-uncased-phrase-30kaddedphrasesfroms2orc-mlm-70000steps # mlm training, priortize phrase (85% of phrases are masked), batch size 64 (previously 32). Using SciBERT instead of distilbert


# lamdo/bert-base-uncased-phrase-30kaddedphrasesfroms2orc-mlm-70000steps # mlm training, priortize phrase (85% of phrases are masked), batch size 64 (previously 32). Using BERT instead of distilbert
# lamdo/cocondenser-phrase-30kaddedphrasesfroms2orc-mlm-70000steps # mlm training, priortize phrase (85% of phrases are masked), batch size 64 (previously 32). Using Co-condenser instead of distilbert


# "/scratch/lamdo/phrase_splade_checkpoints/phrase_splade_71/debug/checkpoint/model" # CASPER, after training with FRIEREN

init_dict:
  model_type_or_dir: lamdo/cocondenser-phrase-30kaddedphrasesfroms2orc-mlm-70000steps
  model_type_or_dir_q: null
  freeze_d_model: 0
  agg: max
  fp16: true
  original_bert_vocab_size: 30522

config:
  tokenizer_type: lamdo/cocondenser-phrase-30kaddedphrasesfroms2orc-mlm-70000steps

